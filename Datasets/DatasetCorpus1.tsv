id	year	title	author	content	abstract
1	2019	Robust velocity-induced acoustic oscillations at cosmic dawn	Julian B. Muñoz	Robust velocity-induced acoustic oscillations at cosmic dawn The redshifted 21-cm line of hydrogen holds great potential for the study of cosmology, as it can probe otherwise unobservable cosmic epochs. In particular, measurements of the 21-cm power spectrum during cosmic dawn—the era when the first stars were formed—will provide us with a wealth of information about the astrophysics of stellar formation, as well as the origin of fluctuations intn our Universe. In addition to their usually considered density fluctuations, dark matter and baryons possess large relative velocities with respect to each other, due to the baryon acoustic oscillations suffered by the latter, which suppress the formation of stars during cosmic dawn, leaving an imprint on 21-cm observables during this era. Here we present 21cmvFAST, a version of the publicly available code 21cmvFAST modified to account for this effect. Previous work has shown that the inclusion of relative velocities produces an acoustic modulation on the large-scale 21-cm power spectrum during cosmic dawn. By comparing analytic calculations with simulations from 21cmvFAST, here we demonstrate that this modulation takes the form of robust velocity-induced acoustic oscillations (VAOs), during both the Lyman-α coupling era and the subsequent epoch of heating. The unique shape of these VAOs, which is determined by the acoustic physics that generated the relative velocities, can be analytically computed and is easily distinguishable from the usual density-sourced fluctuations. We find that, under a broad range of astrophysical assumptions, the VAOs are detectable at high significance by the upcoming HERA interferometer, which could therefore confirm the presence of acoustic oscillations at cosmic dawn. I. INTRODUCTION The matter in our Universe clusters under its own gravitational pull, slowly growing the seeds of primordial fluctuations into the rich large-scale structure of our cosmos [1–4]. Nonetheless, not all matter behaves equally. While the majority of matter is dark, cold, and collisionless, roughly a sixth of it is baryons, capable of interacting with photons through electromagnetic forces. These interactions give rise to the famous baryon acoustic oscillations (BAOs), which are imprinted onto the distribution of baryons thereafter and have been observed in the cosmic microwave background (CMB) [5], as well as at lowredshift surveys [6–8]. Interestingly, the lack of BAOs for the dark matter (DM) gives rise to a bulk relative velocity between it and the baryons [9]. This “streaming” velocity, with a rootmean-squared value of vrms ≈ 30 km s−1 at recombination, becomes supersonic thereafter due to the low baryonic temperature. The resulting supersonic motions between baryons and DM—which fluctuate on acoustic scales— have three suppressive effects on the formation of the first stars, as (i) fewer haloes are formed [9–11], (ii) the amount of gas in each halo available for cooling is smaller [12–14], and (iii) it is more difficult for said gas to settle down into stars [15–20]. Together, these effects modulate the number of star-forming galaxies in the early universe as a function of the DM-baryon relative velocity [21–25]. Even though we do not have direct access to the first star-forming galaxies at redshift z ∼ 20, the hydrogen 21-cm line will allow us to indirectly map the distribution of the first stars, as their emission affects the 21-cm brightness temperature [26–28]. The presence of streaming velocities is often ignored in public 21-cm codes, such as 21cmvFAST [29,30], which is a good approximation if star formation mainly occurs in fairly massive haloes (with virial temperatures Tcool ≳ few × 104 K), but will cause a severe overestimation if there is stellar formation in molecular-cooling haloes (with Tcool ≲ 104 K [31–33]). Here we present 21cmvFAST, 1 a modified version of 21cmvFAST2 which properly accounts for the effects of the DM-baryon relative velocities. include relative velocities in the initial conditions of 21cmvFAST, which then affects the amount of stellar formation at every cell in the simulation box. We additionally account for the averaged effect of Lyman-Werner (LW) feedback, which dissociates molecular hydrogen and raises the cooling temperature of haloes, reducing the effect of the streaming velocities at lower redshifts [22,34,35]. Thus, while for studies of the epoch of reionization, at z ∼ 6, the streaming velocities are fairly unimportant (see, however, Ref. [36]), they have a large impact on the 21-cm signal during the cosmic-dawn era (z ¼ 15–30), as shown in Refs. [13,22–24,24,25] and as we will further explore here. We find that the DM-baryon relative velocities (vcb) produce a modest delay Δz ∼ 2 on all the landmarks of cosmic evolution, as fewer stars form on average. More interestingly, however, the relative velocities show acoustic oscillations in their power spectrum, due to their BAO origin. These oscillations are imprinted in the distribution of the first stars, and therefore in the 21-cm power spectrum during cosmic dawn. This was first proposed in Ref. [13] during the Lyman-α coupling era (LCE), where the Lyman-α photons emitted by the first stars coupled the thermal and spin temperatures of neutral hydrogen [37–39]. A similar prediction was made in Refs. [24,25] for the epoch of heating (EoH), where the x rays emitted by the first galaxies heated the neutral hydrogen, thus affecting the 21-cm signal. Here we use 21cmvFAST to study the effect of the relative velocities across the entire cosmic-dawn era (z ¼ 15–30), comparing our simulations with analytic calculations. Given the large astrophysical uncertainties during this epoch, we consider a broad range of possible feedback strengths. In all cases we find velocity-induced acoustic oscillations (VAOs) on the 21-cm power spectrum, with an amplitude growing in size during the LCE (peaking at z ≈ 24 for our fiducial parameters), vanishing in the transition to the EoH, then peaking again during the heating era (at z ≈ 17), and vanishing after the intergalactic medium (IGM) is fully heated (z ≲ 12). This is in agreement with our analytic estimates. Furthermore, we confirm that the shape of the VAOs follows the wellunderstood fluctuations in v2 cb [13,40,41], with damping at small scales due to the nonlocality of photon propagation. We parametrize this effect through a window function, as in Ref. [13], which dampens VAOs below k ≈ 0.05 Mpc−1 during the LCE, due to the large mean-free path of UV photons, and below k ≈ 0.2 Mpc−1 during the EoH, as x-ray photons typically get absorbed closer to their source [42]. Our analytically calculated VAOs and window functions very well match our numerical simulations during both the LCE and the EoH. Finally, we study the observability of the VAOs with the upcoming hydrogen epoch of reionization array (HERA) [43]. We find that under most of our astrophysical scenarios, the VAOs are observable at large (>5) signal-tonoise ratios. This would confirm that the “small” molecular-cooling galaxies are driving stellar formation at cosmic dawn, and additionally yield the first measurement of baryon acoustic oscillations at a redshift intermediate between galaxy surveys and the CMB, which in Ref. [44] we propose as a novel standard ruler during cosmic dawn. This paper is structured as follows. In Sec. II we review the effects of the streaming velocities on the formation of the first stars. We use these effects in Sec. III to find the changes in the 21-cm signal due to velocities, using 21cmvFAST to find 21-cm maps and power spectra. We quantify the effect of the velocities in detail in Sec. IV, and study its observability in Sec. V. We conclude in Sec. VI. II. THE EFFECT OF VELOCITIES ON THE FIRST STARS We begin by reviewing the effect of the streaming velocities on the first star-forming galaxies. This section heavily draws from Refs. [9,12,13,18], so the reader might want to skip to Sec. III to see the impact on the observable 21-cm line. Throughout this work we use the best-fit cosmological parameters from Planck 2018 [5], and all lengths and wave numbers will be comoving unless otherwise specified. As in Ref. [9], we use a moving-mesh perturbation theory, which is valid for regions of radius R ≲ 3 Mpc, where the velocity is coherent. Then, there exists a zeroorder solution to the Boltzmann equations, with no overdensities, where θi is the velocity divergence of species i, μ ≡ k · vcb=ðkvcbÞ, H is the Hubble expansion rate at redshift z, vcb ¼ jvcbj, and the prime denotes the derivative with respect to redshift. Additionally, ΓC is the Compton interaction rate, Tγ is the CMB temperature, and we include the effect of temperature fluctuations δT ¼ δTg=Tg, where Tg is the gas temperature. Here, the isothermal sound speed is given by [40] A. Gas fraction As in Refs. [12,13], we calculate the effect of the DMbaryon relative velocity on the fraction of gas accreted by DM haloes through the filter mass MF. This quantity, first introduced in Ref. [47], allows us to compute the effect of the baryonic pressure, be it thermal or not, on the collapse of baryons to haloes, by using results from the linear regime. As in Refs. [12,48] we fit the ratio to first order in k2, where we use the notation of Ref. [48] in defining rLSS as the low-k large-scale structure (LSS) difference between baryons and all matter (which, however, we take at k ¼ 1 Mpc−1 to avoid the BAO wiggles). We define the filter mass as [12] where ρ0 m is the matter density today and the (comoving) filter scale is given by rF ¼ π=kF. While this fit is not perfect, it provides us with a good estimate of MF by using linear theory alone. Still, there are two important caveats to note here. The first is that the effect of the vcb-induced pressure in the baryon small-scale distribution has a different k behavior from the usual thermal pressure, although we parametrize it in the same way. The second is that the filter mass is a real-space quantity, whereas the overdensities δb and δm that we computed are Fourier-space quantities. Therefore, we have to average over the (cosine of the) angle μ between k and vcb at some point, since real-space quantities cannot depend on it. We choose to average the ratio δb=δm directly, instead of each density individually, although we have checked that this produces differences below 10% on the filter mass. Given the filter mass we can calculate the amount of matter in each halo that is composed of gas through the fit during matter domination, and assuming our fiducial cosmological parameters, as only haloes with M > Mcoolðz; vcbÞ have virial temperatures large enough to produce stellar formation. We additionally include the effect of LW feedback in the formation of stars. As gas collapses into stars, these will emit UV photons in the Lyman-Werner band (11.2– 13.6 eV), which can easily photodissociate H2 [32,50–53]. This raises the minimum halo mass as the Universe evolves [34,54–56], until it reaches the atomic-cooling threshold, which we take to be at VHI cool ¼ 17 km s−1 [57,58] (corresponding to Tcool ¼ 104 K and Mcool ≈ 3 × 107 M⊙). The LW band overlaps with the Lyman-α band that produces Wouthuysen-Field coupling, which we discuss in Sec. III, albeit the relation between the two fluxes depends on astrophysical assumptions regarding the photon escape fraction, as well as self-shielding, among others [59,60]. We will not attempt to model this process, and instead rely on the results of Refs. [34,35,54], where it was shown that the effect of LW feedback is to raise the mass of cooling haloes asB and β control the effect of the LW feedback on the cooling of gas in haloes. Given the large astrophysical uncertainties in both the effect of velocities and the strength of the LW feedback, we will study four possible feedback strengths: (i) No feedback, B ¼ 0. We do not expect this case to accurately reproduce our Universe, but it is a useful toy model where the effect of the velocities is most prominent in the 21-cm power spectrum. (ii) Low feedback, B ¼ 4 and β ¼ 0.47. This model follows the fits found in Refs. [34,54], albeit with a coefficient B roughly half of the best-fit value. (iii) Regular feedback, B ¼ 7 and β ¼ 0.47. This is the prescription in Refs. [22,35], as a fit to the simulations of Refs. [34,54], which results in a larger impact of FLW on the minimum halo mass. (iv) High feedback, B ¼ 7 and β ¼ 0.47 again; however, here we assume that the effect of LW photons and the relative velocities is fully uncorrelated, as in Ref. [24], so that Vð0Þ coolðFLWÞ ¼ Vð0Þ coolð0Þ × ½1 þ BðFLWÞβ 1=3 instead of the formulation of Eq. (11). This dampens the effect of velocities considerably, as we will see. In each of these models we will take the average LW flux from the strong-feedback case in Ref. [22], where it is selfconsistently computed through the number of baryons collapsed into stars as a function of redshift, except for the low feedback strength, where we divide it by a factor of 2. Additionally, we take α ¼ 4 in the regular- and highfeedback cases, and α ¼ 6 in the no- and low-feedback cases, which enhances the vcb-induced fluctuations. We leave for future work to self-consistently compute the Lyman-Werner feedback in each cell of our 21cmvFAST simulation, similarly to how the Lyman-α flux is computed. Note that we parametrize the unknown strength of the LW feedback through the poorly understood fitting function Eq. (11) (B and β), where the coefficients are fit from a few (three) data points in Refs. [34,54], as opposed to the redshift at which FLW is evaluated, as in Ref. [22], since in Ref. [35] it was found that evaluating at the redshift of collapse is a good approximation unless J21 grows extremely rapidly.1. Correction to f Following Refs. [22,34], we assume that not all haloes above Mcool form stars with the same efficiency, as haloes near the threshold have a smaller fraction of their gas that can cool. We follow the parametrization in Ref. [22], where the fraction of baryons that form stars depends on the halo mass M as C. Halo mass function Through providing an additional source of pressure for baryons at small scales, the DM-baryon relative velocities can greatly affect the abundance—and bias—of the first star-forming haloes of our Universe, as first pointed out in Ref. [9] and confirmed with simulations in Ref. [10]. We follow their calculation here, albeit using the Reed halo mass function (HMF) from Ref. [61], which is similar to the Sheth-Tormen HMF [62], but better calibrated at the redshifts of interest (z ∼ 10–30) to correctly account for a suppression of high-mass haloes. We begin by calculating the local value of the matter power spectrum [9]z ¼ 20 and 30, respectively. This figure shows that, in all cases, higher velocities require larger haloes to form stars, translating into lower values of σcool. The size of this effect decreases with the strength of the LW feedback, as it raises the minimum cooling mass, and increases with redshift, as the velocities are more relevant at earlier times, although the size of the effect does not scale simply as vcbðzÞ, as it depends on its integrated history. Finally, we note that the “kinks” in some of the lines with feedback in Fig. 1 correspond to the transition to atomic-cooling haloes, where the effect of vcb in Vcool [as in Eq. (9)] vanishes. When computing the HMF we work with the dimensionless quantity [62,66] D. All combined Finally, we can combine all the aforementioned effects to calculate the fraction of baryons that collapse to starforming haloes as [25]  where ρb is the comoving baryon abundance. This includes the velocity effect on the HMF [through nðMÞ]; on the gas fraction fgas of haloes; and on the minimum cooling mass Mcool, all of which are functions of redshift and vcb. To illustrate the dependencies in this function, we show Fcoll as a function of the minimum cooling mass M in Fig. 2, for both the no- and the regular-feedback cases. There we see how larger velocities decrease the number of haloes and the gas fraction in every halo, as well as increase the minimum cooling mass. These effects are less apparent in the case with feedback, although still present to a large degree. We show the collapsed function of baryons that can form stars, from Eq. (19), in Fig. 3 as a function of redshift, for different values of vcb, as well as its average over velocities. From this figure we see that velocities have a larger impact at higher z, as their effects partially redshift away at later times, with the average Fcoll being a factor of 2–5 smaller than the vcb ¼ 0 case. This illustrates the necessity of accounting for relative velocities if one is interested in molecular-cooling haloes, since otherwise one would form too many of them. This can be solved easily with 21cmvFAST. As we will see, our observable—the 21-cm brightness temperature—depends on Fcoll nonlocally, as photons where erfc is the complementary error function and FEPS is the global average of FEPS. We follow the prescription in 21cmvFAST by taking the nonlinear overdensities in δ (i.e., extrapolating the z ¼ 0 result backwards) and calculating FEPS numerically by averaging over our cells [29,30]. Notice that in the limit R → ∞, the collapsed fraction in Eq. (21) asymptotes to FEPSð0;hvcbiÞ, which is not necessarily the same as hFEPSðδ; vcbÞi (where hi denotes the spatial average). Nonetheless, as we divide by the actual averaged value of FEPS, any bias that could arise from this procedure is canceled out. Additionally, as shown in Figs. 2 and 3, the averaged Fcoll is almost identical to its value for vcb ¼ hvcbi. This is because the probability density function (PDF) for v2 cb is highly peaked around v2 rms, where vrms ≈ 30 km s−1 is the root mean square of the MaxwellBoltzmann distribution of vcb, so using the mode value gives very similar results to averaging over the PDF. Additionally, for computational efficiency we find the σcoolðvcbÞ (as plotted in Fig. 1) by calculating Fcoll with a Press-Schechter HMF [70] including all velocity effects, as it takes the simple form in Eq. (21) for δ ¼ σR ¼ 0, and one can find σcoolðvcbÞ by inverting the erfc function. III. THE 21-CM LINE The distribution of the first stars during cosmic dawn cannot be directly probed. Fortunately, their presence can be indirectly inferred with measurements of the 21-cm line, which we now describe. A. The basics We start by briefly reviewing the physics of the 21-cm hydrogen line (for a detailed review we encourage the reader to see Refs. [21,26,27]). Hydrogen, in addition to being the most abundant element in our Universe, possesses a rich hyperfine structure. Of particular interest is itsspin-flip transition from the triplet to the singlet ground state, which produces a photon with a 21-cm wavelength. To determine whether cosmological hydrogen emits or absorbs 21-cm photons during a certain era, one has to calculate its spin temperature and compare with that of the CMB. Given a comoving number density of hydrogen atoms in the triplet (n1) and singlet (n0) states, we can define the local spin temperature Ts throughfor our fiducial cosmological parameters, where xHI is the neutral-hydrogen fraction (nearly unity for all eras of interest in this work) and ∂rvr is the line-of-sight gradient of the velocity. The standard history of the 21-cm line of hydrogen is as follows. During the dark ages collisions coupled the spintemperature to that of the gas, producing 21-cm absorption f Fourier-space pixel with density δðiÞ  Given the implementation of 21cmvFAST outlined above, we can now calculate the 21-cm brightness temperature for any combination of cosmological and astrophysical parameters, including molecular-cooling haloes with a time-dependent LW feedback, and the effect of relative velocities. In all of our simulations we take an x-ray luminosity of log10ðLX=SFRÞ ¼ 40 [divided by the star-formation rate (SFR)] and a threshold in x-ray energy of E0 ¼ 0.2 keV [25], with a spectral index of αX ¼ 1, and we do not modify the rest of the standard 21CMMC parameters. We will also explore the case of E0 ¼ 0.5 keV in the Appendix A, as the first x-ray sources might have harder spectra than we assume [74–76]. Instead of a fixed cooling temperature Tcool;Xray ¼ 104 K for x-ray haloes we use the prescription for Fcoll and σcool outlined in Sec. II. Additionally, we have not modified the ionization part of 21cmvFAST, assuming that the haloes we study here provide a negligible number of UV photons that escape into the IGM. Therefore, for ionizations we employ the “faint galaxies” model of Ref. [77], where reionization is driven by galaxies with Tcool;UV ¼ 2 × 104 K, with an ionizing efficiency of ζ ¼ 20, and by the end of our simulations, at z ¼ 12, we always have xe ≪ 1. We show in Fig. 5 the average 21-cm brightness temperature hT21i in our simulation boxes as a function of redshift.for more-massive haloes to form stars at lower redshifts. As a consistency check, we also show in that plot the result for a simulation with fixed vcb ¼ vavg ¼ hvcbi ¼ 0.92vrms, where no velocity-induced fluctuations are present. We see that in this case the average 21-cm temperature is very similar to the full case. So while vcb affects the background evolution of the 21-cm line in a particular way, these effects can potentially be mimicked by time-dependent feedback on the star-formation rate and are thus not unique. We also show, for comparison, the background evolution of T21 in the unphysical case that LW feedback is saturated at all times, and only atomic-cooling galaxies are allowed to form stars. We elaborate further on this case in Appendix B. D. Fluctuations and power spectrum Fortunately, there is more information in the 21-cm line than the global signal carries. Including vcb and LW feedback delays the formation of structure, which affects the 21-cm fluctuations during the cosmic-dawn era in two ways. First, larger velocities result in fewer Lyman-α photons, producing anisotropies during the LCE [13,17,23]. Second, even after Lyman-α coupling is saturated, larger velocities result in fewer x-ray photons being emitted, and therefore less IGM heating [17,25]. Following these two effects we divide the cosmic dawn into two different eras, the EoH and the LCE, which have qualitatively different fluctuation behaviors. This is illustrated in Fig. 6, where we show the 21-cm temperature in slices 1 Mpc thick of three simulation boxes (each 300 Mpc in length), across the entire cosmic dawn, starting at z ¼ 35 and finishing at z ¼ 12, for the same three cases as Fig. 5 (i.e., ignoring both vcb and LW feedback, ignoring only the feedback, and including the full set of effects). Here it isvery clear how the inclusion of vcb and LW feedback delays the onset of the LCE, as well as the transition to the EoH, as the first galaxies take longer to emit the necessary number of UV and x-ray photons. We now study the 21-cm fluctuations during each of these two eras in detail, focusing on our regular-feedback case, with the full set of feedback effects, to explore the effect of relative velocities. Throughout this section we will employ two types of simulations. We will show 21-cm maps with a box size of L ¼ 300 Mpc and a 1-Mpc resolution, as those are easiest to visually inspect.3 These boxes share the density and velocity fields from Fig. 4. When computing power spectra, however, we will increase the box size to L ¼ 900 Mpc, with a 3-Mpc resolution, in order to compute the largescale 21-cm fluctuations with smaller variance. In all cases where we compare simulations with and without vcb we keep an average value vavg for the latter, so that both cases have very similar global signals. E. During the Lyman-α coupling era We start with the LCE, which stretches from the formation of the first abundant sources of Lyman-α photons, to the transition to x-ray heating, where hT21i is minimum. This corresponds to the redshift range z ¼ 20–28 in our fiducial model. During this era, higher velocities produce a shallower 21-cm signal by reducing the number of stars emitting UV photons able to produce Lyman-α coupling. This is, however, a fairly nonlocal effect, as these photons can travel distances as large as 100 Mpc before transitioning into the Lyman-α line, which will smear the VAOs [13]. We show 21-cm maps during the LCE in Fig. 7, both with and without velocity fluctuations. This figure shows three representative redshifts, as marked by blue and green circles in Fig. 5, as landmarks of the LCE. The first redshift we show is z ¼ 28.3, at the onset of Lyman-α coupling. Here regions of large velocity produce shallower 21-cm absorption (shown in red), whereas regions with vcb ≈ 0 produce deeper absorption (shown in blue). This effect is perhaps more apparent at z ¼ 25.0, halfway through the Lyman-coupling era (where hT21i ¼ 0.5 × hT21imin), where regions of large vcb translate into red “islands” in Fig. 7. Finally, we show the 21-cm fluctuations at z ¼ 20.3, in the transition between Lyman-coupling and X-ray heating (where hT21i is at its minimum). Here we find negligible large-scale VAOs. This is to be expected, since the effect of the velocities on the 21-cm brightness temperature during the EoH and LCE are opposite and ought to(non-VAO) 21-cm power spectrum follows a similar evolution. We see in Figs. 7 and 8 how in the transition from the LCE to the EoH all the power resides at small scales, as the first x-ray emitting sources heat up the gas around them preferentially. We note that the regular density BAOs are included through the total-matter fluctuation δðiÞ ðkÞ, whose transfer function we obtain from CLASS [46], but their amplitude is too small (and smoothed by the complicated astrophysics of the cosmic dawn) to be observable in our power spectra. F. During the epoch of heating We now move to the EoH, which we define to begin at the minimum of hT21i, and to last until Ts ≫ Tγ , where the entire box emits in 21 cm (T21 > 0). For our fiducial parameters the EoH extends from z ¼ 20 to z ¼ 15, and throughout the entire EoH Tg and Ts are very efficiently coupled. During this era the 21-cm fluctuations are dominated by gas-temperature anisotropies due to the inhomogeneous x-ray heating of the IGM [79,80]. The way the relative velocities will impact the 21-cm signal is, then, by changing the amount of x-ray heating in different regions. Patches with large velocities will form fewer stars, producing less heating, and therefore causing deeper 21-cm absorption. As in the LCE case we will show results for three representative redshifts, now in Fig. 9. The first is z ¼ 17.2, halfway through the heating transition (where hT21i ¼ 0.5 × hT21imin). In this case there are marked 21-cm fluctuations at large scales due to the DM-baryon relative velocity, since regions with large relative velocities produce a more-negative T21. The same is true at the transition to emission, defined as hT21i ¼ 0, which corresponds to z0 ¼ 14.5 in our model. The last case we show is at z ¼ 12.0, at the end of our simulations, at which point the IGM has been heated to Ts ≈ Tg ≫ Tγ , making the Ts term of Eq. (23) negligible. In this case clearly the DM-baryon relative velocity does not have an important effect, as T21 has saturated. The smooth (non-VAO) power follows roughly the same trajectory, as it is driven by the anisotropies in the distribution of stars, and thus of x-ray photons. As in the previous section we plot the 21-cm power spectrum during the EoH in Fig. 10. For the highest redshift, z ¼ 17.2, we find very important VAOs at k ∼ 0.1 Mpc−1, much larger than during the LCE and not severely damped. As time progresses, x rays slowly heat up the IGM, making the power spectrum smaller (and the VAOs less pronounced) at z ¼ 14.5. We do not find a trace of VAOs in the z ¼ 12.0 case, as expected, since in our model T21 has saturated by then. Notice that here, and throughout this work, we ignore any explicit LoS anistropies in the signal. There are, however, some effects that can make the signal anisotropic, such as redshift-space distortions [81,82] and the effect of the light cone [83,84] (as we only take coevaluated boxes).IV. QUANTIFYING THE EFFECT OF vcb So far we have established that the DM-baryon relative velocities affect the 21-cm signal during the entire cosmic-dawn era, even when accounting for LymanWerner feedback, producing striking VAOs on large scales. Let us, however, confirm that these acoustic oscillations are indeed produced by the relative velocities, and not over/ underdensities. For that, we perform simulations in which we fix vcb to its average value, vavg, and we compare in Fig. 11 the 21-cm power spectrum with our regular case at two redshifts, z ¼ 17.2 (halfway through the EoH) and z ¼ 25.0 (halfway through the LCE). This choice of a nonfluctuating vcb ¼ vavg keeps roughly the same 21-cm global signal, as well as small-scale power spectrum. Nonetheless, as is clear from Fig. 11, the addition of velocity fluctuations produces additional power on large scales, with acoustic wiggles. This is what we call the VAOs. We now move on to quantify the shape and amplitude of these VAOs, comparing them with analytic expectations. A. Shape of the effect The streaming velocities fluctuate with a power spectrum sourced by the BAOs [9], so that the power spectrum of any function fðvcbÞ (such as Fcoll or δT21) will have a component sourced by vcb fluctuations. Two facts will greatly simplify our discussion about velocities moving forward. First, for several examples it was shown in Refs. [13,40,41] that1. Lyman-α pumping The number ni of x-ray or Lyman-α photons present at a point x at redshift z depends on previous redshifts z0 and positions x0 in the past light cone. In particular, for Lyman-α photons this number is given by the integral over all previous redshiftsassuming a flat spectrum of UV photons, where η is conformal time, Nα is the number of Lyman-α photons emitted per collapsed baryon, and c is the speed of light. Therefore, a fluctuation in F_ coll due to velocities will affect the number of photons up to a significant distance away. This nonlocality can be recast as a window function in Fourier space, where now the fluctuation in the number of Lyman-α photons can be written as2. X-ray heating For the case of x rays the situation is slightly different, as their typical mean-free path is significantly shorter than that of Lyman-α photons. We can, nonetheless, define a window function for this scenario as well. We will follow the calculation in 21cmvFAST, and assume that all photons with optical depths τ ≥ 1 are absorbed, whereas the rest travel freely [29]. This makes the mean-free path of an Using all these results we show the x-ray window function in Fig. 12, along with that of Lyman-α photons, each at their respective relevant redshifts. Interestingly, we see that x-ray photon anisotropies are not severely damped within the scales of interest (k ≲ 0.3 Mpc−1), where velocity fluctuations are important. This is in agreement with the x-ray window function obtained for regular overdensities in Ref. [88]. We further illustrate this point in Fig. 13 by showing Δ2 v2 (the power spectrum of δv2 ) multiplied by window functions (squared) due to either x-ray or Lyman-α propagation, at z ¼ 17.2 and z ¼ 25.0, respectively. We see how accounting for x-ray propagation dampens the small-scale power, while leaving the interesting region of 0.01 ≤ k Mpc ≤ 0.3 roughly unaltered, whereas the propagation of Lyman-α photons dampens power at all relevant scales, making the power spectrum difficult to observe beyond k ≳ 0.1 Mpc−1. B. Size of the effect Now that we understand how photon propagation alters the shape of the vcb fluctuations, let us fit for the amplitude of the VAOs in our simulations. As mentioned at the beginning of this section, the VAOs are, to first order, statistically independent from density fluctuations [13]. We, therefore, choose to model the 21-cm power spectrum simply asin the transition from the LCE to the EoH. This is because the effect of velocities on the 21-cm line during the LCE and the EoH goes in opposite directions. During the LCE a large vcb results in higher T21, as fewer Lyman-α photons couple Ts to Tg. On the other hand, during the EoH larger velocities produce a more-negative T21, as fewer x-ray photons heat up the gas. This means that, by necessity, these two components have to cancel at some point, resulting in nearly zero VAO fluctuations, which in our simulations occurs at z ≈ 21. In addition to the VAO-sourced 21-cm variance δT2 21;vel, we show in Fig. 14 the VAO-only power spectrum Δ2 21;vel as a function of redshift at a reference scale kref ¼ 0.14 Mpc−1, computed with Eq. (37). Here the effect of the window functions is obvious, as the fluctuations on T21 during the LCE are significantly smaller than during the EoH at this wave number, even though the effect of the velocities in Fcoll is larger at higher redshifts. As a test, we also show in Fig. 14 results for a case in which we fix vcb to its average value, obtained through the same fitting procedure as our main result. This curve can be taken as an FIG. 14. Amplitude of the 21-cm VAOs as a function of redshift. In the top panel we show the VAO-induced variance, computed from Eq. (39), obtained from our simulations in the black line and from analytic estimates in the red dashed line. As a consistency check we show the result from a simulation without vcb fluctuations in the grey dotted line, which should be consistent with zero, providing an estimate of the error in our fitting procedure. In the bottom panel we show the VAO-only power spectrum for the same case, at a reference scale kref ¼ 0.14 Mpc−1, typically observable by interferometers. JULIAN B. MUÑOZ PHYS. REV. D 100, 063538 (2019) 063538-16 estimate of the errors in both the fitting procedure and our simulations, and is significantly smaller than the amplitude in the case with velocities. We therefore conclude that our procedure is properly capturing both the shape and amplitude of the VAOs. C. Comparison with analytic calculations We now estimate the expected size of the VAO signal from analytic arguments, extending on the work of Refs. [13,24] by including x-ray and Lyman-α fluctuations jointly. During cosmic dawn we can safely neglect collisional hyperfine transitions, so the spin temperature will be given by [27]V. TOTAL SIGNAL AND OBSERVABILITY The inclusion of streaming velocities produces VAOs in the 21-cm power spectrum on large scales. We now explore how the signal changes when changing different assumptions, especially the poorly known Lyman-Werner feedback, as well as how observable these VAOs are by upcoming 21-cm interferometers. A. Different feedback assumptions We start by considering the VAOs under three additional feedback assumptions, different than the regular-feedback strength that we have focused on so far. The first is an unrealistic no-feedback case, which will show the largest 21-cm VAOs. The other two are realistic feedback strengths: low and high, slightly more optimistic and pessimistic than our fiducial case, as explained in Sec. II. We also explore the 21-cm power spectrum for the unphysical case that LW feedback is saturated at all times (so only atomic-cooling haloes can form stars) in Appendix B. We first show, in Fig. 15, the evolution of the global 21-cm signal as a function of redshift for the four feedback cases. Here, barring the effect of the LW feedback, we have kept fixed all the fiducial parameters (except in the nofeedback case, where we have a lower f ¼ 0.03). Adding feedback always delays evolution, as argued in Sec. III, which makes the three realistic feedback cases delayed by Δz ∼ 3 with respect to the no-feedback case. Within the three realistic cases, however, the low feedback is the fastest, due to the smaller effect of LW radiation, followed by the high-feedback case, which is affected by LW as much as the regular case, while the vcb effect is smaller (due to the assumption of uncorrelated feedback), producing smaller halo suppression. The regular case is the most delayed. Even though the global signal does not vary significantly between the different feedback cases, the large-scale 21-cm B. Observability While the 21-cm signal due to the DM-baryon relative velocities is significant during both the EoH and the LCE, the large foregrounds make the task of observing— and characterizing—it challenging. We now forecast the sensitivity of large 21-cm observatories to the VAO signal, focusing on the upcoming Hydrogen Epoch of Reionization Array (HERA)6 [43], for which we obtain the projected power-spectrum uncertainties with the publicly available 21cmSense [93,94]. 7 This code accounts for the u‐v sensitivities of each antenna in an array and returns the errors in the 21-cm power spectrum, including cosmic variance. In addition to the instrumental noise of the antennas, one of the main obstacles to any prospective 21-cm detection is the presence of large foregrounds, from both our Galaxy and the atmosphere, that contaminate the signal. These foregrounds swamp any primordial 21-cm signal in the majority of the Fourier plane, as wave numbers with(i) Pessimistic, with a ¼ 0.1 h Mpc−1, and b given by the horizon limit, as usual. (ii) Moderate, with a ¼ 0.05 h Mpc−1, and the same b. (iii) Optimistic, with a ¼ 0, and a smaller b by a factor of sinðθFWHM=2Þ, where θFWHM is the full-width at half-maximum of the beam. Note that here we report wave numbers multiplied by the reduced Hubble constant h, as those are the inputs of 21cmSense. In all cases we assume that the sensitivity of different baselines can be added coherently, and we take a system temperature ofTsys ¼100þ120ðν=150 MHzÞ−2.55 K, as in Ref. [43]. For reference, the wedge has been experimentally observed in Ref. [100], with a and b similar to the pessimistic case, although it is likely that a better understanding of the foregrounds and the instrument will improve these parameters. We divide the observation range into four redshift bins, three of them in the EoH, centered at redshifts z ¼ 14, 16, and 18, each with a width Δz ¼ 2, and one in the LCE, centered at z ¼ 25 and with Δz ¼ 4. We use the HERA split core configuration, with 540 days of observation (corresponding to roughly three full years), and the standard settings of 21cmSense, albeit binning logarithmically in k instead of linearly to better capture the VAO shape. We calculate the signal-to-noise ratio (SNR) from each redshift slice summing simply over k bins asC. Comparison to previous work We are not the first to compute the effect of the DMbaryon streaming velocities in the 21-cm power spectrum, although we are the first to do so with both a (public) simulation code and analytical results. This has allowed us to confirm that the well-understood shape of the VAOs is indeed present in 21-cm simulations. We will now compare our results with those of previous work on this topic. On the theory side, the authors in Ref. [13] were the first to predict and calculate the velocity-induced 21-cm fluctuations due to Lyman-α pumping. In that work it was predicted that the acoustic shape of fluctuations would follow Δ2 v2 with suppression on small scales, exactly as we found in our simulations, where the window function posited by Ref. [13] provides a good fit to our data. Additionally, that work showed a vcb-induced 21-cm amplitude of δT21 ≈ 3 mK, in line with (albeit smaller than) our result of δT21 ≈ 10 mK at our peak (z ¼ 25). Additionally, in Ref. [24] the VAOs during the EoH were analytically estimated and were found to produce a 21-cm power spectrum of Δ2 21 ≈ 100 and 300 mK2 at z ¼ 15 and 20, respectively. This is in line with the results of our simulations. Note, however, that in Refs. [13,24] the nonVAO part of Δ2 21, which we parametrized as a smooth polynomial Pn, was assumed to be proportional to the density power spectrum. This is an overly simplistic assumption, which does not resemble the real 21-cm signal, as is clear from the results of Refs. [22,25,29] or our own Fig. 11. The non-VAO part of the power spectrum behaves as a shallower function of wave number k, resulting in observable VAOs even at k ∼ 0.1 Mpc−1. On the simulation side, in Ref. [25] the effect of velocities in the 21-cm signal was first implemented, albeit only applied to the x-ray heating era. In that study it was found that the relative velocities can produce 21-cm fluctuations that dominate the signal on large scales and have sharp BAO-like features, as we confirm here. Their results are comparable to our “no-feedback” case (purple line in Fig. 16), which, however, we argue is unlikely to represent reality. This code was improved upon in Ref. [22], where the effects of Lyman-Werner feedback were included. In this case we find broad agreement between their “strong-feedback” and our “regularfeedback” models, as both have the same feedback parameters, although we find larger-amplitude oscillations likely because of our homogeneous LW feedback. Finally, in Ref. [23] that work was extended to include fluctuations during the LCE, where the shape of the VAOs seems to be damped at k ≳ 0.1 Mpc−1, which we find here as well. We note two important differences between our work (based on 21cmvFAST) and that of Refs. [22,25]. First, as in 21cmvFAST we use the EPS formalism to compute the number of photons emitted from galaxies some distance R away by averaging densities and velocities (as described in Sec. II),k we have studied the effect of the dark matter–baryon relative velocities on the 21-cm line. These “streaming” velocities, sourced by the baryon acoustic oscillations, become supersonic after recombination and suppress stellar formation in molecular-cooling haloes (with masses M ≲ few × 107 M⊙), which are expected to host the first stars during cosmic dawn. We performed quasinumerical simulations with the 21cmvFAST code, which we make public. This allows us to properly include molecular-cooling haloes, as setting Tcool < 104 K in the regular 21cmvFAST will result in a severe overestimation of the number of star-forming haloes. The fluctuations in relative velocities become imprinted onto the distribution of the first stars and, through their x-ray and UV emission, onto the 21-cm line. This generates velocity-induced acoustic oscillations in the 21-cm power spectrum on large scales, turning the cosmic dawn into a novel probe of acoustic physics, as the BAOs are imprinted onto the DM-baryon relative velocities on their genesis. We emphasize that the presence of these acoustic oscillations on the 21-cm line has previously been pointed out in Refs. [13,22–25]. Here we have compared, for the first time, analytic and simulation-based results, in order to isolate the shape and size of the VAOs. We have found that, to a good approximation, the VAOs follow a wellunderstood analytic shape, regardless of astrophysical effects such as photon propagation. We exploit this result in Ref. [44], where we propose using VAOs as a standard ruler at cosmic dawn. Interestingly, here we found that VAOs are observable with the upcoming HERA interferometer under a broad range of astrophysical assumptions. This would illuminate the physics of the cosmic-dawn era, showing that molecular-cooling haloes play an important role in stellar formation. In addition, the inclusion of streaming velocities can result in sizable nonGaussianities, which we plan to explore in future work. We note that the VAOs we study here are different from the “regular” (density) BAOs, as they do not originate from small wiggles in over/underdensities but instead from Oð1Þ fluctuations in the relative velocity. The density BAOs would also affect the 21-cm signal during different cosmic eras [110–114]. Nonetheless, density fluctuations affect the cosmic-dawn 21-cm line in a complicated fashion, hampering a precise reconstruction of the matter power spectrum, so we have not included them in the smooth Pn function in Eq. (36). Additionally, while we have included the effect of Lyman-Werner feedback, which increases the minimum mass required to form stars by dissociating molecular hydrogen, there may be other feedback processes, such as chemical [115] or other radiative feedback [116–120], that may increase the halo mass beyond even atomic-cooling haloes, rendering the effects of velocities negligible. Our simulations show, however, that if the VAO signal is present, it will always have the acoustic shape that we have modeled here. To summarize, the cosmic-dawn era will provide us with a new arena to search for acoustic oscillations in the form of VAOs. Their detection is within reach of upcoming 21-cm experiments and will teach us a great deal about the haloes that hosted the first stars. Moreover, given the well-understood shape of the VAOs, they will allow us to separate cosmology from astrophysics, illuminating our knowledge about the end of the cosmic dark ages.	
2	2019	Gravitational effects on geonium and free electron gs-factor measurements in a Penning trap		Gravitational effects on geonium and free electron gs-factor measurements in a Penning trap We present a theoretical analysis of an electron confined by a Penning trap, also known as geonium, that is affected by gravity. In particular, we investigate the gravitational influence on the electron dynamics and the electromagnetic field of the trap. We consider the special case of a homogeneous gravitational field, which is represented by Rindler spacetime. In this spacetime the Hamiltonian of an electron with anomalous magnetic moment is constructed. Based on this Hamiltonian and the exact solution to Maxwell equations for the field of a Penning trap in Rindler spacetime, we derive the transition energies of geonium up to the relativistic corrections of 1=c2. These transition energies are used to obtain an extension of the well known gs-factor formula introduced by L. S. Brown and G. Gabrielse [Rev. Mod. Phys. 58, 233 1986]. I. INTRODUCTION One way to study the properties of a single electron is to analyze the trapped electron in a well-known electromagnetic field configuration. However, extracting characteristics of a free particle from transitions of a trapped one requires a deep understanding of the trapping conditions. For this purpose, commonly a Penning trap is used in modern high precision experiments [1–4]. Such a trap weakly confines the particle under usage of an electric quadrupole and a constant magnetic field. For the case of an electron, this leads to bound states with discrete energy levels [5,6]. The transitions in such an artificial atom, called geonium, are used, for example, to determine the free electron gs-factor. This quantity is a dimensionless measure of the electron’s magnetic moment μ in the unit of Bohr magnetons jμBj ¼ eℏ=ð2mÞ μ ¼ gsμB: ð1Þ While in Dirac’s theory [7] the gs-factor is gsDirac ¼ 2, in practice QED effects lead to deviations from this value. A few years ago, D. Hanneke et al. have reported high accuracy Penning trap measurements, which determine gs ¼ 2.002 319 304 361 46ð56Þ [1,2]. This experimental result is in outstanding accordance with the calculations of T. Aoyama et al. [8]. Such an interplay of theory and experiment can help to test fundamental properties of quantum field theory and to search for physics beyond the standard model, see for example [9,10] and references therein. The gs-factor experiments in Penning traps, as they are carried out by [1,2], are not performed in an isolated environment, but in the presence of the gravitational field of the Earth. This gravitational field distorts both, the electron dynamics and the electromagnetic field configuration of the trap. In this contribution, therefore, we perform the theoretical analysis of the effects of gravity on the result of Penning trap experiments. In particular, we also take into account gravitational effects on the electromagnetic field of the Penning trap, which in turn affects the motion of the electron. While gravitational effects on the bound electron gs-factor [11] and the cyclotron motion of the electron [12] have been considered, to the best of our knowledge, an analysis of the gravitational influence on Penning trap experiments have not been reported before. In order to understand, how gravity influences Penning trap experiments, it is natural to describe both, the electron and the electromagnetic field of the Penning trap, in curved spacetime. In our study, we will consider the case of a homogeneous gravitational field, which is a good approximation for gravity at the surface of the Earth, as we will discuss in Sec. II A. The Dirac Hamiltonian, which describes the dynamics of an electron with anomalous magnetic moment in this spacetime, is obtained in Sec. II B. While this Hamiltonian can be applied for any electron velocities and gravitational field strengths, we aim to use it to describe Penning trap experiments, which are performed in the non-relativistic regime. Therefore, in Sec. II C we perform a Foldy-Wouthuysen transformation to obtain the non-relativistic Hamiltonian and its 1=c2-corrections. Of course, this Hamiltonian accounts not only for gravitational effects, but also for the coupling to the electromagnetic field of a Penning trap. In Sec. III A this field is presented as an exact solution to Maxwell equations in the spacetime of homogeneous gravity. Using first order perturbation theory, we determine the eigenenergies of geonium exposed to gravity up to order 1=c2 in Sec. III B and Sec. III C. Finally, these energies are used to derive an expression for the free electron gs-factor, which generalizes the well-known results of L. S. Brown and G. Gabrielse. The summary of our results is given in Sec. IV. II. ELECTRON IN HOMOGENEOUS GRAVITATIONAL FIELD A. The homogeneous gravitational field in general relativity On Earth the biggest empirical effect of gravity is the acceleration of g ¼ 9.81 m=s2 pointing downwards. In the Newtonian theory of gravity, a vector field of constant acceleration g is a suitable approximation of the gravitational field perceived by this observer. Higher order effects, accounting for the Earth as a spherical body, can be neglected in a small environment of the observers position. The approximation of a homogeneous acceleration g also holds in general relativity in terms of the nongeodesic motion of an observer; Bound to Earth’s surface, the observer is not able to follow gravity in a free fall. In a general relativistic framework, the Newtonian gravitational field of the Earth is replaced by the famous Schwarzschild spacetime [13]. At the surface of the Earth, this spacetime can be approximated by so called Rindler spacetime [14,15], which is merely flat Minkowski spacetime, but seen by an accelerated observer. At this level of approximation, there is no spacetime curvature, but a distortion of spacetime by acceleration. In order to describe physics perceived by an accelerated observer, we start with the line element ds2 of Minkowski spacetime and perform a coordinate transformation towards a coordinate system, that describes the reference frame of the accelerated observer. The Minkowski line element, expressed in terms of Cartesian coordinates r˜ ¼ ðx;˜ y;˜ z˜Þ and proper time τ, reads ds2 ¼ ημνdxμdxν ¼ dðcτÞ2 − dx˜ 2 − dy˜ 2 − dz˜ 2; ð2Þ where we introduced the metric tensor ðημνÞ ¼ diagð1;−1;−1;−1Þ. In this sign-convention timelike distances are described by positive values of the line element ds2 > 0. Moreover, we use Einsteinian sum convention, which means that a sum is performed from 0 to 4 when paired Greek letters appear. The index 0 is set to be the index of the timelike coordinate. The coordinates of Rindler spacetime are related to the coordinates of Minkowski spacetime by As seen from Eq. (5), the line element in the accelerated frame is coordinate dependent. Therefore, the measure of time is different at different heights and the factor ð1 þ gu=c2Þ in front of the infinitesimal time step dðctÞ gives rise to the gravitational redshift [14,15]. For vanishing acceleration the Rindler line element (5) reduces to the Minkowski line element. The same holds in the ðx0 ; y0 Þplane, where we reach flat Minkowski spacetime asymptotically for u → 0. Therefore, it is legitimate to apply methods of quantum mechanics in Minkowski spacetime in a small area around the coordinate center and treat the modifications, caused by deviation from Minkowski spacetime, as corrections later. In our case this assumption is valid, since we are interested in quantum objects bound close to r0 ¼ 0, where typical length scales z0 are in the micrometer domain and, therefore, much smaller than c2=g ∼ ly, which is the typical length scale for the considered gravitational effects. B. Electron with anomalous magnetic moment in Rindler spacetime In the previous section, we introduced the spacetime of a homogeneously accelerated observer, known as Rindler spacetime. Now we want to pay particular attention to the dynamics of an electron in this spacetime. Again we start our analysis in Minkowski spacetime, where the electron motion is described by the Dirac equation [7]Here ψðxνÞ is the Dirac spinor, whose four components represent not only the electron, but also the positron in their two spin states. The Dirac matrices γμ are chosen such, that their anticommutator generates the metric tensor of the line element (2) of Minkowski spacetimeIndeed, in the case of Penning trap experiments, the electron is not only exposed to gravity, but is located in an electromagnetic field. Therefore, we go the common way to introduce a minimal coupling to the electromagnetic field by the replacement of the partial derivative ∂μ0→∂μ0þie ℏAμ0 , which brings in the four potential Aμ0 ¼ ðΦ=c;−AÞ, which contains the electric scalar potential Φ and the magnetic vector potential A. In our considerations, we will treat these potentials as classical. With these alterations, the Dirac action (9) becomeswhich now is the action for a Dirac electron, in the presence of an electromagnetic field and seen by a homogeneously accelerated observer. However, an important feature of the system is still missing in Eq. (11): the anomalous contribution to the magnetic moment of the electron. Therefore, we introduce the anomaly a, which accounts for the discrepancy between the gyromagnetic ratio gsDirac ¼ 2 of Dirac theory and the measured value gs ¼ 2ð1 þ aÞ, caused by interactions between the electron and the quantum vacuum. In order to account for this anomaly, we introduce a nonminimal coupling of the electron to the electromagnetic field strength tensor Fμ0 ν0 ¼ ∂μ0Aν0 − ∂ν0Aμ0 . With this additional term the action readswhere the commutators of the coordinate dependent Dirac matrices are the generators of local Lorentz transformations. The structure of the additional term in the action can be motivated by QED considerations [19]. The action (12) is discussed in detail for an electron in an inertial system in [5,20,21] and references therein. Naturally, we recover their cases in the limit of vanishing acceleration g. As we discussed above, these steps were performed in order to derive Dirac equation in Rindler spacetime. Since we want to give this equation in the Hamiltonian representation, we separate the spacetime coordinates C. Nonrelativistic reduction of the Hamiltonian In the last section, we derived the Hamiltonian of a relativistic spin 1=2 particle, that moves in Rindler spacetime in the presence of an electromagnetic field. In addition we introduced the anomalous magnetic moment of this particle, which accounts for the interaction of the electron with the quantum vacuum. The Hamiltonian (18), in its general form, can be applied for any velocities (v < c) of an electron. In this work, however, we concentrate on a scenario, where the electron is stored in a Penning trap in a laboratory on Earth. In this case, the velocity v ≪ c is We see, that the former odd part O appears in the commutators with W only, while the new odd part O0 is proportional to 1=c in the leading order. To further minimize the order of O0 , we can iterate the Foldy-Wouthuysen transformation until O000 ¼ Oð1=c3Þ is reached, such that where we defined the acceleration vector g ¼ ð0; 0; gÞ. In the absence of gravity, i.e., for g ¼ 0, all terms in H000 are well known and have been studied extensively, first and foremost [5,20] and references therein. The presence of gravity, however, gives rise to additional parts, which have to be discussed in more detail. As seen from Eq. (27), gravity enters this Hamiltonian at three points. (i) It gives the usual Newtonian potential, as it is known from classical mechanics, (ii) it acts as a correction to the spin orbit coupling therm and (iii) it induces a redshift of the nonrelativistic kinetic energy and the coupling term between B and s, which is important for our investigation of gravitational effects on free electron gs-factor measurements. The Hamiltonian (27) acts separately on the electronic and positronic d.o.f. Therefore, these two sectors are decoupled up to the desired order Oð1=c3Þ. The choice of sector is made by selecting the positive or negative eigenvalue of β. In our case we restrict ourselves to the discussion of the electron only, whose dynamics is described by H000 after the replacement β ¼ þ1. The Hamiltonian H000 now contains all effects on the electron caused by the homogeneous acceleration and the anomalous magnetic moment and all relativistic effects, up to the order of 1=c2. It provides two particular limits, which are well known, either in the theory of Fermions inIII. THE ELECTRON IN A PENNING TRAP A. The electromagnetic field of a Penning trap in Rindler spacetime In the previous section, we derived the Hamiltonian (27) of an electron with anomalous magnetic moment, affected by a homogeneous gravitational and an arbitrary electromagnetic field. Although, this Hamiltonian can be applied to any kind of electric and magnetic fields, we want to apply it to the particular case of the electromagnetic field of a Penning trap. Therefore we assume an ideal trap potential, without any imperfections, consisting of a static homogeneous magnetic field and a static electric quadrupole field. Moreover, the Penning trap is placed in the spacetime of an accelerated observer. Therefore, the electromagnetic field of the trap is distorted. In order to account for this distortion, we need to formulate Maxwell equations in Rindler spacetime. These equations for a sourcefree electromagnetic field in their covariant form read which determines the vector potential A ¼ ðA1; A2; A3Þ. In this expression, moreover, ∇ ¼ ð∂x0 ; ∂y0 ; ∂uÞ is the gradient in the coordinate system ðx0 ; y0 ; uÞ. In order to solve Eq. (32), one has to define explicit boundary conditions. In the case of a Penning trap configuration, for example, we demand that A is the vector potential of a constant magnetic field Bð0Þ ¼ ðB1; B2; B3Þ in the center of the trap. For this requirement, the solution of (32) is given by For vanishing acceleration g ¼ 0, the potential (36) reduces to the ideal quadrupole potential r0 · ðQˆ · r0 Þ. Having found the solutions (33) and (36) for the vector and scalar potential, we are ready now to set up the Penning trap field configuration. For this purpose, we need to specify the geometry of the trap. We adopt the coordinate system such that B2 ¼ 0 and introduce the angle θ between Bð0Þ and g. For this choice, the constants Bð0Þ and Qˆ are given by B. The electron in a Penning trap in Newtonian gravity We derived the Hamiltonian H000 of an electron with anomalous magnetic moment in an accelerated frame and an arbitrary electromagnetic field in Sec. II C. Below, we want to use this Hamiltonian to describe the electron dynamics in a Penning trap, distorted by accelerationTherefore, we insert the vector and scalar potentials (33) and (36) into Eq. (27). For the sake of brevity, we will not present this lengthy expression here, that contains all relativistic effects on both, the electron and the trap, up to the order of 1=c2. In this section, we derive the exact solution of the eigenvalue problem of the Hamiltonian in the Newtonian limit of low velocities and weak gravitational fields. Treating 1=c2-effects as first order perturbations, we find the solution of the whole eigenvalue problem of H000, afterwards. Within the Newtonian limit, the nonrelativistic Hamiltonian H0 is obtained by considering the zeroth order in an 1=c-expansion of H000, only:In this expression we dropped the auxiliary coordinate u in favour of the coordinate system r0 ¼ ðx0 ; y0 ; z0 Þ, given by Eq. (4). Since u ¼ z0 þ Oð1=cÞ, this coordinate transformation allowed us to replace the canonical momentum by π ¼ p0 þ 1 2 er0 × Bð0Þ and the electromagnetic potentials by Φ ¼ r0 · ðQˆ · r0 Þ and A ¼ −1 2 r0 × Bð0Þ . Equation (42) closely resembles the well-known Hamiltonian of a nonrelativistic electron in a Penning trap [5]. The essential difference is the presence of the Newtonian potential mg · r0 . The Hamiltonian H0 can be further simplified by performing two additional transformations. First, we rotate the coordinate system, such that the z-axis is aligned with the direction of the magnetic field Bð0Þ , see Fig. 1. This is conventional in the analysis of Penning trap experiments [5,6]. As a second step, we shift the coordinate center by a constant vector, such that it coincides with the new equilibrium position of the electronHere, due to the axial symmetry of a Penning trap, it is convenient to use cylindrical coordinates R ¼ ðρ; φ; ζÞ. The momentum operator in this coordinate system is denoted as P. In Eq. (43), moreover,are the cyclotron frequency ωc and the axial frequency ωz. The operator H˜ 0 now is the well known Hamiltonian of an electron in a Penning trap [5], except for the very last term in Eq. (43). This term describes the effect of Newtonian gravity on the electron and depends on the orientation of the Penning trap with respect to the acceleration g. Since this term is constant, we are able to solve the eigenvalue problemTogether with the axial frequency ωz, these are standard observables in Penning trap experiments. As seen from (47), Newtonian gravity leads to a constant shift of energy levels, only. This shift is independent of the quantum numbers of the electron in the trap and, therefore, does not effect frequencies of bound-bound transitions in geonium. In the next section we will see, that this is not the case if we take into account relativistic effects. C. Relativistic energy correction for a gravitationally influenced electron in a Penning trap In the previous section we considered the Hamiltonian of a nonrelativistic electron in a Penning trap in the presence of a homogeneous Newtonian gravitational field. The eigenvalues of this Hamiltonian are given by (47), while the explicit form of corresponding eigenfunctions is given in Appendix C. In this section we will use these eigenfunctions as a basis for a perturbation analysis in order to account for relativistic effects. The perturbation H˜ I of the Hamiltonian H˜ 0 can be formally written asare related to the first order nonvanishing gravitational effects. In these expressions we assumed ωc ¼ ω0 c and neglected all higher orders of ωz=ωc. In order to investigate the corrections to the gs-factor formula introduced by [5] we will use the energy correction (52) in the next section. D. Gravitational effect on free electron gs-factor measurements Having derived the energy (47) of an electron in a Penning trap and its relativistic correction (52), we are prepared to discuss the effect of gravity on the result of free electron gs-factor measurements. Therefore, we follow the steps, performed by L. S. Brown and G. Gabrielse in order to obtain the gs-factor formula, presented in [5], which in our case will contain additional corrections. In their analysis, the gs-factor is extracted by the measurement of two frequencies of transitions in geonium, namely the anomalous frequency and the reduced cyclotron frequency. The first one we obtain from the spin-flip transition between the energy levels (n ¼ 1, s¼−1=2) and (n ¼ 0, s ¼ þ1=2), see Fig. 3. By employing Eq. (47) and Eq. (52), this frequency can be calculated asIn the recent Penning trap experiments of [1], a cyclotron frequency of ωc ¼ 2π · 149 GHz is used. For such an experiment performed in a laboratory on Earth, i.e., g ¼ 9.81 m=s2, we obtain δσgs=gs ∼ 6.1 × 10−40. IV. SUMMARY AND CONCLUSION In this work we presented a theoretical investigation of an electron in a Penning trap in the presence of a gravitational field. In this system we analyzed how the presence of gravity may affect the result of free electron gs-factor measurements. Therefore, we considered a single electron with anomalous magnetic moment in the presence of electromagnetic fields in the spacetime of homogeneous acceleration. For this scenario we derived the Hamiltonian (27), which accounts for the relativistic effects up to order 1=c2. This Hamiltonian has been applied to the electron dynamics in a gravitational distorted Penning trap, whose electromagnetic field (33), (36) is given as an exact solution of Maxwell equations in Rindler spacetime. Making use of first order perturbation theory, we derived analyticalexpressions for the energy eigenvalues (47), (52) of that Hamiltonian up to order 1=c2 . A detailed analysis of these energies has shown, that Newtonian gravity only leads to constant shifts of the energy levels of geonium. Thus, Newtonian gravity has no effect on measured transition frequencies. In contrast, the relativistic effects of order 1=c2 lead to relative shifts of the energy levels. We, therefore, argue that these relativistic corrections may affect the gs-factor measurements, which rely on transitions in geonium. In order to quantify the gravitational effects, we derived the expression (62), which for g ¼ 0 recovers the known gs-factor formula introduced by L. S. Brown and G. Gabrielse, while for g ≠ 0 it predicts a shift of the measured gs-factor of δσgs=gs ∼ 6.1 × 10−40. While this can not be measured in experiments of current accuracy, it can be enhanced in the case of lower frequencies and higher accelerations and, therefore, may be important for future studies.	
3	2019	Revisiting the second post-Minkowskian eikonal and the dynamics of binary black holes	Arnau Koemans Collado ,1 Paolo Di Vecchia,2,3 and Rodolfo Russo1	Revisiting the second post-Minkowskian eikonal and the dynamics of binary black holes In this paper we study the two-body gravitational scattering of massive scalars with different masses in general spacetime dimensions. We focus on the Regge limit (eikonal regime) of the resulting scattering amplitudes and discuss how to extract the classical information representing the scattering of two black holes. We derive the leading eikonal and explicitly show the resummation of the first leading energy contribution up to second order in Newton’s gravitational constant. We also calculate the subleading eikonal showing that in general spacetime dimensions it receives a nontrivial contribution from the box integral. From the eikonal we extract the two-body classical scattering angle between the two black holes up to the second post-Minkowskian order. Taking various probe-limits of the two-body scattering angles we are able to show agreement between our results and various results in the literature. We highlight that the box integral also has a log-divergent (in energy) contribution at subsubleading order which violates perturbative unitarity in the ultrarelativistic limit. We expect this term to play a role in the calculation of the eikonal at the third post-Minkowskian order. I. INTRODUCTION The high energy limit of scattering amplitudes in gravitational theories has been thoroughly studied as a gedanken-experiment that provides a nontrivial test of the consistency of the gravitational theory. A particularly tractable regime is the Regge limit, where both the energies and the impact parameter are large and unitarity is preserved due to a resummation of Feynman diagrams which reproduces the effect of a classical geometry [1–4]. These early studies focused on the case of external massless states whose high energy Regge scattering matches the gravitational interaction of two well-separated shock waves. However it is possible to generalize the same approach to the scattering of massive states [5] where the large center of mass energy is due to both the kinetic and rest mass energy. It is then possible to interpolate between the ultra-relativistic Regge scattering mentioned above and the study of the nonrelativistic large distance interaction between massive objects. This can be done both for pure general relativity (GR) as well as for string theory; see for instance [6] for the analysis of the scattering of a perturbative massless state off a d-brane which we recall is a massive object.1 The technique of deriving the relativistic interaction of two massive objects from an amplitude approach has recently attracted renewed attention [9–16] since it links directly to the post-Minkowskian approximation of the classical gravitational dynamics relevant for the inspiraling phase of binary black hole systems [17–20]. The amplitude approach to the relativistic two-body problem can be stated in the following conceptually simple way. Consider 2 → 2 scattering where the external states have the quantum numbers necessary to describe the classical objects one is interested in (massless states describe shock waves, massive scalars can describe Schwarzschild black holes, then spin and charge can be added to describe Kerr2 and Reissner-Nordström black holes). Then the limit is taken where the Newton’s gravitational constant GN is small, but all classical parameters, such as the Schwarzschild radius or the classical angular momentum, are kept finite. Since in this paper we only classical parameter in the problem is the effective Schwarzschild radius, RD−3 s ∼ GNM, where M is the largest mass scale in the process. We can have M ¼ ffiffi s p in the ultrarelativistic/massless case or M ¼ m1 in the probelimit with m2 1 ≫ ðs − m2 1Þ, m2 2. In either case the relevant kinematic regime is the Regge limit, since the center of mass energy ffiffi s p has to be much larger than the momentum transferred ffiffiffiffi jtj p . Since GN is small, one might think that the perturbative diagrams with graviton exchanges yield directly the effective two-body potential, but one must be careful in performing this step. In the limit mentioned above the perturbative amplitude at a fixed order in GN is divergent thus creating tension with unitarity. These divergent terms should exponentiate when resumming the leading contributions at a large energy at different orders in GN. This exponential, called the eikonal phase,3 is the observable that we wish to calculate and that, as we will see, contains the relevant information for the two-body potential. In this paper we will focus on the 2 → 2 scattering of massive scalar particles [5,10,11,13] up to order G2 N [i.e., second post-Minkowskian (2PM) level]. Here we keep the spacetime dimension D general, which serves as an infrared regulator, and also consider the subleading OðG2 NÞ contributions that do not directly enter in the 2PM classical interaction but that should be relevant for the third postMinkowskian (3PM) result [16]. Since our analysis is D-dimensional we cannot apply the standard 4D spinorhelicity description, but we construct the relevant parts of the amplitudes with one and two graviton exchanges by using an approach similar in spirit where tree-level amplitudes are glued together [28,29]. The scaling limit discussed above can be spelled out for this case as follows: (i) We take GN small by keeping GNM fixed, and we are interested in the nonanalytic contributions as t → 0 since they determine the large distance interaction. (ii) The ratios m2 i =s, where m1;2 are the masses of the external scalars, can be arbitrary; when they are fixed, one is describing the scattering of two Schwarzschild black holes, but it is possible to smoothly take them to be small or large and make contact with different relativistic regimes. (iii) At each order in Gn N the terms that grow faster than E1 or E2 (at large Ei and fixed GNM) should not provide new data, but just exponentiate the energy divergent contributions at lower perturbative orders. (iv) The terms that grow as Ei provide a new contribution to the eikonal phase at order Gn N from which one can derive the contribution to the classical two-body deflection angle and from it the relevant information on the nPM effective two-body potential. We carry out this approach explicitly up to the 2PM order. The D-dimensional case is slightly more intricate than the 4D one as we find that the contribution from the scalar box integral not only contributes to the exponentiation of the first post-Minkowskian (1PM) result, but also yields nontrivial subleading terms that have to be combined with the triangle contributions to obtain the full 2PM eikonal. We also see that our result smoothly interpolates between the general, the light-bending [when m2 1 ≫ ðs − m2 1Þ ≫ m2 2] and the ultrarelativistic cases (when s ≫ m2 1, m2 2); this holds not just for the classical part of the 2PM eikonal phase, which is trivially zero in the massless case, but also for the quantum part [30,31]. This feature does not seem to be realised in the recent 3PM result [16], and it would be interesting to understand this issue better. The paper is structured as follows. In Sec. II we introduce the basic objects needed for our analysis, i.e., the tree-level on shell vertices between two massive scalars and one and two gravitons. The field theory limit of a string expression provides a rather simple D-dimensional expression that we use to derive the relevant part of the amplitude with two graviton exchanges. We then extract the box and the triangle contributions that determine the 2PM eikonal phase. In Sec. III we discuss the exponentiation pattern mentioned above and obtain explicit expressions for the 1PM and 2PM D-dimensional eikonal. As a check we derive the deflection angle in various probe-limits where it is possible to compare with a geodesic calculation in the metric of an appropriate black hole finding perfect agreement. Section IV contains a brief discussion on the possible relevance of our result for the study of the 3PM eikonal. In the two Appendixes we provide the technical results needed in Secs. II and III; in Appendix A we evaluate the box and the triangle integrals in the limit s, m2 i ≫ jtj, while in Appendix B we derive the deflection angle through a classical geodesic calculation in the background of a D-dimensional Schwarzschild black hole. II. MASSIVE SCALAR SCATTERING In this section we focus on the 2 → 2 gravitational scattering process between two massive scalars in D spacetime dimensions with both one and two graviton exchanges. As mentioned in the Introduction, we are interested in extracting the classical contributions to this process, so instead of calculating the full amplitude by using the standard Feynman rules, we glue on shell building blocks that capture just the unitarity cuts needed for reconstructing the classical eikonal. While this approach is by now commonly used in a D ¼ 4 setup, it is possible to implement it in general D, and here we follow [28], now including mass terms for the scalars.A. One graviton exchange Using the gluing procedure outlined above we can calculate the tree-level four-point massive scalar scattering by gluing two amplitudes (2.1) with a de Donder propagator (2.4) and obtainIn the high energy limit and after moving into impact parameter space (as defined below) we can see that this contribution grows as Ei (since GNM is constant) and violates perturbative unitarity at large energies, we will come back to this point when discussing the two graviton exchange amplitude as well as in Sec. III. By construction, this result just captures the pole contribution in t of the amplitude, but this is sufficient to extract the classical interaction between two well separated particles. This is more clearly seen by transforming the amplitude to impact parameter space. As is standard in the discussion of the eikonal phase, we introduce an auxiliary (D-2)-dimensionalB. Two graviton exchanges In this subsection we discuss the gluing procedure at one-loop illustrated in Fig. 2. Schematically we haveFor two graviton exchanges we have two amplitude topologies that contribute; the box and triangle integrals, which are shown in Fig. 3. The masses can be of the same order or much smaller than the center of mass energy, and of course the integrals take different forms in these two cases. In Appendix A we focus on the case s ∼ m2 i and evaluate the first terms in the high energy expansion (2.16) for the box and triangle integrals. In the ultrarelativistic case one recovers the massless results that can be found for instance in [32]. 1. Box contribution From the procedure outlined at the start of this subsection we find the following expression for the numerator, N □, of the box diagram contribution to the two graviton exchange amplitude:At large energies this result scales as Ei exactly as A1. This contribution should be exponentiated by the first subleading terms in the energy expansion of the higher loop contributions and so provides a new contribution to the eikonal phase. In impact parameter space (2.23) becomesThus, for D > 4 there is a contribution to the eikonal from the box integral which becomes trivial in the four-dimensional case. In general, this contribution is crucial in order to match, in the probe-limit, with the geodesic calculations as discussed in Sec. III and in [28] for the massless case m1 ≫ 0, m2 ¼ 0. The subsubleading contributions to the box diagram are naively expected to be finite in the limit described by (2.16), but there is actually a log-divergent term in the amplitude, as discussed for the massless case in [30,31]; see also [33,34] for an explicit evaluation of the same 2 → 2 one-loop process with external gravitons. This contribution comes from using (2.18) and the next order in the expansion of the box integral, which in our case yieldsBy using arcsinh y ¼ logðy þ ffiffiffiffiffiffiffiffiffiffiffiffiffi y2 þ 1 p Þ in Eq. (2.25) we can see that the second term on the second line and the term on the last line are log-divergent at large energies. It is interesting to highlight the following points. First, the same arcsinh-function arising from this subsubleading contribution also appears in the recent 3PM result [16]. Then these terms violate perturbative unitarity in the s=m2 i → ∞ limit and [30] conjectured that they should resum to provide a quantum correction to the eikonal phase. This contribution is relevant in the discussion of the Reggeization of the graviton; for a recent discussion see [35] and references therein. Finally the contribution (2.27) provides an additional imaginary part to A˜ 2 beside that coming from the leading term (2.22). In [30], it was shown that this subleading imaginary part vanishes in the D ¼ 4 massless case. Since the last term in the second line vanishes in D ¼ 4, here we find through a direct calculation that the same result holds also for the scattering of massive scalars. We will briefly come back to these points in Sec. IV. 2. Triangle contribution Following the procedure outlined at the beginning of this subsection we find that the expression for the numerator, N △, for the trianglelike contributions, with the m1 massive scalar propagator, is given bywhere we have already neglected some terms which are subleading in the limit given by (2.16) (i.e., do not contribute classically at second post-Minkowskian order). As we have done before we can express this in terms of an integral basis in which the expression (2.28) becomesThe results in this subsection agree with results for D ¼ 4 found in [11,13,19]. We have not considered the subleading trianglelike contribution explicitly in this subsection because we have found that it does not contribute to the log-divergent terms we discuss in Secs. II B 1 and IV. This should be clear from the results for the various integrals in Appendix A 2. Note also that these subleading contributions do not produce contributions to the real part of iA2. III. THE EIKONAL AND TWO-BODY DEFLECTION ANGLES In this section we summarize general expressions for the eikonal and the deflection angle for the case of two different masses. We then discuss explicit expressions for these quantities using the amplitudes derived in Sec. II. We also discuss various probe-limits for both general D and D ¼ 4 in order to compare with existing results in the literature. We will start by defining what the eikonal phase is in the context we are considering in this paper. We recall from Sec. II that the amplitudes in impact parameter space are defined via,where GN is the usual Newton’s constant. This relates the discussion presented here with the so-called postMinkowskian approximation discussed in [13,18,19] and references therein. The leading eikonal corresponds to the 1PM order in the post-Minkowskian expansion, the subleading eikonal corresponds to the 2PM order and so on. Using the various relations shown above and the results from Sec. II we can write the leading (1PM) and first subleading eikonals (2PM). Using Eq. (2.7) we find for the leading eikonal,spacelike momentum in the center of mass frame of the two scattering particles. A. Various probe limits in arbitrary D The corresponding deflection angle for the leading eikonal is given byIn order to compare with the more general results for timelike geodesics in a D-dimensional Schwarzschild background obtained in Sec. B we can also take the timelike probe-limit. In this limit we have as before m1 ¼ M ≫ m2 where m2 ¼ m ≠ 0, so we have ffiffi s p ∼ M and ðs − m2 1 − m2 2Þ ∼ 2E2M. Using this we findWe can easily check, by using the relation J ≃ jpjjbj, that this agrees with Eqs. (B14) and (B16) as expected. B. Various probe limits in D = 4 We will now set D ¼ 4 in the various equations obtained in the previous subsection. We find that the leading eikonal in D ¼ 4 is equal to where we have used the definition of the Schwarzschild radius given in (3.12). This agrees with Eq. (B18). We can similarly look at the timelike probe-limit described before (3.14). In this case we find that (3.15) becomes known expression for the leading contribution to the deflection angle of a Schwarzschild black hole reproduced in (B20). Taking the timelike probe-limit of (3.20) as described in the previous subsecThe factor of m1 þ m2 in front implies that the subleading eikonal in the massless limit is vanishing [30],This also implies that there is no contribution of order 1=b2 to the deflection angle which is consistent with the result found in the previous subsection that this contribution is zero for any number of spacetime dimensions. From Eq. (3.25) we can compute the deflection angle,IV. DISCUSSION In this paper we have studied the classical gravitational interaction between two massive scalars in D-dimensions up to 2PM order. As usual the spacetime dimension can be used as an infrared regulator and physical observables, such as the deflection angle discussed in Sec. III have a smooth D → 4 limit. The structure of the D-dimensional result is in some aspects richer than the one found in D ¼ 4. For instance the box integral provides not only the contribution necessary to exponentiate the leading energy behavior of the tree-level diagram, but also a new genuine contribution to the subleading classical eikonal; see (2.24). The box integral also provides a subsubleading contribution (2.25) that for D ≠ 4 has a new imaginary part, while its real part has a structure which also appears in the OðG3 NÞ amplitude presented in [16]. In the ultrarelativistic limit s ≫ m2 i , this contribution is log-divergent and, if itwhere we have taken the limit s ≫ m2 i in order to compare with Eq. (5.18) of6 [30] and λP is the Planck length. By restoring the factors of ℏ we can see from (3.19) that δð1Þ =ℏ is dimensionless and is therefore the combination that is exponentiated. On the contrary, δ ð2Þ q is dimensionless without the need for any factor of ℏ [see the first expression in (4.1) or equivalently it can be written in terms of λ2 P if we extract a factor of 1=ℏ], which highlights its quantum nature. An interesting feature of the 2PM eikonal phase for massive scalars that we have obtained is that its ultrarelativistic limit smoothly reproduces the massless result up to 2PM order. This is valid also for the quantum contribution mentioned above (4.1). By comparing the results of [30] and [16], the same property does not seem to hold in the 3PM case, and it would be very interesting to understand the origin of this mismatch. Another interesting development would be to generalize the analytic bootstrap approach of [30,31] beyond the massless D ¼ 4 case. In that approach the quantum part of the eikonal δ ð2Þ q plays an important role in the derivation of the subsequent classical PM order, and we expect that a similar pattern is valid also beyond the setup of [30,31]. This approach has the potential to provide an independent derivation of the 3PM eikonal phase both in the massless higher dimensional case and in the physically interesting case of the massive scattering in D ¼ 4.	
4	2019	Light-cone modular bootstrap and pure gravity	Nathan Benjamin , Hirosi Ooguri, Shu-Heng Shao , and Yifan Wang	Light-cone modular bootstrap and pure gravity We explore the large spin spectrum in two-dimensional conformal field theories with a finite twist gap, using the modular bootstrap in the light-cone limit. By recursively solving the modular crossing equations associated with different PSLð2; ZÞ elements, we identify the universal contribution to the density of large spin states from the vacuum in the dual channel. Our result takes the form of a sum over PSLð2; ZÞ elements, whose leading term generalizes the usual Cardy formula to a wider regime. Rather curiously, the contribution to the density of states from the vacuum becomes negative in a specific limit, which can be canceled by that from a nonvacuum Virasoro primary whose twist is no bigger than c−1 16 . This suggests a new upper bound of c−1 16 on the twist gap in any c > 1 compact, unitary conformal field theory with a vacuum, which would in particular imply that pure AdS3 gravity does not exist. We confirm this negative density of states in the pure gravity partition function by Maloney, Witten, and Keller. We generalize our discussion to theories with N ¼ ð1; 1Þ supersymmetry and find similar results. I. INTRODUCTION Despite progress in the classification program of rational conformal field theories, we have shockingly little understanding of the general landscape of two-dimensional (2D) conformal field theories (CFTs). For example, there is no explicit construction of any 2D compact, unitary CFT with central charge c > 1 and no Virasoro conserved currents of any spin [1]. Such CFTs are expected to be generic, and our ignorance of them clearly shows the limitation in our understanding. To go beyond the realm of rational CFTs, we will consider CFTs with a finite twist gap. The twist of an operator is defined as Δ − jjj ¼ 2 minðh; h¯Þ. Theories with conserved currents, such as rational CFTs, necessarily have a vanishing twist gap.1 For this reason we will think of the twist gap as a measure on how irrational a CFT is. In this paper, we will address the following two general questions for CFTs with a finite twist gap: (1) Is there a universal Cardy-like growth for the large spin states? (2) At a fixed central charge c, how large can the twist gap be? These two questions are tied together by modular invariance of the torus partition function. We start with the first question. We generalize Cardy’s argument for the asymptotic growth of states with large scaling dimensions [2]. More specifically, we analytically continue the torus moduli τ, τ¯ to two independent complex variables, and consider the limit ImðτÞ → 0 while keeping τ¯ fixed. This is similar to the light-cone limit studied in the conformal bootstrap of four-point functions [3,4], but now applied to the modular bootstrap program [5]. By solving the modular crossing equations in the light-cone limit with arbitrary rational real parts of τ, τ¯, 2 we recursively identifythe universal contribution from the Virasoro vacuum multiplet to the density of large spin states for any c > 1 CFTwith a finite twist gap. Our formula generalizes the usual Cardy formula from the regime h; h¯ ≫ c to h ≫ c but with h¯ − c−1 24 > 0 finite. The universal density of states takes the form of a sum over PSLð2; ZÞ images,3 whose leading term is the extended Cardy formula discussed recently in [6–8]. The density of states depends on the number-theoretic properties of the spin j ¼ h − h¯ and is in particular nonanalytic in j. Now we turn to the second question. Rather curiously, our density of states from the vacuum contribution becomes negative in the double limit where j → ∞ and h¯ − c−1 24 → 0. Such negative density of states of course should not be present in a physical, unitary CFT. This negativity can be canceled by the contribution from a nonvacuum primary operator of twist Δ − jjj at or below c−1 16 in the dual channel. We are therefore led to the following tentative conclusion: any compact, unitary CFT with a PSLð2;CÞ invariant vacuum must have a twist gap of at most c−1 16 . Our argument is not yet rigorous, and we will discuss the gaps to complete the proof. If true, our result improves the earlier c−1 12 bound on the twist gap by Hartman and [1]. 4 Via the holographic correspondence [9], our result has interesting implications on pure Einstein gravity as a quantum gravity theory in AdS3. In the strictest sense, pure AdS3 gravity is dual to a 2D large c, unitary CFT where all nonvacuum Virasoro primary operators have h; h¯ ≥ c−1 24 and are interpreted as Bañados-TeitelboimZanelli (BTZ) black holes. The new twist gap bound c−1 16 suggested by our argument would imply that pure AdS3 gravity does not exist.5 Indeed, we will check explicitly that the pure gravity partition function computed by MaloneyWitten-Keller [12,13] agrees with our formula in the specific double limit mentioned above. The sum over the PSLð2; ZÞ elements in our formula is identified as a sum over geometries in AdS3. 6 In particular, we confirm that the pure gravity partition function has an identical negative density of states in this limit. This gives another interpretation of our result: while the pure gravity partition function of [12,13] is unphysical in various ways, it approximates the universal density of large spin states dictated by the vacuum state in the dual channel of the modular crossing equation. In other words, the pure gravity partition function of Maloney-Witten-Keller is the analog of double-twist operators in d > 2 [3,4], or of “Virasoro mean field theory” in 2D [7] for the modular bootstrap (see also [16]). The paper is organized as follows. In Sec. II we review the argument by Hartman and [1] for the c−1 12 bound on the twist gap. In Sec. III, the extended Cardy formula for the density of large spin states is reviewed. In Sec. IV, we generalize the extended Cardy formula to include subleading corrections by solving recursively the crossing equations associated with general elements of PSLð2; ZÞ. In Sec. V, the implications of this universal density of large spin states are discussed, which suggest that the twist gap in any compact, unitary c > 1 CFT can be at most c−1 16 . Section VI discusses the interpretation of our result in relation to the pure AdS3 gravity partition function. In Sec. VII, we discuss the N ¼ ð1; 1Þ supersymmetric generalization. The Appendixes A and B describe some technical steps needed in solving the crossing equations. Appendix C discusses some subtleties present when there is an accumulation of operators in twist. In Appendix D, we record the modular crossing kernels for more general elements of PSLð2; ZÞ. II. WARM-UP: THE c − 1 12 TWIST GAP In this section we review an argument by Hartman and [1] showing that the twist gap in any compact unitary 2D CFT has to be no larger than c−1 12 . This argument has been generalized from the Virasoro algebra to the WN algebra in [17]. Consider the partition function Zðq; q¯Þ of a 2D CFT on a torus with complex structure moduli q ¼ expð2πiτÞ; q¯ ¼ expð−2πiτ¯Þ. We will analytically continue so that τ and τ¯ are two independent complex variables. Let us parametrize the torus moduli asThe torus partition function can be expanded in Virasoro characters. For a 2D unitary CFT with c > 1, the possible modules of the Virasoro algebra are the degenerate module, i.e., the vacuum module h ¼ 0, and a continuous family of nondegenerate modules labeled by a positive conformal weight h > 0. Their Virasoro characters are given byhere vac ≡ð1 − qÞð1 − q¯Þq−c−1 24 q¯−c−1 24 . We have used ηðq0 Þ ¼ ffiffiffiffi β 2π q ηðqÞ. The  are contributions from the nonvacuum operators in the cross channel. As we take β → 0 (but keep β¯ finite), the divergence on the right-hand side (RHS) has to be reproduced by an infinite number of states on the left-hand side (LHS). Let us further simplify the LHS of (2.11) in the β → 0 limit. First we can drop the vacuum term since any individual term does not give a divergence as β → 0. We then write the LHS asUsing (2.13), we now prove the twist gap 2tgap cannot be larger than c−1 12 . Let us assume otherwise, i.e., 2tgap > c−1 12 . We multiply both sides by eβ¯ðtgap−c−1 24 Þ . Then the LHS has a negative β¯ derivative, but the β¯ derivative of the RHS will eventually be positive for large enough β¯ (while still keeping β¯ ≪ 1=β) due to the exponential growth of the factor eβ¯ðtgap−c−1 24 Þ . We therefore arrive at a contradiction. III. EXTENDED CARDY FORMULA In this section, we will review the derivation in [6–8] of the universal spectrum of large spin Virasoro primaries for all c > 1 2D CFTs with nonzero twist gap, i.e., 2tgap > 0. We will argue that the physical density of states ρjðh¯Þ in the large spin j ≫ c limit is universally approximated byand similarly for the holomorphic (left-moving) characters. This equation will be crucial for our crossing solution. The argument leading to the extended Cardy formula (3.1) is similar to the original argument by Cardy, but now in the light-cone limit where β → 0 while β¯ is held fixed. This leads to the crossing equation (2.13), where the divergence on the RHS needs to be reproduced by a certain asymptotic growth of states with large spin. Below we show that (3.1) is indeed a solution to the crossing equation. Plugging in the solution (3.1) into (2.13), the LHS becomesleading divergent terms in the β → 0 limit on the RHS of (2.13). Therefore, Eq. (3.1) is indeed a solution to the crossing equation and gives the universal density of large spin states in any compact unitary CFT with a finite twist gap, up to an error that grows slower than exp ð4π ffiffiffiffiffiffiffiffiffi c−1 24 j q Þ in the large spin limit. On the other hand, ρ0 j;1 ¼ 0 if h <¯ c−1 24 , meaning that there is no exponential growth in the large spin limit. Let us comment on the corrections to (3.1) in the large spin limit. First, there are error terms from approximating the discrete spectrum by a continuous density of states. This error for the density of states ρðΔÞ that is insensitive to the spin has recently been quantified in [19,20] (see also Appendix C of [21]). Second, there are contributions to the density of states from the lowest twist, nonvacuum primary operators, corresponding to the last term in (2.13). Last, there are contributions coming from the vacuum but for different elements of PSLð2; ZÞ. The last two corrections will be discussed in later sections. We emphasize that the converse of our statement here is also true, which follows simply from running our argument backwards. Namely, if a 2D CFT has a large spin spectrum that satisfies (3.1), it is guaranteed to have a nonzero twist gap 2tgap > 0. It would be interesting if there is a holographic interpretation of (3.1) for h¯ not in the Cardy regime, in terms of the entropy of BTZ black holes. If so, it may suggest the theories holographically dual to Einstein gravity in AdS3 generically have a nonzero twist gap. IV. PSLð2;ZÞ MODULAR CROSSING EQUATIONS In this section we will repeat the analyses in Secs. II and III, but with a more general PSLð2; ZÞ transformation, and we will find qualitatively new behavior. A. Crossing Equation We now repeat the previous analysis but instead take7B. Solution to the crossing equations The universal density of states in (3.1) does not successfully reproduce the RHS of (4.6). In particular, if we plug (3.1) into (4.6), the LHS in the β → 0 is finite and fails to provide the divergence exp ð4π2 s2β c−1 24 Þ on the RHS of the equation. See Appendix B for detailed derivations.extended Cardy formula (3.1), which grows as exp ð4π ffiffiffiffiffiffiffiffiffi c−1 24 j q Þ in the large spin limit. The higher s0 terms grow as exp ð4π s0 ffiffiffiffiffiffiffiffiffi c−1 24 j q Þ and are subleading corrections to (3.1). Let us comment on the sum in s0 . The solution presented above is designed to reproduce the divergence on the RHS of the crossing equation (4.6) as β → 0. However, the divergence on the RHS is present only if s ≲ 1= ffiffiffi β p . The divergence, if present, controls the density of states whose spins are of the order j ∼ 1=β due to the suppression factor e−βj on the LHS of (4.6). It follows that for a fixed large spin j, we can trust the solution only if s ≲ ffiffi j p ; therefore the sum in (4.11) should be truncated before order ffiffi j p . For a fixed s, we show in Appendix B that the modular crossing equations labeled by r with gcdðr; sÞ ¼ 1 are solved by the term ρj;sðh¯ þ j; h¯Þ in the sum in (4.11). In particular, we show that, to leading order in β → 0, the terms with s0 ≠ s do not contribute to the modular crossing equations for any r at a fixed s. Just as in Sec. III, where the density of states for the extended Cardy formula can be interpreted as a product of modular kernels for the S transformation, the density of states we derived in this section can also be interpreted as modular kernels for more general PSLð2; ZÞ transformations. In Appendix D, we present simplified expressions for some of these modular kernels.V. TWIST GAP REVISITED Let us examine the solution (4.11) to the crossing equation. In the large spin limit j ≫ c, the leading terms areThe density of states of a physical CFT receives correction to the solution ρ0 j ðh¯Þ from various sources. One obvious correction comes from the lowest twist 2tgap operator in the dual channel. Let the conformal weights of this lowest twist operator be ðtgap; h¯ gapÞ. 9 Repeating the same argument in Sec. III, this lowest twist operator contributes to the density of states byIf we take the inverse Laplace transform of the RHS of (5.7), we would obtain the first two terms of (4.11); a twist gap of c−1 16 or below would cure the negativity in this density.11 In [1], the authors found a bootstrap upper bound on the twist gap 2tgap that is numerically close to the analytic bound c−1 12 reviewed in Sec. II. A simple partition function that saturates this bound comes from the c ≥ 1 Liouville theoryh ¼ h¯ ¼ c−1 24 primary. A notable difference of this example from the Liouville case is that the spectrum of primaries is discrete and includes all spins. At first sight, this seems to imply that one cannot lower the twist gap below c−1 12 , which is in tension with our suggested twist gap c−1 16 . However, recall that in our argument, it is crucial that there is a normalizable vacuum with h ¼ h¯ ¼ 0 in the spectrum for us to perform the light-cone bootstrap. Neither the Liouville partition function nor the shifted compact boson partition function contains a vacuum state; therefore they need not obey the constraints we derived. Similarly in the numerical modular bootstrap analysis, it is difficult to impose the condition that there is a normalizable vacuum in the spectrum. We therefore predict that the functionals found in [1] would have zeros above the twist gap that “coalesce.” In other words, as the truncation order in the derivative increases, the zeros become denser and denser rather than approach a fixed spectrum. Interestingly, unlike in [23], the limit as the truncation order goes to infinity does not produce a nontrivial “extremal functional.” In the previous paragraph, we argued that the limit would produce a functional that vanishes for all integer spins with twist at least c−1 12 . However, in Sec. VI, we will review a construction by [12,13] that inputs any single state with twist below c−1 12 and produces a modular invariant function by adding states all with twist at least c−1 12 . Since the proposed extremal functional would vanish on the crossing equation for this modular invariant function, it must in addition vanish on all states with twist below c−1 12 . Therefore the extremal functional approaches zero as the truncation order goes to infinity. The twist 2t ¼ 2 minðh; h¯Þ ¼ c−1 16 has also appeared in other contexts. In [7], the authors introduced the notion of the Virasoro mean field theory, defined as the inversion of the vacuum Virasoro block for the sphere four-point function. While the Virasoro mean field theory by itself does not give a consistent four-point function of a physical theory, it approximates the large spin CFT data of any compact, unitary 2D CFT with nonzero twist gap. For identical external operators with conformal weight ðh; h¯Þ, the authors show that the associated Virasoro mean field theory spectrum is qualitatively different for h > c−1 32 versus h < c−1 32 : In the former case, the spectrum consists only of a continuum above h > c−1 24 , while in the latter case, there are in addition a discrete set of primaries.12 Indeed, the fourpoint function of identical scalar primaries with scaling dimension Δ ¼ c−1 16 is special. The four-point sphere conformal block with external scalar scaling dimension c−1 16 and internal scalar scaling dimension c−1 12 is a simple power of jzj; j1 − zj, and is self-crossing invariant by itself [see, e.g., (3.15) of [24] ]. Furthermore, it has been shown in [25] that c−1 16 is the minimal external scalar scaling dimension for a four-point function with only internal scalar primaries.13 This value of the twist has also appeared in the discussion of the Renyi entropy after a local quench [6,8]. VI. PURE GRAVITY The twist gap that we have proposed has very interesting implications for two-dimensional CFTs holographically dual to large-radius Einstein gravity in AdS3. Recall that the classical BTZ black hole has mass, M, and angular momentum, j, related to the CFT conformal dimensions h and h¯ via [26] M ¼ 1 lAdS  h þ h¯ − c 12 ; j ¼ h − h¯ ð6:1Þ in the large c limit. In particular, classical BTZ black holes have MlAdS ≥ jjj, which implies h; h¯ ≥ c 24 þ Oð1Þ. There have been attempts to formulate a “pure” theory of quantum gravity in AdS3 [12,13]. Via AdS=CFT, the strictest definition of a pure theory of gravity is a 2D unitary CFT at large c where all nonvacuum Virasoro primary operators can be interpreted as BTZ black holes.14 The bound (5.5) on the twist gap suggested by our argument would imply that no such CFT exists. In this section we check that the pure gravity partition function computed in [12,13] indeed has a negative density of states in the regime (5.2), confirming our general argument. Our main result (4.11) is the universal contribution from the vacuum character in the dual channel to the large spin density of states. Its expression is very reminiscent of the sum over geometries in the calculation of the partition function of pure AdS3 gravity [12,13]. This is not a coincidence. We will further show that in the limit h¯ − c−1 24 → 0 and j ≫ c, the pure gravity partition function matches our ρ0 j ðh¯Þ (4.11). We therefore reach an importantconclusion: Even though the pure gravity partition function derived in [12,13] has various unphysical properties, it is the universal contribution from the vacuum character to the density of large spin states in any CFT with a finite twist gap. A. Maloney-Witten-Keller partition function The Maloney-Witten-Keller (MWK) partition function [12,13] is computed by starting with the vacuum Virasoro characterA. Maloney-Witten-Keller partition function The Maloney-Witten-Keller (MWK) partition function [12,13] is computed by starting with the vacuum Virasoro character χ0ðqÞχ0ðq¯Þ ¼ 1 jηðqÞj2 ðq−c−1 24 q¯−c−1 24 − q−c−1 24 þ1q¯−c−1 24 − q−c−1 24 q¯−c−1 24 þ1 þ q−c−1 24 þ1q¯−c−1 24 þ1Þ; ð6:2Þ and adding its PSLð2; ZÞ images. The sum is divergent, and a certain regularization is required to make the answer finite. The MWK partition function has the following features: (i) It has a unique vacuum, and all other Virasoro primaries have h; h¯ ≥ c−1 24 . (ii) The spectrum contains a continuum of states with integer spins. (iii) The density of states is not always positive. In particular, the degeneracy of the state with h ¼ h¯ ¼ c−1 24 is −6. Since the MWK partition function has no nonvacuum state with twist below c−1 16 , our argument in Sec. V suggests that the density of states must turn negative in the regime (5.2), in addition to the known negativity at h ¼ h¯ ¼ c−1 24 . We will show that this is exactly the case. Below we review the density of states for the MWK partition function. Instead of using the spin j and the twist 2t, we will follow the convention in [13] and use the variables e and j defined asThis gives the subleading term in ðh¯ − c−1 24 Þ when plugged into ρjðeÞ. In particular, this accounts for the leading nonzero contribution at s ¼ 1. We have also confirmed our prediction (4.11) numerically with the MWK density of states. In [13], the authors show that the density of states is positive if we fix e and j, and then take c to be large. Here we uncover the negative density of states in a different regime (5.2) where both e and j are taken to be much larger than c. This new regime of negative density of states makes it more challenging to correct the MWK partition function to a unitary, physical partition function. To cancel the negative density of states in the regime (5.2) of the MWK partition function without ruining modular invariance, one tentative candidate is to add N copies of the PSLð2; ZÞ sum of the state ðh; h¯Þ¼ðc−1 32 ; c−1 32 Þ above the vacuum. The “seed” term of this addition to theVII. SUPERSYMMETRIC GENERALIZATION Our arguments in this paper can be generalized to 2D CFTs with any chiral algebra. In this section we will perform a similar analysis for the N ¼ 1 super-Virasoro algebra. Recall that there are four partition functions we can consider, depending on the four spin structures on the torus, which correspond to (anti)periodic boundary conditions of the fermions in the space and time directions.19 These correspond to partition functions restricted to the NeveuSchwarz or Ramond sectors, and with or without a ð−1ÞF insertion. Three of these partition functions are related by PSLð2; ZÞ transformations, and the remaining one is the Witten index. In this section we will focus on the partition function with antiperiodic boundary conditions for the fermions in both directions on the torus, namely	
5	2019	Search for the isotropic stochastic background using data from Advanced LIGO’s second observing run		Search for the isotropic stochastic background using data from Advanced LIGO’s second observing run The stochastic gravitational-wave background is a superposition of sources that are either too weak or too numerous to detect individually. In this study, we present the results from a cross-correlation analysis on data from Advanced LIGO’s second observing run (O2), which we combine with the results of the first observing run (O1). We do not find evidence for a stochastic background, so we place upper limits on the normalized energy density in gravitational waves at the 95% credible level of ΩGW < 6.0 × 10−8 for a frequency-independent (flat) background and ΩGW < 4.8 × 10−8 at 25 Hz for a background of compact binary coalescences. The upper limit improves over the O1 result by a factor of 2.8. Additionally, we place upper limits on the energy density in an isotropic background of scalar- and vector-polarized gravitational waves, and we discuss the implication of these results for models of compact binaries and cosmic string backgrounds. Finally, we present a conservative estimate of the correlated broadband noise due to the magnetic Schumann resonances in O2, based on magnetometer measurements at both the LIGO Hanford and LIGO Livingston observatories. We find that correlated noise is well below the O2 sensitivity. I. INTRODUCTION A superposition of gravitational waves from many astrophysical and cosmological sources creates a stochastic gravitational-wave background (SGWB). Sources which may contribute to the stochastic background include compact binary coalescences [1–8], core collapse supernovae [9–14], neutron stars [15–24], stellar core collapse [25,26], cosmic strings [27–31], primordial black holes [32–34], superradiance of axion clouds around black holes [35–38], and gravitational waves produced during inflation [39–47]. A particularly promising source is the stochastic background from compact binary coalescences, especially in light of the detections of one binary neutron star and ten binary black hole mergers [48–55] by the Advanced LIGO detector, installed in the Laser Interferometer Gravitational-wave Observatory (LIGO) [56], and by Advanced Virgo [57] so far. Measurements of the rate of binary black hole and binary neutron star mergers imply that the stochastic background may be large enough to detect with the Advanced LIGO-Virgo detector network [58,59]. The stochastic background is expected to be dominated by compact binaries at redshifts inaccessible to direct searches for gravitationalwave events [60]. Additionally, a detection of the stochastic background would enable a model-independent test of general relativity by discerning the polarization of gravitational waves [61,62]. Because general relativity predicts only two tensor polarizations for gravitational waves, any detection of alternative polarizations would imply a modification to our current understanding of gravity [63–65]. For recent reviews on relevant data analysis methods, see Refs. [66,67]. In this paper, we present a search for an isotropic stochastic background using data from Advanced LIGO’s second observing run (O2). As in previous LIGO and Virgo analyses, this search is based on cross-correlating the strain data between pairs of gravitational-wave detectors [68,69]. We first review the stochastic search methodology and then describe the data and data quality cuts. As we do not find evidence for the stochastic background, we place upper limits on the possible amplitude of an isotropic stochastic background as well as limits on the presence of alternative gravitational-wave polarizations. Upper limits on anisotropic stochastic backgrounds are given in a publication that is a companion to this one [70]. We then give updated forecasts of the sensitivities of future stochastic searches and discuss the implications of our current results for the detection of the stochastic background from compact binaries and cosmic strings. Finally, we present estimates of the correlated noise in the LIGO detectors due to magnetic Schumann resonances[71] and discuss mitigation strategies that are being pursued for future observing runs. II. METHOD The isotropic stochastic background can be described in terms of the energy density per logarithmic frequency intervalwhere dρGW is the energy density in gravitational waves in the frequency interval from f to f þ df and ρc ¼ 3H2 0c2=ð8πGÞ is the critical energy density required for a spatially flat universe. Throughout this work, we will use the value of the Hubble constant measured by the Planck satellite, H0 ¼ 67.9 km s−1 Mpc−1 [72]. We use the optimal search for a stationary, Gaussian, unpolarized, and isotropic stochastic background, which is the cross-correlation search [66,67,73,74] (however, see Ref. [75]). For two detectors, we define a cross-correlation statistic CˆðfÞ in every frequency binwhere s˜iðfÞ is the Fourier transform of the strain time series in detector i ¼ f1; 2g, T is the segment duration used to compute the Fourier transform, and S0ðfÞ is the spectral shape for an ΩGW ¼ const background given byThe quantity γTðfÞ is the normalized overlap reduction function for tensor (T) polarizations [73], which encodes the geometry of the detectors and acts as a transfer function between strain cross-power and ΩGWðfÞ. Equation (2) has been normalized so that the expectation value of CˆðfÞ is equal to the energy density in each frequency binwhere P1;2ðfÞ are the one-sided noise power spectral densities of the two detectors and Δf is the frequency resolution, which we take to be 1=32 Hz. An optimal estimator can be constructed for a model of any spectral shape by taking a weighted combination of the cross-correlation statistics across different frequency bins fk,III. DATA We analyze data from Advanced LIGO’s second observing run, which took place from 16∶00:00 UTC on November 30, 2016 to 22∶00:00 UTC on August 25, 2017. We crosscorrelate the strain data measured by the two Advanced LIGO detectors, located in Hanford, Washington, and Livingston, Louisiana, in the United States [56]. Linearly coupled noise has been removed from the strain time series at Hanford and Livingston using Wiener filtering [79,80]; see also Refs. [81–83]. By comparing coherence spectra and narrowband estimators formed with and without Wiener filtering, we additionally verified that this noise subtraction scheme does not introduce correlated artifacts into the Hanford and Livingston data. Virgo does not have a significant impact on the sensitivity of the stochastic search in O2 because of the larger detector noise, the fact that less than one month of coincident integration time is available, and that fact that the overlap reduction function is smaller for the Hanford-Virgo and Livingston-Virgo pairs than for Hanford-Livingston. Therefore, we do not include Virgo data in the O2 analysis. The raw strain data are recorded at 16,384 Hz. We first downsample the strain time series to 4096 Hz and apply a 16th-order high-pass Butterworth filter with knee frequency of 11 Hz to avoid spectral leakage from the noise power spectrum below 20 Hz. Next, we apply a Fourier transform to segments with a duration of 192 s, using 50% overlapping Hann windows, and then we coarse grain six frequency bins to obtain a frequency resolution of 1=32 Hz. As in Ref. [68], we observe in the band 20–1726 Hz. The maximum frequency of 1726 Hz is chosen to avoid aliasing effects after downsampling the data. Next, we apply a series of data quality cuts that remove non-Gaussian features of the data. We remove times when the detectors are known to be unsuitable for science results [84] and times associated with known gravitational-wave events [55]. We also remove times where the noise is nonstationary, following the procedure described in the supplement of Ref. [69] (see also Ref. [68]). These cuts remove 16% of the coincident time, which is in principle suitable for data analysis, leading to a coincident live time of 99 days. In the frequency domain, we remove narrowband coherent lines that are determined to have instrumental or environmental causes, using the methods described in Ref. [85]. These cuts remove 15% of the total observing band, but only 4% of the band below 300 Hz, where the isotropic search is most sensitive. The narrow frequency binning of 1=32 Hz was needed to cut out a comb of coherent lines found at integer frequencies. A list of notch filters corresponding to lines which were removed from the analysis is also available on the public data release page [86]. IV. O2 RESULTS In Fig. 1, we plot the observed cross-correlation spectrum CˆðfÞ and uncertainty σðfÞ obtained from Advanced LIGO’s O2 run. We only plot the spectrum up to 100 Hz to focus on the most sensitive part of the frequency band. These data are also publicly available on the webpage [86] and can be used to search for stochastic backgrounds of any spectral shape. We perform several tests that the cross-correlation spectrum is consistent with uncorrelated Gaussian noise. The χ2 per degree of freedom for the observed spectrum is 0.94. The loudest individual frequency bin is 51.53 Hz, with a signal-to-noise ratio CðfÞ=σðfÞ of 4.2. With a total of 46,227 (un-notched) frequency bins, there is a 71% probability that random Gaussian noise would yield an equally loud bin. In Table I, we list the broadband point estimates and 1σ uncertainties obtained from the O2 data when assuming power laws with α ¼ 0, 2=3, and 3. Given the uncertainties, uncorrelated Gaussian noise would produce point estimates at least this large with probability 30%, 22%, and 21%, respectively. We conclude there is not sufficient evidence to claim detection of the stochastic background. V. UPPER LIMITS ON ISOTROPIC STOCHASTIC BACKGROUND Since we do not find evidence for the stochastic background, we place upper limits on the amplitude Ωref. We use the parameter estimation framework described in Refs. [61,62,76], applied to the cross-correlation spectrum obtained by combining the results from O1 given in Ref. [68] with those from O2 which are described above (please see the Supplemental Material [87] for more details). We present results assuming two priors, one which is uniform in Ωref and one which is uniform in log Ωref. We additionally marginalize over detector calibration uncertainties [88]. In O2, we assume 2.6% and 3.85% amplitude uncertainties in Hanford and Livingston, respectively [89,90]. In O1, the calibration uncertainty for Hanford was 4.8% and for Livingston was 5.4% [89]. Phase calibration uncertainty is negligible.VI. IMPLICATIONS FOR COMPACT BINARY BACKGROUND In Fig. 3, we show the prediction of the astrophysical stochastic background from binary black holes (BBHs) and binary neutron stars (BNSs), along with its statistical uncertainty due to Poisson uncertainties in the local binary merger rate. We plot the upper limit allowed from adding the background from neutron star–black hole (NSBH) binaries as a dotted line. We use the same binary formation and evolution scenario to compute the stochastic background from BBH and BNS as in Ref. [59], but we have updated the mass distributions and rates to be consistent with the most recent results given in Refs. [55,91]. For NSBHs, we use the same evolution with redshift as BNSs. As in Refs. [54], for BBHs, we include inspiral, merger, and ringdown contributions computed in Ref. [92], while for NSBH and BNSs, we use only the inspiral part of the waveform. For the BBH mass distribution, we assume a power law in the primary mass pðm1Þ ∝ m−2.3 1 with the secondary mass drawn from a uniform distribution, subject to the constraints 5 M⊙ ≤ m2 ≤ m1 ≤ 50 M⊙. In Ref. [55], rate estimates were computed by two pipelines, PyCBC [93] and GstLAL [94]. We use the merger rate measured by GstLAL, Rlocal ¼ 56þ44 −27 Gpc−3 yr−1 [55], because it gives a more conservative (smaller) rate estimate. Using the methods described in Ref. [59], the inferred amplitude of the stochastic background is ΩBBHð25 HzÞ ¼ 5.3þ4.2 −2.5 × 10−10. For the BNS mass distribution, following the analysis in Ref. [55], we take each component mass to be drawn from a Gaussian distribution with a mean of 1.33 M⊙ and a standard deviation of 0.09 M⊙. We use the GstLAL rate of Rlocal ¼ 920þ2220 −790 Gpc−3 yr−1 [55]. From these inputs, we predict ΩBNSð25 HzÞ ¼ 3.6þ8.4 −3.1 × 10−10. Combining the BBH and BNS results yields a prediction for the total SGWB of ΩBBHþBNSð25 HzÞ ¼ 8.9þ12.6 −5.6 × 10−10. This value is about a factor of 2 smaller the one in Ref. [59], due in part to the decrease in the rate measured after analyzing O1 and O2 data with the best available sensitivity and data analysis techniques. For NSBH, we assume a delta function mass distribution, where the neutron star has a mass of 1.4 M⊙ and the black hole has a mass of 10 M⊙, and we take the upper limit on the rate from GstLAL [55]. The upper limit from NSBH is ΩNSBHð25 HzÞ ¼ 9.1 × 10−10. We show the sum of the upper limit of ΩNSBHðfÞ, with the 90% upper limit on ΩBBHþBNSðfÞ, as a dotted line in Fig. 3. We also show the power law–integrated (PI) curves [96] of the O1 and O2 isotropic background searches. A powerlaw stochastic background that is tangent to a PI curve is detectable with SNR ¼ 2 by the given search. We additionally show a projected PI curve based on operatingAdvanced LIGO and Advanced Virgo at design sensitivity for 2 years, with 50% network duty cycle. By design sensitivity, we refer to a noise curve which is determined by fundamental noise sources. We use the Advanced LIGO design sensitivity projection given in Ref. [95], which incorporates improved measurements of coating thermal noise relative to the one assumed in Ref. [58]. This updated curve introduces additional broadband noise at low frequencies relative to previous estimates. As a result, the updated design-sensitivity PI curve is less sensitive than the one shown in Ref. [58]. VII. IMPLICATIONS FOR COSMIC STRING MODELS Cosmic strings [97,98] are linear topological defects which are expected to be generically produced within the context of grand unified theories [99]. The dynamics of a cosmic string network is driven by the formation of loops and the emission of gravitational waves [100,101]. One may therefore use the stochastic background in order to constrain the parameters of a cosmic string network. We will focus on Nambu-Goto strings [102,103], for which the string thickness is zero and the intercommutation probability equals unity. Gravitational waves will allow us to constrain the string tension Gμ=c2, where μ denotes the mass per unit length. This dimensionless parameter is the single quantity that characterizes a Nambu-Goto string network. We will consider two analytic models of cosmic string loop distributions [104,105]. The former [104] gives the distribution of string loops of given size at fixed time, under the assumption that the momentum dependence of the loop production function is weak. The latter [105] is based on a different numerical simulation [106] and gives the distribution of non–self intersecting loops at a given time [107]. The corresponding limits found by combining O1 and O2 data are Gμ=c2 ≤ 1.1 × 10−6 for the model of Ref.[104] and Gμ=c2 ≤ 2.1 × 10−14 for the model of Ref. [105]. The Advanced LIGO constraints are stronger for the model of Ref.[105] because the predicted spectrum is larger at 100 Hz for that model. This can be compared with the pulsar timing limits, Gμ=c2 ≤ 1.6 × 10−11 and Gμ=c2 ≤ 6.2 × 10−12, respectively [108]. VIII. TEST OF GENERAL RELATIVITY Alternative theories of gravity generically predict the presence of vector or scalar gravitational-wave polarizations in addition to the standard tensor polarizations allowed in general relativity. Detection of the stochastic background would allow for direct measurement of its polarization content, enabling new tests of general relativity [61,62]. When allowing for the presence of alternative gravitational-wave polarizations, the expectation value of the cross-correlation statistic becomesIX. ESTIMATE OF CORRELATED MAGNETIC NOISE Coherent noise between gravitational-wave interferometers may be introduced by terrestrial sources such as Schumann resonances, which are global electromagnetic modes of the cavity formed by the Earth’s surface and ionosphere [71]. These fields have very long coherence lengths [110] and can magnetically couple to the gravitational-wave channel and lead to broadband noise that is coherent between different gravitational-wave detectors. As the detectors become more sensitive, eventually this source of correlated noise may become visible to the crosscorrelation search and, if not treated carefully, will bias the analysis by appearing as an apparent stochastic background. Unlike the lines and combs discussed in Ref. [85], we cannot simply remove affected frequency bins from the analysis because Schumann noise is broadband. Here, we estimate the level of correlated electromagnetic noise (from Schumann resonances or other sources) in O2 following Refs. [68,111,112]. We first measure the crosspower spectral density M12ðfÞ between two Bartington Model MAG-03MC magnetometers [113] installed at Hanford and Livingston. We then estimate the transfer function TiðfÞ (i ¼ f1; 2g) between the magnetometer channel and the gravitational-wave channel at each site, as described in Ref. [114]. Finally, we combine these results to produce an estimate for the amount of correlated magnetic noise, which we express in terms of an effective gravitational-wave energy density ΩmagðfÞ, ΩmagðfÞ ¼ jT1ðfÞjjT2ðfÞjRe½M12ðfÞ γTðfÞS0ðfÞ : ð10Þ We show ΩmagðfÞ in Fig. 4, alongside the measured O1 +O2 PI curve and the projected design-sensitivity PI curve. The trend for the magnetic noise lies significantly below the O1+O2 PI curve, indicating that correlated magnetic noise is more than an order of magnitude below the sensitivity curve in O2, although it may be an issue for future runs. Experimental improvements can mitigate this risk by further reducing the coupling of correlated noise. From O1 to O2, for instance, the magnetic coupling was reduced by approximately an order of magnitude, as indicated by the dotted and dot-dashed curves in Fig. 4. Additionally, work is ongoing to develop Wiener filtering to subtract Schumann noise [110,112,115] and to develop a parameter estimation framework to measure or place upper limits on the level of magnetic contamination [116]. This work will take advantage of low noise LEMI-120 magnetometers [117] that were recently installed at both LIGO sites, as described in the Supplemental Material [87]. X. CONCLUSIONS We have presented the results of a cross-correlation search for the isotropic stochastic background using data from Advanced LIGO’s first and second observing runs. While we did not find evidence for the stochastic background, we obtain the most sensitive upper limits to date in the approximately 20–100 Hz frequency band. We have also placed improved upper limits on the existence of a stochastic background from vector and scalar-polarized gravitational waves. While the upper limits on the SGWB presented in this work are the strongest direct limits in the frequency band of current ground-based gravitational-wave detectors, other observations place stronger constraints in other frequency bands. The NANOGrav Collaboration has reported the 95% upper limit of ΩGW < 7.4 × 10−10 at a frequency of 1 yr−1 after marginalizing over uncertainty in the Solar system ephemeris [118]. Combining data from the Planck satellite and the BICEP2/Keck array constrains the tensorto-scalar ratio from the cosmic microwave background to be r < 0.064 at 95% confidence at comoving scales of k ¼ 0.002 Mpc−1, corresponding to a gravitational-wave frequency of f0.002 ¼ ð2πÞ−1ck ¼ 3.1 × 10−18 Hz [119], assuming the single field slow roll consistency condition. Using Eq. (4) of Ref. [108], this can be converted into the constraint ΩGWðfÞ ≤ 3.2 × 10−16 × ðf=f0.05Þ−r=8½16=9þ f2 eq=ð2f2Þ, where feq is the frequency of a gravitational wave of which the wavelength was the size of the Universe at matter-radiation equality and f0.05 is the pivot scale. Combining constraints at different frequency ranges can probe models which span many orders of magnitude in frequency [108,119]. While we have targeted an isotropic, stationary, and Gaussian background, other search techniques can probe backgrounds that violate one or more of these assumptions. Upper limits on an anisotropic gravitational-wave background from O1 were presented in Ref. [120]. Furthermore, non-Gaussian searches targeting the compact binary stochastic background are currently being developed [121–124]. A successful detection of the stochastic background by any of these approaches would offer a new probe of the gravitational-wave sky. The supporting data for this paper are openly available via the LIGO Document Control Center (DCC) [86].	
6	2019	Origin and impacts of the first cosmic rays	Yutaka Ohira 1 and Kohta Murase	Origin and impacts of the first cosmic rays Nonthermal phenomena are ubiquitous in the Universe, and cosmic rays (CRs) play various roles in different environments. When, where, and how are CRs first generated since the big bang? We argue that blast waves from the first cosmic explosions at z ∼ 20 lead to Weibel mediated nonrelativistic shocks and CRs can be generated by the diffusive shock acceleration mechanism. We show that protons are accelerated at least up to sub-GeV energies, and the fast velocity component of supernova ejecta is likely to allow CRs to achieve a few GeV in energy. We discuss other possible accelerators of the first CRs, including accretion shocks due to the cosmological structure formation. These CRs can play various roles in the early Universe, such as the ionization and heating of gas, the generation of magnetic fields, and feedbacks on the galaxy formation.I. INTRODUCTION In the current Universe, high-energy nonthermal particles are ubiquitous at various scales from the Earth to clusters of galaxies. Cosmic rays (CRs) provide one of the best examples. The energy density of CRs is about 1 eV cm−3 in our Galaxy, which is comparable to that of thermal particles. Therefore, CRs are thought to have important roles in galaxies. It is widely accepted that blast waves of supernova remnants are the origin of CRs with energies up to 1015 eV and the standard acceleration mechanism is diffusive shock acceleration (DSA) [1]. In fact, radio, x-ray and gamma-ray observations of supernova remnants have shown the evidence that electrons and ions are accelerated to highly relativistic energies [2]. Furthermore, a CR precursor ahead of the shock front, which is a prediction of the diffusive shock acceleration, was directly imaged [3]. The CRs could even affect outflow dynamics of galaxies in the halo region, and their energy density is comparable to that of the warm-hot intergalactic medium [4]. It has not been studied when, where, and how the first population of CRs are generated since the big bang. These questions are, in other words, relevant for us to reveal the history of the nonthermal Universe and their roles in highredshift environments. The CRs in the early universe can heat the intergalactic medium, and future HI 21-cm observations could shed light on the beginning of the nonthermal universe [5]. Reference [5] showed that CRs with energies of a few tens MeV most efficiently heat the intergalactic medium, but so far, we do not understand whether the first CRs can be accelerated to the energy scale or not. In addition, in order for CRs to heat the intergalactic medium, CRs have to escape from the acceleration site. In the early universe, there are two energetic shocks that could accelerate the first population of CRs, supernova blast waves of the first stars and accretion shocks of the large scale structure formation in the universe. In this work, we consider particle accelerations by the two types of shocks, showing that the first CRs are provided by Weibel mediated nonrelativistic collisionless shocks driven by the first star explosion at z ≈ 20. We use the notation Qa;x ¼ Qa=10x in centimeter-gram-second units. II. SUPERNOVA BLAST WAVES OF THE FIRST STARS AND COLLISIONLESS SHOCKS Several cosmological simulations show that the first stars can be formed in dark matter halos with halo masses of Mh ∼ 106 M⊙ at z ≈ 20 [6]. The first stars are expected to be more massive than the Sun [7] but may have a wide range of masses, 10 M⊙ ≲ M ≲ 103 M⊙ [8]. Because their lifetime is shorter than the Hubble time at the age of the Universe, the first stars gravitationally collapse at z ∼ 5–20. During their lives, the first stars emit a lot of ultraviolet photons ionizing surrounding gas, forming HIIregions. The number density, temperature, and ionization fraction of the HII region are typically n ∼ 1 cm−3 , T ∼ 1 eV, and fi ¼ 1, respectively, just before the first stars end their lives [9]. After the core collapse of the first stars, their envelopes may be ejected as supernovae. According to a recent cosmological simulation [8], about 30% of the first stars may directly collapse to black holes and the remaining ∼70% explode as normal core-collapse, pair-instability, and pulsational pair-instability supernovae. In this work, we mainly consider normal core-collapse supernovae from the first stars as the sources of the first CRs, and the others are discussed later. Supernova ejecta consist of the inner core with a shallow density profile and the outer envelope with a steep density profile. The bulk of the ejecta has a velocity of uej ≃ 4.5 × 108 cm s−1E1=2 SN;51M−1=2 ej;34 , where ESN and Mej are explosion energy and ejected mass, respectively. The shock velocity is almost constant, ush ≈ uej, until the sweptup mass becomes comparable to the ejecta mass, t ≲ tdec ¼ ð3Mej=4πmpnÞ1=3=uej, where mp and n are the proton mass and number density in the HII region. However, in the earliest phase of the SN expansion, the outer region of the ejecta drives a faster shock with ush ≫ uej [10]. Details depend on mass-loss mechanisms of the first stars, which could be caused by the pulsational instability and perhaps rotationally induced chemical mixing [11]. Assuming a wind density profile of ϱcs ¼ DR−2 with an outer ejecta profile ϱej ∝ t −3ðR=tÞ−δ (where δ ¼ 10 for a radiative envelope with a convective core), we obtain ush ≃ 3.3 × 109 cm s−1E7=16 SN;51M−5=16 ej;34 D−1=8 12 t −1=8 4 [10]. Here we take a fiducial value for blue supergiants, D ∼ 1012 g cm−1 [12]. If the mass loss is small, the fast shock initially propagates in the wind region, and subsequently propagates in the HII region. Thereafter, the transition to the homologous expansion occurs, and finally after t>tdec, the shock velocity decreases with time as ush ∝ t −3=5. To understand whether particles are accelerated by the shock or not, we first investigate what types of collisionless shock and magnetic field turbulence are generated. Magnetic fields in the surrounding HII region of the first stars are poorly constrained. The Biermann battery effect around the first stars generates magnetic fields of ∼10−17 G in the 102–103 kpc scales at z ≈ 20 [13]. Since the field strength is too small to affect the collisionless shock, the HII region can be treated as an unmagnetized plasma. Then, some of the downstream hot plasma leaks to the shock upstream region. Collisionless plasma instabilities are excited between the upstream plasma and the leaking plasma. As a result, the upstream cold flow is dissipated by electromagnetic fields generated by the collisionless plasma instabilities and a collisionless shock is formed. Note that the shock propagating inside a star is radiation mediated rather than collisionless, and the radiation pressure is important even after the shock breakout. However, collisionless shocks should eventually form when the shock propagates in a wind environment [14], and then the first and earliest CRs will be produced. Then, collisionless shocks of supernova remnants will propagate in the HII region. Since numerical simulations show that unmagnetized shocks with βsh ¼ ush=c ≳ 10−1 are the nonrelativistic Weibel mediated shock [15], the shock driven by the fast-envelope ejecta is so. However, unmagnetized shocks with βsh ≈ 10−2 are poorly understood. As a first step, we consider evolution of two counterstreaming electron-proton plasmas where the relative velocity is βsh ∼ 10−2 and the temperature of each plasma is T ∼ 1 eV. Since the electron mass is smaller than the proton mass, instabilities caused by the two electron beams grow initially. If the relative velocity (or shock velocity) is a nonrelativistic velocity, growth rates of the electron and ion Weibel instabilities are given by γ ∼ βshωpe and γ ∼ βshωpp [16], and the growth rate of the electron two-stream instability is given by γ ∼ ωpe [17]. ωpe and ωpp are the electron and proton plasma frequencies. Since the electron two-stream instability is the most unstable mode, the two electron beams initially excite electrostatic fields. If only protons leak to the upstream region from the downstream region, instead of the electron two-stream instability, the Buneman instability becomes the most unstable mode and generates electrostatic fields [18]. As a result, only electrons are heated, but protons remain cold beams because the frequency of excited waves is close to the electron plasma frequency. The initial electrostatic instabilities are saturated when the electron thermal velocity becomes comparable to the relative velocity between the two beams. Thus, the electrons are heated to Te ∼ meu2 sh, where me is the electron mass [19]. In a such plasma, the ion acoustic instability, ion-ion twostream instability, and the ion Weibel instability are unstable. Since the ion-ion two-stream instability has the largest growth rate of γ ∼ ωpp [19], the electrostatic fluctuations are excited and protons are heated to Tp ∼ Te ∼ meu2 sh [19]. Thereby, the ion acoustic instability and the ion-ion two-stream instability are stabilized but the ion Weibel instability is still unstable. Most of the kinetic energy of the proton beams are not dissipated by the early electrostatic instabilities. Then, the ion Weibel instability finally generates magnetic field fluctuations and dissipates the proton beams. Therefore, collisionless shocks driven by the core ejecta is also nonrelativistic Weibel mediated shocks. III. ACCELERATION OF THE FIRST CRs Ab initio plasma simulations have shown that relativistic Weibel mediated shocks can accelerate particles by DSA [20]. Therefore, we can expect that first supernova remnant shocks first accelerate CRs at z ≈ 20. We here derive theacceleration timescale of DSA in the nonrelativistic Weibel mediated shock. The acceleration timescale of DSA is given by tacc ¼ 20κ=u2 sh [21], where κ is the diffusion coefficient of the accelerated particle, and we assume that the downstream diffusion coefficient is the same as the upstream one. The ion Weibel instability generates magnetic field fluctuations, δB, with the coherent length scale of the proton inertial length, λδB ¼ αc=ωpp, where α is a numerical factor. The nonlinear evolution of the Weibel instability has been widely discussed theoretically and investigated by numerical simulations [22]. Very recently, it is shown that the drift kink instability at λδB ≈ 10 c=ωpp stops the nonlinear growth of the magnetic field fluctuations [23]. In this work, we adopt α ¼ 10 as a fiducial value. The magnetic field strength around the nonrelativistic collisionless shock is subject to more uncertainties. To estimate the acceleration timescale, we introduce a magnetic field energy fraction, ϵB ¼ δB2=ð4πnmpu2 shÞ. The gyroradius of particles with a momentum of p is represented by rg ¼ λδBp=ˆ ðαβshϵ 1=2 B Þ, where pˆ ¼ p=ðmpcÞ. If the momentum is larger than mpushαϵ 1=2 B , the gyroradius is larger than the coherent length scale of magnetic field fluctuations. For Weibel mediated shocks, because ϵB is smaller than about 10−2 as argued later, most protons thermalized by the shock satisfy this condition. Then, the diffusion coefficient is given by κ ≈ 2πvr2 g=λδB [24], where v is the particle velocity. Then, the acceleration timescale of DSA in the nonrelativistic Weibel mediated shock is given bywhere the subscript s ¼ e; p represents particle species. The diffusion length around the shock, κ=ush, depends on the energy of accelerated particles. We naively expect that ϵB is not constant in space because the number flux density of leaking protons, fleak, decreases with the distance from the shock and consequently the electric current that generates magnetic field fluctuations decreases as well as fleak. From the Biot-Savart law, one can estimate the magnetic field strength by δB=λδB ≈ 4πefleak=c. In order to estimate the number flux density of protons, fleak, we introduce the CR proton energy flux fraction, ηp ¼ fleakðγp − 1Þmpc2=ðnmpu3 sh=2Þ, where γp is the Lorentz factor of leaking protons. In the current Universe, ηp is expected to be about 0.1 to supply galactic CRs by supernova remnants [25]. Then, the magnetic field energy fraction in the diffusion region of protons with the momentum of pˆ ¼ ðγ2 p − 1Þ1=2 is represented by ϵB ¼ fαfleak=ðnushÞg2 ¼ α2η2 pβ4 sh=f4ðγp − 1Þ2g. The above estimate is based on the CR current in theThe maximum energy of accelerated particles is decided by a finite acceleration time, a finite size of the acceleration region, or cooling [27]. Since the acceleration timescale for Weibel mediated shocks strongly depends on the shock velocity, tacc ∝ u−8 sh , particles are most efficiently accelerated during the free expansion phase t ≲ tdec. The timelimited maximum energy is obtained from the condition tacc;s ¼ tdec, which is almost the same as the size-limited maximum energy for the case of supernova remnants at t ¼ tdec [27]. At the deceleration time of the core ejecta, the size-limited maximum energies are given byFor the above parameters, both protons and electrons are accelerated to the relativistic energies, and cooling is negligible. Therefore, for first core-collapse supernovae with the above parameters, CRs are accelerated to a few GeV and a few hundreds of MeV by the fast-envelope and core ejecta, respectively. Since the shock velocity decreases with time after the deceleration time of the core ejecta, the size or time-limited maximum energy also decreases with time. The time evolution is given byThe cooling time of protons due to Coulomb losses is given by tcool ≈ 1.5 × 1015 sec n−1 ;0 pˆ 3 . For the parameters adopted at Eqs. (5) and (6), the maximum energy of protons is limited by the Coulomb loss for t ≥ tc ≃ 29tdec and the time evolution is given by Emax;p ∼ 2.9 MeVðt=tcÞ−12=5 . Once the maximum energy is limited by cooling (t>tc), newly accelerated particles cannot escape from the acceleration site. Therefore, the first supernova remnants can scatter CR protons in the energy ranges 3 MeV ≲ E ≲ 3 GeV to the intergalactic medium as long as the downstream diffusion coefficient is not much larger than the upstream one. They emit gamma rays and produce light nuclei by nuclear interactions, which would affect something and be observable [28]. Since magnetic field fluctuations are generated by the Weibel instability driven by the accelerated protons, the size of the acceleration region is limited by the diffusion length of protons with the maximum energy, κ=ush. For t ≥ tc, the maximum energy of electrons is limited not by the Coulomb loss but by the size of the diffusion region. In that case, the maximum energy of electrons is obtained from the condition, pˆ 2 max;e ¼ pˆ 3 max;p, and one can obtain Emax;e ∼ 21 MeVðt=tcÞ−9=5. All the electrons accelerated by the first supernova remnants can escape from the first supernova remnants and heat the intergalactic medium. IV. OTHER POSSIBLE ACCELERATORS OF THE FIRST CRs Here, we discuss the CR acceleration by explosion phenomena except the normal core-collapse supernova. For pulsational pair-instability supernovae, stellar mass of Mej ∼ 1034 g, is ejected with ESN ∼ 1051 erg before the collapse. The remaining stars with M ∼ 1035 g finally explode as core-collapse supernovae with ESN ∼ 1051 erg and Mej ∼ 1034–1035 g [29]. For pair-instability supernovae, all the stellar mass, M ∼ 2 × 1035 g, is ejected with ESN ∼ 1053 erg. About 30% of the first stars are expected to collapse to black holes directly, which can eject a small mass because neutrino cooling during the protoneutron star phase makes the gravitational mass of the core small. The expected ejecta mass and explosion energy are Mej ∼ 1030 g and ESN ∼ 1047 erg for massive stars in the current universe [30]. From Eqs. (2) and (3), the pulsational explosion of pulsational pair-instability supernovae, the final explosion of pulsational pair-instability supernovae, pair-instability supernovae, and the direct collapse of first stars accelerates CR protons to ∼130 MeV, ∼17–130 MeV, ∼1.0 GeV, and ∼57 MeV, respectively. It should be noted that the kinetic energy of some supernova ejecta are dissipated not in the HII region but in the stellar wind and ejecta that might be sufficiently magnetized. If so, the shock would be a magnetized collisionless shock and CRs would be accelerated to higher energies like CRs in the current universe. If first stars explode as a gamma-ray burst or form blackhole binaries, relativistic jets would be produced, resulting in relativistic collisionless shocks and CR acceleration to higher energies [31]. However, whether the relativistic jets are produced by the collapse of first stars and black-hole binaries in the early universe and whether magnetic fields are sufficiently generated in the stellar wind and ejecta are open issues. Therefore, the maximum energy of first CRs could tell us the magnetic field strength around the first star. A. Acceleration of CRs associated with the earliest cosmological structure formation Large-scale structures naturally serve as CR reservoirs, and accretion shocks have been considered as one of the high-energy accelerators in these environments [32]. Strong accretion shocks are expected for local massive clusters, while the shocks are expected to be weaker at higher redshifts. The physical situation should largely be different in the early universe because typical halos are smaller and the intergalactic medium is less ionized. Halo masses that can collapse at z ¼ 20 within the 3-sigma fluctuations is about Mh ∼ 106 M⊙ [33]. The velocity of the accretion shock is about the virial velocity, ush ≈vvir ¼ 6.8×105 cm s−1M1=3 h;39fð1þzÞ=20g1=2. In order to accelerate charged particles by the shock, about a half of matters around the accretion shock has to be ionized [34]. Since most of the matters in the universe are not ionized at z ≈ 20, ionization by radiation from the shocked region is important for the particle acceleration. However, the shock velocity has to be larger than about 107 cm=s in order to ionize the upstream matter sufficiently [35]. Therefore, accretion shocks due to the cosmological structure formation at z ≈ 20 are unlikely to accelerate particles to high energies. At z ≲ 10, halos with Mh ∼ 1010 M⊙ can collapse and the velocity of the accretion shock is about 107 cm s−1 [33]. The ionization fraction and temperature of the shockupstream region are fi ∼ 0.2u2 sh;7 and T ∼ 1 eV [35]. The photoionization precursor length is 1020 cm n−1 ;−1 [36]. In the spherical top-hat halo model, the baryon number density is given by n ∼ 18π2n¯ b ¼ 4.2 × 10−2 cm−3fð1 þ zÞ=10g3 when the halo is virialized, where n¯ b is the mean baryon number density of the Universe at the virialization time. Magnetic fields in the upstream region of the accretion shock would be amplified by the turbulent dynamo or CR streaming because accretion flows would be very complicated and the first CRs have already accelerated at z ≈ 20. However, since the above magnetic-field amplification and generation are still open issues, here we assume that the accreting plasma is unmagnetized. Then, since the thermal velocity of electrons, vth;e ¼ 4.2 × 107 cm s−1ðT=1 eVÞ1=2, is larger than the shock velocity of the accretion shock, only the Weibel instability is unstable in the collisionless shock transition region. Therefore, the accretion shock at z ≲ 10 is also the Weibel mediated nonrelativistic shock and can accelerate particles by DSA. The acceleration timescale is given by Eq. (3) or Eq. (4). For the accretion shock at z ≲ 10, the maximum energy of accelerated protons is nonrelativistic and limited by cooling due to the Coulomb loss. Then, the cooling-limited maximum energy is obtained from the condition tacc;p ¼ tcool:Since the acceleration of protons is not limited by the size of the acceleration region, most accelerated protons cannot escape to the upstream region. Even though the accelerated protons escape to the upstream region of the accretion shock, they are quickly thermalized and cannot heat the far upstream region of the accretion shock. The free streaming distance during the cooling time of ionization is ∼2 kpc½ð1 þ zÞ=21 −3ðE=330 keVÞ2 [5], which is much smaller than that of protons accelerated by the first supernova remnants. For electrons, the maximum energy is limited by the size of the diffusion length of accelerated protons, where the cooling due to the ionization loss is negligible for electrons. Then, the maximum energy of electrons is obtained from the condition, pˆ 2 max;e ¼ pˆ 3 max;p:V. IMPACTS ON COSMOLOGICAL EVOLUTION Our study has several important implications for the cosmological evolution of galaxies and intergalactic medium. First, low-energy CRs may impact the heating and ionization of ambient gas in the early universe. In the current universe, the local CR luminosity density is QCR≈2×1046 ergMpc−3 yr−1ðSFR=0.015 M⊙Mpc−3 yr−1Þ [4], where SFR is the star-formation rate density. With a massive POP-III SFR of ∼10−5–10−4 M⊙ Mpc−3 yr−1 around z ∼ 20, we have QIII CR ∼1043–1044 ergMpc−3 yr−1. By comparing the CR loss time tlossðzÞ to the Hubble time at z, tHðzÞ ≈ 1.9 × 108½ð1 þ zÞ=21 −3=2 yr, the CR energy density is estimated to be uCR ∼ 3 × 10−18 erg cm−3½ð1þ zÞ=21 3ðmin½tHðzÞ; tlossðzÞ=100 MyrÞ ðSFRIII=10−4 M⊙ × Mpc−3 yr−1Þ in the physical volume, and the similar value can be obtained by using a POP-III supernova rate of 10−7–10−6 Mpc−3 yr−1 and a typical kinetic energy of ESN ∼ 1051 erg. A low-energy part of first CRs (E ≲ 30 MeV) ionizes  where fheat ∼ 0.25 is the energy fraction used for heating, Ωb ¼ 0.04 and ϱcrit ≈ 1.88h2 × 10−29 g cm−3 is the critical density. Given that the CMB temperature is TCMB ¼ 2.7 Kð1 þ zÞ, the effect of the first CRs may be observed by future HI 21-cm observations, especially if the first CR production rate comoving density may continue down to lower redshifts, e.g., z ≈ 6 (see [37] for the redshift evolution of the POP III SFR). Second, the first CRs may provide seed fields for intergalactic magnetic fields. In our Galaxy, the magnetic field energy density is comparable to the CR energy density. If the energy of the first CRs is converted to the energy of magnetic fields by some plasma effects at z ≈ 20, the comoving magnetic field strength is estimated to beVI. CONCLUSIONS We studied the origin of the first CRs. We have found the following: (1) The first CRs can be accelerated by the first supernova remnants driven by the explosion of first stars at z ≈ 20 (corresponding to about 180 million years after the big bang). (2) The first CRs are accelerated by DSA in the nonrelativistic Weibel mediated shocks, although CR streaming instabilities can play some roles in the presence of violent mass losses in the preexplosion phase. (3) The maximum energy of the first CRs lies in the 0.1–10 GeV range. As a result, secondary neutrinos produced by these CRs are not expected in the IceCube range unless supernovae are accompanied by choked jets or dense magnetized environments [42]. (4) The first CR protons with energies larger than a few MeV can escape from the first supernova remnants, whereas all accelerated electrons can. These CRs may impact the heating, ionization, magnetic field generation, and the cosmological evolution of protogalaxies. In particular, we suggest that HI 21-cm signatures can be used as a powerful test for the first CRs. (5) Accretion shocks associated with the cosmological structure formation at z ≈ 20 are unlikely to accelerate particles because the shocks propagate in neutral media. On the other hand, the accretion shocks at z ≲ 10 can accelerate particles. The maximum energies of protons and electrons are a few hundreds keV and several MeV, respectively. Only the accelerated electrons can escape from the accretion shock without energy losses.	
7	2019	Stress-tensor commutators in conformal field theories near the lightcone	Kuo-Wei Huang	Stress-tensor commutators in conformal field theories near the lightcone Starting with the general stress-tensor commutation relations consistent with the Poincar´e algebra in local quantum field theory, we impose the tracelessness condition and focus on the dominating contributions in the lightcone limit. It is shown that, under a certain assumption on the Schwinger term, a Virasoro-algebra-like structure emerges near the lightcone in d > 2 conformal field theories. I. INTRODUCTION As the universal central extension of the Witt algebra, the existence of the Virasoro algebra [1] plays a crucial role in mathematics and theoretical physics, particularly of deep importance in conformal field theory (CFT). It is, however, a special luxury one has in two-dimensional spacetime. In higher dimensions, where the conformal group is finite dimensional, Virasoro-algebra related techniques employed in understanding d ¼ 2 CFT become generally invalid. Nevertheless, one may still ask the following question: does an effective similar structure exist in higher-dimensional CFTs that can be used to control the CFT data within a certain subspace (i.e., a subsector of the full parameter space)? As the form of two commuting copies, the Virasoro algebra can be expressed as the stress-tensor commutation relationdenote ðxþ; x−Þ¼ðt þ y; t − yÞ with ðx0; x1Þ¼ðt; yÞ, and ðTþþðx−Þ; T−−ðxþÞÞ ¼ −2ðT0 0 − T0 1; T0 0 þ T0 1Þ in Minkowski metric ημν ¼ diagð−1; 1Þ. The central-extension part containing the central charge c arises from the quantum anomaly. While the tracelessness condition in d ¼ 2 allows one to replace the purely spatial-component of the stress tensor, T11, with T00, independent spatialcomponents appear in d > 2, and, in general, there is no stress-tensor algebra in higher dimensions. In this work, we make an initial attempt, starting from the most general stress-tensor commutation relations in Lorentz invariant, local quantum field theory (QFT) [2–4], to search for a possible Virasoro-like structure in higher-dimensional CFTs. In particular, while the tracelessness constraint must be imposed, we would like to see under what additional conditions an effective Virasoro-like algebra may emerge. What clue do we have? The AdS=CFT correspondence [5–7] provides an interesting hint toward this direction. In a recent work [8], it was found that the operator product expansion (OPE) coefficients of the multi stress-tensor conformal blocks of a scalar four-point function in a large class of d > 2 CFTs are universal in the lowest-twist subspace. (The twist of an operator is its dimension minus its spin.) These isolated OPE coefficients are universally fixed by the dimensions of the light and heavy scalar operators, and the central charge CT, the coefficient of the stress-tensor two-point function. In d ¼ 2, the Virasoro algebra dictates all the related structures. While additional assumptions were made in the gravitational computation considered in [8], such as having a large CT and ignoring additional bulk matters, it is tempting to ask if the lowesttwist limit is essentially sufficient in a more general analysis in d > 2 CFTs. As the lowest-twist limit corresponds to the lightcone limit, where operators in a correlator approach each other’s lightcone, we are therefore motivated to consider the CFT stress-tensor commutators near the lightcone. In the next section, we first review the general stresstensor equal-time commutation relations in QFT, based on earlier works [2–4]. The tracelessness condition and the lightcone limit shall be imposed in a later section. The main result is to obtain an effective lightcone stress-tensor commutator in CFT. By effective, we mean that the lightcone limit is taken when stress tensors are inserted in a correlation function. In this case, the purely lightcone component of the stress tensor, denoted as T˜ þþ below, dominates the contributions. A crucial point is that, in such an effective lightcone limit, one avoids the commutatorwith purely spatial components, i.e., ½TijðxÞ; Tmnðx0 Þ, whose form cannot be determined by Poincar´e symmetry or conformal invariance and thus is generally model dependent in d > 2. (The central charge CT is also model dependent, but we say a quantity has a universal meaning if all the model-dependent data can be absorbed into CT.) The resulting effective lightcone commutator will have a nonextension part and also a Schwinger term. The nonextension part formally looks the same as that in the d ¼ 2 Virasoro algebra. The Schwinger term in general dimensions can be related to the central charge CT. Some subtleties of the Schwinger term will also be discussed. II. STRES-TENSOR COMMUTATION RELATIONS IN QFT Here we first review the stress-tensor commutators in Lorentz invariant, local QFT (see [4] and [2,3] for more discussions). Denote a classical action S embedded in a curved background and write the curved-space stress tensor asA factor ffiffiffiffiffiffi −g p is normally factored out from Cμν as the conventional stress tensor, but we adopt the above notation for later convenience. Eventually, we will be interested in the commutation relations of the flat-space stress tensor, denoted as TμνðxÞ, in metric ds2 ¼ −dt2 þ δijdxi dxj . Below, we denote hCμνðxÞi ¼ −2i δZ δgμνðxÞ , where Z is the partition function. The starting point is: (One can also consider xþ → 0 with x− fixed.) and focus on the effective commutation relation, where stress tensors are in a correlation function. The reason to adopt such a scenario is that, for many purposes, such as in the conformal block decomposition, stress tensors always appear in a correlator and thus having an effective commutator would be sufficient. We shall focus on stress tensors with indices uncontracted, corresponding to the lowest-twist or largest-spin limit. Intuitively, the lightcone-component Tþþ should dominate near the lightcone. However, in d > 2, the existence of Ta a in (22) causes trouble: since purely spatial components of the stress-tensor commutator do not have a model-independent expression, there is no universal way to compute ½TþþðxÞ; Tþþðx0 Þ in d > 2. This obstacle may be circumvented by adopting an effective lightcone commutator. Let us here demonstrate via an example. Consider the following CFT correlator [9]:where we focus on the short-distance (small x2) behavior; a is a constant and Δϕ the dimension of ϕ. We will put scalar fields on a xþ − x− plane and consider the lightcone limit x− → 0. The stress tensors generally allow the transversecoordinate dependence, and we consider that stress tensors also approach to the lightconeexample involves only a stress tensor, one may consider more stress tensors and verify that the contributions near the lightcone are dominated by ðT0 0 − T0 1Þn with n stress tensors. We thus focus on the commutator involving T0 0 − T0 1 as the effective lightcone description. In what follows, we letLet us also remark that the operator ∂þT˜ þþ does not contribute in the same lightcone limit either. We next compute the commutator of T˜ þþ using (9), (10), and (12), together with the tracelessness condition. We find, before imposing the lightcone limit, the following intermediate expression:The lightcone limit is imposed after computing the commutation relation. In d ¼ 2, it is not necessary to impose a lightcone limit. IV. REMARKS ON THE SCHWINGER TERM The connection between the Schwinger term and CT can be deduced from the spectral representation [10,11]. First, consider the Källen-Lehmann spectral form of the stresstensor two-point function in unitary QFT (in Euclidean signature),term in general d should not touch the coefficient of the existing T˜ þþ piece. (One may adopt additional limits, such as a large CT, to suppress possible unwanted contributions, but we do not consider such a limit here.) On the other hand, a direct canonical computation shows that the Schwinger terms (14)–(17) in d > 2 free theories simply vanish, as first pointed out in [4], while (18), which is however irrelevant in the lightcone limit, can be nonzero but its expression is model dependent. (Related computation details can be found in [15].) The authors of [4,15] then argued that quantum effects are responsible for producing the anomalous c-number delta-function distribution predicted from the spectral forms. While their results suggest that the free theories belong to the class of theories we are interested in, it would be interesting to revisit the freetheory calculations and derive the c-number contribution in view of the results presented here. In the class of d ¼ 4 CFTs, for instance, where the Schwinger term is effectively a c-number near the lightcone, we may write the lightcone effective algebra aswhere δ3 ¼ δðxþ − x0þÞδ2ðxa − x0aÞ. The appearance of the UV divergence in the Schwinger term is expected to be a general figure in d > 2 CFTs, based on essentially dimensional analysis. The coefficient of the power-law divergence has no universal meaning and thus we shall focus on the universal finite piece (the coefficient of the highest-order derivative of the delta function) in the Schwinger term and subtract the divergence off when computing a correlator. We hope to discuss the applications of the lightcone stress-tensor commutation relation elsewhere. In particular, it would be interesting to realize the lightcone algebra in a holographic framework and explore potential connections with the lowest-twist universality [8] and also some recent works [16–21].	
8	2018	Analytic representation of FK=Fπ in two loop chiral perturbation theory	B. Ananthanarayan,1 Johan Bijnens,2 Samuel Friot,3,4 and Shayan Ghosh1	Analytic representation of FK=Fπ in two loop chiral perturbation theory We present an analytic representation of FK=Fπ as calculated in three-flavor two-loop chiral perturbation theory, which involves expressing three mass scale sunsets in terms of Kamp´e de F´eriet series. We demonstrate how approximations may be made to obtain relatively compact analytic representations. An illustrative set of fits using lattice data is also presented, which shows good agreement with existing fits. I. INTRODUCTION The spectrum of QCD contains as lightest particles the pseudoscalar octet, and their properties provide a test of its nonperturbative features, including chiral symmetry breaking. Of particular importance is the ratio FK=Fπ, which has been investigated on the lattice even at quark masses that include the physical values [1]. In chiral perturbation theory (ChPT) [2] at two-loops, expressions for Fπ and FK involve certain integrals (sunsets) that are evaluated numerically [3]. In this work, we provide an analytic expression for FK=Fπ, which uses double series derived using MellinBarnes (MB) representations of the sunsets, providing a template for easy fitting to lattice simulations. II. METHODOLOGY The ratio FK=Fπ in two-loop SUð3Þ ChPT is given by: Aside from this basic scalar integral, tensor integrals in which the momenta qμ and qμqν appear in the numerator, and derivatives of both the scalar and tensor integrals w.r.t. p2 contribute to dP sunset [3]. The tensor and derivative integrals, as well as all the derivatives, may be reduced into a linear combination of scalar integrals using the methods given in [4]. The full list of sunset integrals contributing to dP sunset can thus all be expressed in terms of a set of four master integrals and the one-loop tadpole integral. The problem reduces to solving these analytically in the required mass configurations. For FK=Fπ, seven distinct three mass scale MI need evaluation. MB theory leads to representations of these MI where each integral consists of at least one double complex plane integral. These double MB integrals are evaluated using the method proposed in [5] and fully systematized in [6] to obtain results in the form of sums of single and double infinite series [7–9].III. THE ANALYTIC REPRESENTATION FF consists of terms arising from pure sunset contributions. The split between the Kˆ i terms and FF is not unique: one convenient decomposition, that takes into account the freedom to distribute the chiral logs while keeping the final result unchanged, isThe MI are denoted by H¯ S aPbQcR ≡ H¯ d fa;b;cgðm2 P; m2 Q; m2 R; p2 ¼ m2 SÞ, the “bar” indicating that the chiral subtraction prefactor ðμ2 eγE−1 4π Þ 4−d has been taken into account, and that the chiral logarithms have been extracted and included in the log terms of Eq. (2). Expressions for the two mass scale MI are given in [10], and those for the three mass scale are given below in terms of generalized hypergeometric (pF q) and Kamp´e de F´eriet (KdF) series. The three mass scale MI not explicitly presented here can be derived from the following by differentiation w.r.t the appropriate square propagatorOne may obtain simplified representations for FF by truncating the series at the desired precision, and taking an expansion around ρ ¼ m2 π m2 K ¼ 0. For illustrative purposes, we present one such representation in which we truncate the series such that the error between the exact and truncated values is <1% for most of the sets of masses used in the lattice study of [1]. We get:The range of validity of Eqs. (18)–(20) is shown in Fig. 2, in which the exact value of FF is plotted against x ¼ ffiffiffi ρ p , as are the approximate FF retained up to various orders of ρ. The expansion up to Oðρ4Þ approximates the exact value of FF to 1% for mπ=mK < 3 and to 6% for mπ=mK < 0.5. One may obtain a representation with greater accuracy by truncating the series with a larger number of terms. For the reader to be able to verify the implementation of these expressions, we give the numerical values of FK=Fπ coming from both exact and approximate expressions and obtained with physical values mπ ¼ 0.1350 GeV, mK ¼ 0.4955 GeV, Fπ ¼ 0.0922 GeV, as well as the LEC values of the BE14 fit of [11]. We get, using Eq. (8), FK=Fπ ¼ 1.19897; ð20Þ and using the approximation of Eqs. (18)–(20),IV. ILLUSTRATIVE LATTICE FITS We present an exploratory numerical study based on our analytical representation by fitting Eq. (5) with the data of the lattice study [1] to determine best-fit values of the NLO LEC Lr 5 and the NNLO LEC combinations Cr 14 þ Cr 15 andCr 15 þ 2Cr 17. We perform the fit (using [12]) on the mass sets for which mπ < 0.40 GeV. We do the fit on the ‘exact’ FF, i.e. truncating the KdF series after 10002 terms and the pF q series after 1000 terms, and cross-check by fitting the exact purely numerical version of Eq. (3) with CHIRON [13]. The fit on the approximate version presented in Eq. (18) gives compatible results. The uncertainties on the values of the LEC given in this section derive from the errors of the FK=Fπ data of the lattice study, but do not take into account other uncertainties. As detailed in [1], systematic effects due to lattice artificats can arise from correlator fit time choices, lattice spacings, renormalization and finite volume corrections, among other things. When these effects are taken into account, using e.g. [14,15], the values of the LEC presented in this section are likely to change. However, determining the exact nature and magnitude of the change involves a detailed study that is outside the scope of this paper. We fix the renormalization scale μ at mρ ¼ 0.77 GeV, and use the values of the BE14 fit [11] for the other Lr i. In addition we fix Fπ in the determination of ξπ and ξK to 92.2 MeV and obtain: Lr 5 ¼ ð3.92  0.55Þ10−4 Cr 14 þ Cr 15 ¼ ð2.59  0.63Þ10−6 Cr 15 þ 2Cr 17 ¼ ð6.10  1.41Þ10−6: ð22Þ The correlation parameters are given in Table I and the quality of the fit is shown in Fig. 3 (Left). The correlation is shown graphically in Fig. 3 (Middle, Right) by plotting a number of random points in a distribution given by the correlation matrix of the fit projected on the two different planes. With these LEC values and the physical meson masses as inputs, we get for the value of FK=Fπ: FK=Fπ ¼ 1.194; ð23Þ which agrees well with the literature value of [11]. The values of Eq. (22) differ from those of the BE14 exact fit (L5 ¼ 10.1 × 10−4, C14 þ C15 ¼ −4.00 × 10−6, C15 þ 2C17 ¼ −5.00 × 10−6) significantly, but are more compatible with those of [16] (L5 ¼ 0.76 × 10−3, C14 þ C15 ¼ 3.15 × 10−6, C15 þ 2C17 ¼ 10.96 × 10−6 in dimensionaless units) and [17] (L5 ¼ 0.75 × 10−3, C14 þ C15 ¼ 1.70 × 10−6, C15 þ 2C17 ¼ 6.04 × 10−6). A similar fit, but now with Fπ also varied in ξπ, ξK requires the use of lattices common to [1,18] to obtain the values of Fπ for each lattice. This fit gives: Lr 5 ¼ ð0.49  1.08Þ10−4 Cr 14 þ Cr 15 ¼ ð5.59  1.08Þ10−6 Cr 15 þ 2Cr 17 ¼ ð39.7  2.10Þ10−6: ð24Þ The change in the values above arises primarily due to the variation of Fπ. Keeping Fπ fixed at 92.2 MeV but with the set of inputs used to calculate Eq. (24) results in changes of ≈20%, 35% and 10% in the Eq. (22) values of the Lr 5, Cr 14 þ Cr 15 and Cr 15 þ 2Cr 17, respectively. As the difference in the inputs for Eqs. (22) and (24) is primarily the data from the coarsest lattices, it seems that the lattice data has a significant impact on fitting the LECs. V. CONCLUSIONS The ratio FK=Fπ is a quantity at the heart of chiral symmetry breaking, a fundamental property of the strong interactions that is measured in ab initio calculations on the lattice. An analytic expansion for this quantity in quark or meson masses is the order of the day. Using modern loop calculation techniques, we have achieved this goal. At present, two-loop precision is sufficient to fit the lattice data; this might change when the lattice precision improves in the future. While there exist three-loop results in twoflavor ChPT [19], in three-flavor ChPT two-loops is state ofthe art, adding to the significance of the results presented herein. We hope this work encourages similar crossdisciplinary studies of other quantities of interest.	
9	2018	Results from phase 1 of the HAYSTAC microwave cavity axion experiment	L. Zhong,1 S. Al Kenany,2 K. M. Backes,1,* B. M. Brubaker,1 S. B. Cahn,1 G. Carosi,3 Y. V. Gurevich,1 W. F. Kindel,4 S. K. Lamoreaux,1 K. W. Lehnert,4 S. M. Lewis,2 M. Malnou,4 R. H. Maruyama,1 D. A. Palken,4 N. M. Rapidis,2 J. R. Root,2 M. Simanovskaia,2 T. M. Shokair,2 D. H. Speller,1 I. Urdinaran,2 and K. A. van Bibber2	Results from phase 1 of the HAYSTAC microwave cavity axion experiment We report on the results from a search for dark matter axions with the HAYSTAC experiment using a microwave cavity detector at frequencies between 5.6 and 5.8 GHz. We exclude axion models with two photon coupling gaγγ ≳ 2 × 10−14 GeV−1, a factor of 2.7 above the benchmark KSVZ model over the mass range 23.15 < ma < 24.0 μeV. This doubles the range reported in our previous paper. We achieve a nearquantum-limited sensitivity by operating at a temperature T<hν=2kB and incorporating a Josephson parametric amplifier (JPA), with improvements in the cooling of the cavity further reducing the experiment’s system noise temperature to only twice the standard quantum limit at its operational frequency, an order of magnitude better than any other dark matter microwave cavity experiment to date. This result concludes the first phase of the HAYSTAC program utilizing a conventional copper cavity and a single JPA. I. INTRODUCTION The Standard Model of particle physics requires the violation of charge parity (CP) symmetry in the strong interaction, which leads to a theoretical neutron electric dipole moment orders of magnitude larger than the current experimental limit. To solve this problem, Peccei and Quinn proposed a solution by which the CP-violating θ term of the QCD Lagrangian would dynamically relax to its CP-conserving minimum [1,2]. Shortly thereafter, Weinberg and Wilczek realized that this mechanism implied the existence of a light pseudoscalar, termed the axion [3,4]. Subsequently, it was realized that the properties of the axion and the mechanism by which it would be created in the early universe made it an excellent candidate for the cold dark matter in galactic halos. The axion mass, ma, has historically been taken to be in the range 1 μeV ≲ ma ≲ 1 meV [5]. Recent lattice QCD calculations have motivated higher mass axion searches, favoring ma ≳ 50 μeV [6]. Because of its low mass and its very weak interaction with matter and radiation, detecting an axion is very challenging. In 1983, P. Sikivie proposed an experimental axion detection scheme based on the axion-photon conversion [7–9]. The natural conversion rate is very low. For it to be detectable on a reasonable time-scale, this conversion must be resonantly enhanced with a high quality factor microwave cavity in a strong magnetic field. The resulting resonant axion conversion power isHere, gγ is a model dependent coupling constant, α is the fine structure constant, ρa ≈ 0.45 GeV=cm3 is the local axion density [10], and Λ ¼ 78 MeV encodes the dependence of the axion mass on hadronic physics. The physical coupling that appears in the axion-photon Lagrangian is gaγγ ¼ maðαgγ=πΛ2Þ. The terms in the second set of parentheses in Eq. (1) are experimentally controllable: the coupling coefficient β, unloaded cavity quality factor Q0, loaded cavity quality factor QL ¼ Q0=ð1 þ βÞ, magnetic field B0, cavity frequency ωc, cavity volume V, and mode form factor Cmnl. For ma ≈ 24 μeV, a typical KSVZ model axion with gγ ¼ −0.97 [11,12] gives a conversion  power of PS ≈ 5 × 10−24 W based on the properties of our detector [13]. The axion has an approximately Maxwellian velocity distribution, and the signal line shape is given by the corresponding energy distribution. More detailed discussions of the axion signal line shape can be found in Sec. VII A in Ref. [14]. Because the mass of the axion is unknown, we tune the cavity in discrete steps Δνs ≤ Δνc=2 and average the cavity noise at each step for an integration time τ. We define the signal-to-noise ratio (SNR) Σ as the ratio of signal power to uncertainty in noise power within the signal bandwidth: where the added noise is NA ≥ 1 2. This method has made axion detection feasible and is the basis of tremendous effort in axion searches. Experiments of this type aim for high magnetic field B0, high Q0, large cavity volume V, high form factor Cmnl, and low system noise temperature TS. In this paper, we report the results from data runs 1 and 2 of the HAYSTAC (Haloscope At Yale Sensitive To Axion Cold dark matter) experiment. This extends our total coverage to the range 5.6–5.8 GHz with an analysis based on the lab-frame axion line shape [15]. Section II describes the experimental apparatus, and Sec. III describes the improvements on the experiment implemented between data runs 1 and 2. The data analysis and results are described in Sec. IV, with the conclusion in Sec. V. This completes phase 1 of the HAYSTAC experimental program which utilizes a conventional copper cavity and a single Josephson parametric amplifier (JPA). The experiment is now being upgraded with a squeezed-vacuum state receiver to improve the sensitivity and scan speed of the search [16]. II. EXPERIMENT The HAYSTAC experiment was first operated in January 2016. Figure 1 shows a simplified diagram of the receiver circuit. The apparatus is described further in Ref. [13]. The experiment employs a 2-liter, high-quality factor, tunable microwave cavity maintained at TC ¼ 127 mK. The system is immersed in a strong magnetic field (B ¼ 9 T) with typical parameters QL ≈ 10 000, C010 ≈ 0.5, and β ≈ 2. QL achieves 75% of its theoretical maximum, and this overcoupling of the cavity allows us to maximize scan rate. Galactic halo axions would convert to radio-frequency (RF) photons in the strong magnetic field, and the cavity serves as an impedance matching network that couples the near infinite impedance signal source to a coaxial cable (this can be understood as an extension of the Purcell effect, as originally conceived in Ref. [17]). This cable in turn delivers the RF power to a JPA [18]. The experiment requires a narrow-band step-tuned search over frequency for an excess RF noise signal due to axion conversion that would appear as an addition to expected quantum fluctuation noise (along with minimal thermal noise). This tuning (further discussed in Sec. III A) is achieved by the rotation of a copper rod occupying 25% of the cavity’s volume. To minimize the system noise temperature and allow for in situ noise calibration, we designed a receiver that incorporates a near-quantum-limited JPA, which is a nonlinear LC circuit whose inductance is provided by an array of superconducting quantum interference devices (SQUIDs) [19], and a microwave switch near the receiver input. The switch can be toggled between a hot load and a cold load. A 50 Ω termination thermally anchored to the dilution refrigerator’s still plate at TH ¼ 775 mK serves as the hot load, and the cavity serves as the cold load. This toggle setup allows us to incorporate the noise calibrations into the axion search. We do this calibration approximately once every 11 h. Our preamplifier is composed of the JPA, a directional coupler for the JPA’s driving pump tone near its resonant frequency, and a circulator to route the output signal away from the cavity. Two additional circulators isolate the JPA from the cavity and the second stage amplifier, a high electron mobility transistor (HEMT) which is kept at the 4K stage. At room temperature, the signal is further amplified, down-converted to an intermediate frequency (IF) band centered at 780 kHz, and digitized for analysis. Further details on the experimental setup and signal path can be found in Ref. [13]. The first data run was carried out over 110 days and covered the frequency range 5.7–5.8 GHz. Twenty-three days of rescan focusing on 27 rescan candidates followed this initial run. It was completed in August 2016, with the results and the details of the analysis reported in Refs. [14,15]. The run 1 data was analyzed with the axion lineshape in the rest frame, and the exclusion limit in the range of 5.7–5.8 GHz was obtained based on this assumption. Rescan candidates from run 1 were re-investigated with a virialized line shape after run 2. Several technical improvements were implemented between runs 1 and 2. They are described in detail in Sec. III. Run 2 was carried out over 54 days and finished in July 2017, covering 5.6–5.7 GHz. Run 2 was followed by 53 days of rescan of potential candidates, where about 75% of the time was dedicated to candidates from run 1. III. IMPROVEMENTS The challenges that lead to the technical improvements between the first and second data run are as follows. First, the pulley and Kevlar line system that was used to rotate the tuning rod in the cavity had considerable mechanical hysteresis due to unexpected stiction in the cavity bearings. After each 100 kHz tuning step (0.003° rotation), the tuning rod would take 20 min to drift slowly to its final position. Second, the tuning rod was supported solely by thin alumina tubes that did not provide a sufficient thermal link to the tuning rod. Because of this, the temperature of the rod remained at 600 mK, far above the base temperature of 125 mK. Finally, the use of thick Cu elements in the construction of the cavity support framework led to major damage of the experiment from the eddy current forces resulting from a superconducting magnet quench during a power outage in March 2016.We now describe our improvements in detail. A. Piezoelectric motor tuning During run 1, the pulley and Kevlar line system was only used for large frequency steps in order to mitigate the timedependent drift it caused. Fine stepping was performed by inserting a thin dielectric rod to shift the mode frequency. Unfortunately, the tuning range of the dielectric rod was frequency dependent, and in some regions had no effect at all. Motion of the dielectric rod also generated significantly more heat than the Kevlar pulley system. To eliminate the stiction and hysteresis problems, we replaced the pulley and Kevlar line system with an Attocube ANR240 piezoelectric precision rotary motor after the first data run [20]. The motor is supported by a bracket attached to the experiment frame about 12” above the cavity. The rotary motion is transmitted by 6” long, 0.25” wide brass rods, connected by a corrugated stainless steel flexible shaft coupler. The addition of the new system allowed us to remove the 1.4∶1 gear box that coupled the pulley and Kevlar line system to the tuning rod. The motor is driven by a sawtooth-voltage electrical signal (50 V amplitude) and draws a high current (1.5 A) when actuated. To ensure low resistance, 28 AWG Cu wires were used to drive the motors from room temperature to the 4 K stage, and NbTi superconducting wire was used below the 4 K stage. The motor wires were physically separated from the signal wiring for flux bias current, HEMT amplifier controls, thermometry, heater, and microwave switch to protect the delicate signal wires. The system was tested extensively prior to cool down. To allow for room temperature testing of the motors, the superconducting wires were temporarily replaced with copper wires. The torque of the Attocube motor is sufficient to move the cavity tuning rod even in the presence of the 9 T magnetic field. Empirically the mechanical stiction depends on the direction of rotation. At 9 T, the Attocube motor is only able to tune in one direction when the tuning rod is at angles where the stiction is large. This is ok because we carried out the axion search by stepping the cavity frequency in one direction. When necessary, the cavity can be tuned freely in both directions by reducing the magnetic field to 6 T. The Attocube system generates more heat than the Kevlar pulley system alone, but most of the previous system’s heat was generated by the friction cased by the large motions of the dielectric rod. Thus, this upgrade reduced the total heat load of the tuning system. The Attocube system has provided seamless operation with an acceptable heat load and no observable drift (Fig. 2). The addition of the Attocube tuning system has also allowed us to reduce the size of the large notch at 5.704 GHz in our exclusion limit caused by a cavity mode crossing. We fixed the dielectric rod at two different positions and used the new precision tuning of the Attocube to tune the frequency of the cavity closer to the mode crossing than we were able to before. B. Improved thermal linkage between the tuning rod and the cavity Prior to this experiment, such a large and uniquely configured cavity had never been coupled to a JPA. During run 1, the system noise temperature at the cavity resonance was observed to be significantly higher than off resonance. For a thermally well-linked system, we expect the two to be similar. By performing various tests, such as raising the system temperature to the point where the resonance and off-resonance noise levels were nearly equal, it was determined that the excess noise was due to the tuning rod failing to cool to the base temperature of the system. These tests alleviated concerns that the excess noise was due to a spurious interaction between the cavity and the JPA which would have been difficult to eliminate. The thermal link to the cavity tuning rod comprised two 0.250” outer-diameter, 0.125” inner-diameter polycrystalline alumina tubes, of 4” length each, on either end of the cavity cylinder axis. The only contacts between the tubes and the support frame (which serves as the thermal link to the dilution refrigerator mixing chamber) were bearings that both ensured free rotation of the tuning rod and maintained perfect parallelism between the tuning rod and the cavity body. The contact area between the ball bearings themselves and the inner and outer races of the bearing is vanishingly small by design and did not provide an adequate thermal link. The first attempt to improve the thermal linkage was to glue short brass rods into the external ends of the alumina tubes, using a thermal epoxy, and then connecting those brass rods to the support frame with flexible Cu braids. This solution proved insufficient, as the brass rods were only inserted 0.25” into the tubes, leaving a low-thermal conductivity path of several inches of alumina on either end of the axle. Tests on a nearly identical cavity suggested that 0.125” copper rods could be inserted sufficiently far into the alumina tubes to provide adequate thermal linkage without undue loss of cavity Q. Such rods were incorporated into the system and held in place with conductive silver epoxy (Epo-Tek H20E). The rods extend 0.5” beyond the tube where copper braids were soldered onto before gluing. The other ends of the rods serve as connection points to the piezoelectric motor, discussed above. The copper rods reduce the total system noise photon number (noise per unit bandwidth) at the cavity resonant frequency from around 3 to 2.3 quanta on average, corresponding to a reduction in tuning rod temperature from 600 to 250 mK (Fig. 3). Unfortunately, the cavity Q was reduced by about 40%. We now believe that the failure to achieve the original Q was due to the construction of the tuning rod’s axle for the cavity used in actual running. It prevented the copper rods from being placed in the optimum position. This will be corrected for future runs, where we predict no additional thermal contribution from the rod, and essentially no diminution of Q. The improved thermal link reduced the time the system takes to cool from the first condensing of the mixture to base temperature from over six hours to under one hour. Without the copper tubes inserted, the alumina tubes’ weak thermal link was a bottleneck in the cooling process and maintained a substantial heat load. When the tuning rod reached 600 mK, the alumina tubes became effective thermal insulators, keeping it from reaching 125 mK. After this quasi-equilibrium was established, we saw no discernible decrease in thermal noise level over months of operation, implying a time of perhaps years for the tuning rod to significantly cool beyond this point. We have yet to identify the remaining source of excess thermal noise (250 mK compared to a system temperature of 125 mK) which is likely a further issue with the thermal link of the tuning rod. C. Copper-plated stainless steel thermal links and shields The original design incorporated several massive OFHC copper components. A magnet quench during a universitywide power outage caused significant damage to the experiment. The cavity is made of copper-plated stainlesssteel and has a high thermal conductivity. Readings from the thermometers on the cavity top and bottom indicate that there is less than 30 s time delay between a change in temperature at the top, and a subsequent change at the bottom. The rapid change of the cavity temperature at its bottom (far end from thermal link) led us to conclude that heavily plated stainless steel could be used to construct effective thermal radiation shields while minimizing the amount of copper in the system. The damaged still-temperature thermal shield was replaced with a stainless steel shield plated with 0.002” copper. This new shield has been sufficient, with no obvious excess heat load at the mixing chamber level. IV. DATA ANALYSIS AND RESULTS The combined data from run 1 and run 2 covers the frequency range 5.6–5.8 GHz. The data runs resulted in a total of 10 406 raw subspectra, of which 10 090 were used for the analysis presented here. The remaining 316 were rejected due to their poor JPA gain stability, cavity frequency drift, proximity to cavity mode crossing, etc. Each sub-spectrum covers a 1.3 MHz analysis band with resolution of Δνb ¼ 100 Hz. Here we give a brief description of the analysis. The analysis procedure is detailed in Ref. [14]. The final limit shown in Fig. 4 is obtained by combining the 10 090 selected subspectra in a weighted sum that maximizes the SNR. The subspectra are aligned by their IF frequency and averaged to extract the average shape of the spectral baseline. Aligning them in this manner allows us to cut IF bins that have been compromised by narrow IF spikes from the analysis. Next, the average shape of the spectral baseline is removed from each raw subspectrum. The remaining baseline structure is removed by dividing out a Savitzky-Golay (SG) fit and subtracting 1. In the absence of an axion, each raw subspectrum is now a dimensionless processed subspectrum described by the same Gaussian distribution. This Gaussian distribution has a mean of μ ¼ 0 and standard deviation of σ ¼ 1= ffiffiffiffiffiffiffiffiffiffi τΔνb p . To put the raw subspectra in units of watts, each raw subspectrum is multiplied by the average noise power per bin. This also undoes the suppression of any potential signal that would appear in a particularly noisy bin. Now to scale the raw subspectra such that an axion present in any bin would have the same value, we divide by the Lorentzian axion conversion power profile. The expected axion power is different across the RF frequency spectrum and depends on the cavity quality factor Q, coupling factor β, mode form factor C, and the cavity transmission. In order to form the combined spectrum, corresponding RF bins in different spectra are added together with maximum likelihood (ML) weighting. Groups of ten neighboring bins are then added together with an extension of the ML method reducing the resolution of the spectrum from Δνb ¼ 100 Hz to Δνb ¼ 1 kHz. Next, overlapping groups of nine neighboring bins are added together, this time taking into accountthe expected axion line shape. In each step of this process, the standard deviation of each sample is also scaled accordingly. We set a threshold in the combined spectrum based on a predetermined confidence level and target axion coupling. This allows us to select frequencies passing this threshold as rescan candidates. We set a frequencyindependent target SNR Σ ¼ 5.32σ, implying a frequency-dependent target axion coupling. If an axion with the target coupling exists at a certain frequency, it will appear in the grand spectrum with a mean of 5.32σ and a standard deviation of σ. To determine rescan candidates, we set a power threshold determined by this target SNR, along with the false negative rate Fn of our entire detection protocol, consisting of an initial scan followed by as many as two rescans. For each frequency, each rescan is performed only if the power measured in the previous scan exceeds the threshold. Any candidates passing all three scans, of which we observed none, would be interrogated manually by protocols designed to have miniscule false negative rates, and to discriminate other systematic sources of RF noise excess from an axion. The total false negative rate is the chance that at least one scan delivers a false negative, Fn ¼ 1 − ð1 − fnÞ3; ð4Þ where fn is the false negative rate for each scan, and is the same for all three scans. In order to exclude at 90% C.L., we must have Fn ¼ 10%, hence fn ¼ 3.45%. Setting the single-scan false negative rate fn ¼ 3.45% and the target Σ ¼ 5.32σ yields a power threshold of 3.50σ. 27 rescan candidates, defined as frequency ranges with normalized power exceeding this threshold, were identified. This is consistent with the number of rescan candidates expected by simulating Gaussian white noise subjected to the same co-adding procedure. To determine whether each rescan candidate is due to a statistical fluctuation or a persistent RF noise excess, such as an axion to photon conversion, we must set a target rescan SNR Σr for the rescans associated with each data run. As discussed in Sec. III B, the improvements in the thermal coupling of the alumina rod reduced the system noise temperature by 20% but decreased the Q by 40%. This Q degradation implies that a different Σr is appropriate for the two data sets. The rescan integration time τ required to achieve Σr is frequency dependent. It is also depends on Σr as follows: τ ∝  TΣr QL 2 ð5Þ From Eq. (5), we can see that the decreased Q leads to a significantly increased integration time required to reach the same SNR as in the initial scan. The longer the integration time at a certain frequency, the more pronounced the baseline systematics become in the shape of the data. This limits the amount of time we can take data at each frequency. Accounting for this effect, we chose values of Σr ¼ 4.53σ and Σr ¼ 5.1σ for the first and second data runs, respectively. From Σr, cavity parameters, and parameters measured in the initial scan, the rescan integration time per frequency can be determined. The data collected is then analyzed in a similar method as detailed above with different filtering parameters. This process is further detailed in Sec. IX (Rescan Data and Analysis) in Ref. [14]. Of the 27 rescan candidates, four passed this second threshold and were at the frequencies 5.72648, 5.71761, 5.71652, and 5.66417 GHz. These four rescan measurements were repeated, and they did not pass the threshold a third time. We thus exclude all frequency bins to 90%, corresponding to our total false negative rate of 10%. Of the 27 rescan candidates, the spectra around 5.79697, 5.76952, 5.76318, 5.75986, 5.74421, 5.74418, and 5.73688 GHz (all from the frequency range covered by the first data run) exhibited non-Gaussian statistics. During the initial run, these frequencies had Gaussian spectra. Because the non-Gaussian behavior only appeared after the thermal coupling problem was fixed, it is believed that the added Cu rod acts as an antenna and couples the spurious RF signals into the cavity. This problem is more prominent during rescans than during data runs because the cavity sits at one frequency taking data for a longer time duration. During the second data run, there were also six narrow features (≤ axion width). It is believed that these signals were also coupled into the cavity through the Cu rod. These were proven not to be from an axion signal by taking measurements without the magnetic field or by taking measurements off cavity resonance. When the cavity is refurbished for the next data run, it will have the thermal links positioned so there will be no degradation of Q, and correspondingly, no effective path for spurious RF signals to enter the cavity. We report the 90% exclusion limit for gγ based on the combined axion search data from runs 1 and 2 in Fig. 4. V. CONCLUSION We report results from the first haloscope axion detector to achieve sensitivity to cosmologically relevant couplings at masses above 20 μeV. The difficulty of reaching higher axion masses comes from the fact that the effective volume VCmnl of the cavity in which axion coupling can occur falls off rapidly with increasing frequency. Despite the difficulty of working in this mass range, we were able to reduce the total noise to 2.3 times the standard quantum limit, and set an exclusion limit of jgγ j ≳ 2.7 × jgKSVZ γ j over the range 23.15 < ma < 24.0 μeV. This sensitivity is already well into the space of plausible model couplings and best-estimate halo densities. That such a small experiment,of order 1% of the volume of prior experiments [25] is discovery-capable, is a remarkable achievement, and is primarily due to dramatic advances in amplifiers enabling operation very near the quantum limit. The experiment was further refined by the solution to the thermal coupling problem, addition of the Attocube tuning system, and improved shielding. This concludes the first phase of HAYSTAC’s operation. The next phase will include upgrades to the analysis and the implementation of a squeezed stated receiver which will allow us to push down even further in sensitivity [16].	We report on the results from a search for dark matter axions with the HAYSTAC experiment using a microwave cavity detector at frequencies between 5.6 and 5.8 GHz. We exclude axion models with two photon coupling gaγγ ≳ 2 × 10−14 GeV−1, a factor of 2.7 above the benchmark KSVZ model over the mass range 23.15 < ma < 24.0 μeV. This doubles the range reported in our previous paper. We achieve a nearquantum-limited sensitivity by operating at a temperature T<hν=2kB and incorporating a Josephson parametric amplifier (JPA), with improvements in the cooling of the cavity further reducing the experiment’s system noise temperature to only twice the standard quantum limit at its operational frequency, an order of magnitude better than any other dark matter microwave cavity experiment to date. This result concludes the first phase of the HAYSTAC program utilizing a conventional copper cavity and a single JPA.
10	2018	Measurement of time-dependent CP asymmetries in B0 → K0 Sηγ decays	H. Nakano,83 A. Ishikawa,83 K. Sumisawa,18,14 H. Yamamoto,83 I. Adachi,18,14 H. Aihara,85 S. Al Said,78,37 D. M. Asner,3 V. Aulchenko,4,66 T. Aushev,55 R. Ayad,78 V. Babu,79 I. Badhrees,78,36 V. Bansal,68 P. Behera,24 C. Beleño,13 B. Bhuyan,22 T. Bilka,5 J. Biswal,32 A. Bozek,62 M. Bračko,49,32 D. Červenkov,5 V. Chekelian,50 B. G. Cheon,16 K. Chilikin,44 K. Cho,38 S.-K. Choi,15 Y. Choi,77 S. Choudhury,23 D. Cinabro,89 S. Cunliffe,7 N. Dash,21 S. Di Carlo,42 Z. Doležal,5 S. Eidelman,4,66 J. E. Fast,68 T. Ferber,7 B. G. Fulsom,68 R. Garg,69 V. Gaur,88 N. Gabyshev,4,66 A. Garmash,4,66 M. Gelb,34 A. Giri,23 P. Goldenzweig,34 Y. Guan,25,18 E. Guido,30 J. Haba,18,14 T. Hara,18,14 K. Hayasaka,64 H. Hayashii,59 M. T. Hedges,17 S. Hirose,56 W.-S. Hou,61 T. Iijima,57,56 K. Inami,56 G. Inguglia,7 R. Itoh,18,14 M. Iwasaki,67 Y. Iwasaki,18 W. W. Jacobs,25 I. Jaegle,9 H. B. Jeon,41 S. Jia,2 Y. Jin,85 T. Julius,51 A. B. Kaliyar,24 G. Karyan,7 T. Kawasaki,64 C. Kiesling,50 D. Y. Kim,75 H. J. Kim,41 J. B. Kim,39 K. T. Kim,39 S. H. Kim,16 K. Kinoshita,6 P. Kodyš, 5 S. Korpar,49,32 D. Kotchetkov,17 P. Križan,45,32 R. Kroeger,52 P. Krokovny,4,66 T. Kuhr,46 R. Kulasiri,35 T. Kumita,87 Y.-J. Kwon,91 J. S. Lange,11 I. S. Lee,16 S. C. Lee,41 L. K. Li,26 Y. Li,88 Y. B. Li,70 L. Li Gioi,50 J. Libby,24 D. Liventsev,88,18 M. Lubej,32 T. Luo,10 J. MacNaughton,18 M. Masuda,84 T. Matsuda,53 M. Merola,29,58 K. Miyabayashi,59 H. Miyata,64 R. Mizuk,44,54,55 G. B. Mohanty,79 H. K. Moon,39 R. Mussa,30 E. Nakano,67 M. Nakao,18,14 T. Nanut,32 K. J. Nath,22 Z. Natkaniec,62 M. Nayak,89,18 M. Niiyama,40 S. Nishida,18,14 S. Ogawa,82 S. Okuno,33 H. Ono,63,64 P. Pakhlov,44,54 G. Pakhlova,44,55 B. Pal,6 S. Pardi,29 H. Park,41 S. Paul,81 T. K. Pedlar,47 R. Pestotnik,32 L. E. Piilonen,88 V. Popov,44,55 M. Ritter,46 A. Rostomyan,7 G. Russo,29 D. Sahoo,79 Y. Sakai,18,14 M. Salehi,48,46 S. Sandilya,6 L. Santelj,18 T. Sanuki,83 V. Savinov,71 O. Schneider,43 G. Schnell,1,20 C. Schwanda,27 A. J. Schwartz,6 Y. Seino,64 K. Senyo,90 M. E. Sevior,51 V. Shebalin,4,66 C. P. Shen,2 T.-A. Shibata,86 N. Shimizu,85 J.-G. Shiu,61 B. Shwartz,4,66 F. Simon,50,80 A. Sokolov,28 E. Solovieva,44,55 S. Stanič, 65 M. Starič, 32 J. F. Strube,68 M. Sumihama,12 T. Sumiyoshi,87 M. Takizawa,74,19,72 U. Tamponi,30 K. Tanida,31 F. Tenchini,51 K. Trabelsi,18,14 M. Uchida,86 T. Uglov,44,55 S. Uno,18,14 P. Urquijo,51 Y. Usov,4,66 C. Van Hulse,1 G. Varner,17 V. Vorobyev,4,66 A. Vossen,8 B. Wang,6 C. H. Wang,60 M.-Z. Wang,61 P. Wang,26 X. L. Wang,10 M. Watanabe,64 E. Widmann,76 E. Won,39 H. Ye,7 C. Z. Yuan,26 Y. Yusa,64 S. Zakharov,44,55 Z. P. Zhang,73 V. Zhilich,4,66 V. Zhukova,44,54 V. Zhulanov,4,66 and A. Zupanc	Measurement of time-dependent CP asymmetries in B0 → K0 Sηγ decays We report a measurement of time-dependent CP violation parameters in B0 → K0 Sηγ decays. The study is based on a data sample, containing 772 × 106BB¯ pairs, that was collected at the ϒð4SÞ resonance with the Belle detector at the KEKB asymmetric-energy eþe− collider. We obtain the CP violation parameters of S ¼ −1.32  0.77ðstatÞ  0.36ðsystÞ and A ¼ −0.48  0.41ðstatÞ  0.07ðsystÞ for the invariant mass of the K0 Sη system up to 2.1 GeV=c2. I. INTRODUCTION The radiative b → sγ decay proceeds dominantly via one-loop electromagnetic penguin diagrams at lowest order in the standard model (SM). Since heavy unobserved particles might enter in the loop, such decays are sensitive to new physics (NP). Precision measurements of the branching fraction for B → Xsγ by CLEO [1], BABAR [2–4] and Belle [5,6] are consistent with SM predictions [7,8] and give a strong constraint to NP models [9]. Another important observable that is sensitive to NP signatures in the b → sγ process is the photon polarization. Within the SM, the photon is mostly produced with left-handed polarization; the right-handed contribution is suppressed by ms=mb at leading order, where ms (mb) is the strange (bottom) quark mass. Various NP models, such as supersymmetry [10–15], left-right symmetric models [16] and extra dimensions [17–22], allow right-handed currents in the loops and hence can enhance the right-handed photon contribution [23–27]. Thus, a measurement of the photon polarization in the b → sγ process is an important tool to search for NP. Several methods have been proposed to measure the photon polarization in the b → sγ process. A measurement of time-dependent CP violation in B0 → P0 1P0 2γ is the most promising one, where P0 1 and P0 2 are scalar or pseudoscalar mesons and the P0 1P0 2 system is a CP eigenstate [28,29]. As the left- (right-)handed photon contributions are suppressed in B0 (B¯ 0) decays in the SM, an interference between B¯ 0 → P0 1P0 2γLðRÞ and B0 → P0 1P0 2γLðRÞ can generate a small mixing-induced CP violation parameterized by S ∼ −2ξCPðms=mbÞsin 2ϕ1 ∼ −0.02ξCP. Here, ξCP is the CP eigenvalue of the P0 1P0 2 system, and ϕ1 is an interior angle of the Cabibbo-Kobayashi-Maskawa unitarity triangle [30,31], defined as ϕ1 ≡ arg½−VcdV cb=VtdV tb. Potential contributions from NP-associated right-handed currents could enhance the value of S in the B0 → P0 1P0 2γ process [28,32–41]. At Belle and BABAR, the CP violation parameters for the b → sγ transition were measured in the decays of B0 → K0 Sπ0γ including K0 → K0 Sπ0 [42,43], B0 → K0 Sηγ [44], B0 → K0 Sρ0γ [45,46], and B0 → K0 Sϕγ [47]. All results are consistent with the SM prediction within the uncertainties [48–53]. In this paper, we report the first measurement of time-dependent CP violation in B0 → K0 Sηγ at Belle. The study is based on the full data sample of 711 fb−1 containing 772 × 106BB¯ pairs recorded at the ϒð4SÞ resonance with the Belle detector [54] at the KEKB eþe− collider [55]. II. TIME-DEPENDENT CP VIOLATION At the KEKB asymmetric-energy collider (3.5 GeV eþ on 8.0 GeV e−), the ϒð4SÞ is produced with a Lorentz boost of βγ ¼ 0.425 nearly along the z axis, which is antiparallel to the eþ beam direction. In the decay chain ϒð4SÞ → B0B¯ 0 → frecftag, one of the B mesons decays at proper time trec to a final state frec (our signal mode), and the other (Btag) decays at proper time ttag to a final state ftag that is used to determine the flavor of the signal B meson. The distribution of the proper time difference Δt ¼ trec − ttag is given bybetween the two B0 mass eigenstates, and q ¼ þ1 (−1) is the b-flavor charge when the tagging B meson is a B0 (B¯ 0). Since the B0 and B¯ 0 mesons are approximately at rest in the ϒð4SÞ center-of-mass (CM) frame, Δt can be determined from the displacement in z in the laboratory frame between the frec and ftag decay vertices: Δt ≃ ðzrec − ztagÞ=βγc≡ Δz=βγc, where zrec and ztag are the decay positions along the z axis of the signal and tag-side B mesons. III. BELLE DETECTOR The Belle detector [54] is a large-solid-angle magnetic spectrometer that consists of a silicon vertex detector (SVD), a 50-layer central drift chamber (CDC), an array of aerogel threshold Cherenkov counters (ACC), a barrellike arrangement of time-of-flight scintillation counters (TOF), and an electromagnetic caloriemeter (ECL) comprised of CsI(Tl) crystals. All these detector components are located inside a superconducting solenoid coil that provides a 1.5 T magnetic field. An iron flux-return located outside of the coil is instrumented with resistive plate chambers to detect K0 L mesons and muons. Two inner detector configurations were used: A 2.0 cm radius beampipe and a 3-layer SVD was used for the first sample of 152 × 106BB¯ pairs (SVD1), while a 1.5 cm radius beampipe, a 4-layer SVD and a small-inner-cell CDC were used to record the remaining 620 × 106BB¯ pairs (SVD2) [56]. IV. EVENT SELECTION The most energetic isolated cluster in the ECL in the CM frame of an event that is not associated with any charged tracks reconstructed in the SVD and CDC is selected as the prompt photon. Its energy must lie between 1.8 and 3.4 GeV. We require that its shower shape be consistent with an electromagnetic shower by imposing the criterion E9=E25 > 0.95 for the ratio of energy deposits in a 3 × 3 array of CsI(Tl) crystals to that in a 5 × 5 array, both centered on the crystal with the largest energy deposit. To reduce contamination from the decays π0 → γγ or η → γγ, the prompt photon candidate is paired with all other photons in the event with energy exceeding 40 MeV in the laboratory frame. We reject the event if the pair is consistent with the above decays, based on a likelihood constructed from the invariant mass, the energy and polar angle of the second photon in the laboratory frame [57]. Neutral pion candidates are reconstructed from two photons whose energies exceed 50 MeV in the laboratory frame. We require the invariant mass of the photon pairs to lie between 114 and 147 MeV=c2, which corresponds approximately to a 3σ window in resolution about the nominal π0 mass [58]. To reduce the combinatorial background, we retain candidates with a momentum greater than 100 MeV=c in the CM frame. Charged particles, except for pions from K0 S decays, are required to have a distance of closest approach to the interaction point (IP) within 5.0 cm along the z axis and 0.5 cm in the transverse plane. Charged kaons and pions are identified with a likelihood ratio constructed from specific ionization measurements in the CDC, time-of-flight information from the TOF, and the number of photoelectrons in the ACC. Neutral kaon (K0 S) candidates are reconstructed from pairs of oppositely charged tracks, treated as pions, and identified by a multivariate analysis [59] based on two sets of input variables [60]. The first set that separates K0 S candidates from the combinatorial background are: (1) the K0 S momentum in the laboratory frame, (2) the distance along the z axis between the two track helices at their closest approach, (3) the flight length in the x − y plane, (4) the angle between the K0 S momentum and the vector joining its decay vertex to the nominal IP, (5) the angle between the π momentum and the laboratory-frame direction of the K0 S in its rest frame, (6) the distances of closest approach in the x − y plane between the IP and the pion helices, (7) the numbers of hits for axial and stereo wires in the CDC for each pion, and (8) the presence or absence of associated hits in the SVD for each pion. The second set of variables, which identifies Λ → pπ− background that has a similar long-lived topology, are: (1) particle identification information, momentum, and polar angles of the two daughter tracks in the laboratory frame, and (2) the invariant mass calculated with the proton- and pion-mass hypotheses for the two tracks. In total, the first and second sets comprise 13 and 7 input variables, respectively. The selected K0 S candidates are required to have an invariant mass within 10 MeV=c2 of the nominal value, corresponding to a 3σ interval in mass resolution. We reconstruct η candidates from the γγ and πþπ−π0 final states, denoted as η2γ and η3π, respectively. For the η2γ mode, we require that the photon energy in the CM system be greater than 150 MeV. The candidates satisfying the diphoton invariant mass requirement of 510 MeV=c2 < Mγγ < 575 MeV=c2 are retained. For the η3π mode, the invariant mass of the three-pion system is required to be in the range 537 MeV=c2 < Mπππ < 556 MeV=c2. These requirements correspond to about 2σ windows in mass resolution. We reconstruct B candidates by combining a K0 S with an η and a γ candidate. We form two kinematic variables to select B mesons: the energy difference ΔE ≡ ECM B − ECM beam and the beam-energy constrained mass  B are the energy and momentum, respectively, of the B candidate in the CM system. We define the signal region in ΔE and Mbc for the measurement of CP violation as −0.15 GeV < ΔE < 0.08 GeV and 5.27 GeV=c2 < Mbc < 5.29 GeV=c2. To determine the signal fraction, a larger fitting region, jΔEj < 0.5 GeV and 5.20 GeV=c2 < Mbc < 5.29 GeV=c2, is employed. The average number of B candidates in an event with at least one candidate is 1.47; this is primarily due to multiple η candidates. If there is more than one B candidate in the fitting region, the candidate whose η daughter’s mass is closest to the nominal value is selected. If still necessary, the B candidate with the K0 S daughter’s mass closest to the nominal value is retained. V. BACKGROUND SUPPRESSION To suppress the dominant eþe− → qq¯ ðq ∈ fu; d; s; cgÞ continuum background, we use a neural network based on four input variables calculated in the CM frame: (1) the cosine of the angle between the B momentum and the z axis, (2) the likelihood ratio of modified Fox-Wolfram moments [61,62] that gives the strongest separation power, (3) the cosine of the angle between the third sphericity axes [63] calculated from the B candidate and all other particles in the rest of the event (ROE), and (4) the cosine of the angle between the first sphericity axis in the ROE and the z axis. The network is trained with a GEANT3-based Monte Carlo (MC) simulation [64]. The output variable, ONB, in the range [−1, 1], is used as one of the variables to determine the signal fraction. To enable a simple analytical modeling, ONB is transformed intowhere Omin NB and Omax NB are chosen to be −0.7 and 0.935 (0.915), respectively, for the η2γ (η3π) mode. The events with ONB < Omin NB are discarded; this selection keeps 80% (73%) of the signal while removing 92% (95%) qq¯ background for the η2γ (η3π) mode. The decay modes of the following CP eigenstates constitute peaking backgrounds: B0 → J=ψðηγÞK0 S, B0 → aXðηπ0ÞK0 S, B0 → D¯ 0ðK0 SηÞπ0, B0 → D¯ 0ðK0 SηÞη, B0 → D¯ 0ðK0 Sπ0Þη, and B0 → ηKXðK0 Sπ0Þ, where aX and KX represent a light unflavored resonance and a kaonic resonance, respectively. To suppress these backgrounds, we require 2.0 GeV=c2 < Mγη < 2.9 GeV=c2 or Mγη > 3.2 GeV=c2 to eliminate J=ψ → ηγ and aX → ηπ0, MKη < 1.82 GeV=c2 or MKη > 1.90 GeV=c2 to remove D¯ 0 → K0 Sη, and MγK > 2.0 GeV=c2 to suppress KX → K0 Sπ0 and D¯ 0 → K0 Sπ0, where a soft photon from the π0 decay is undetected. One of the decays arising from the b → sγ transition, B0 → K0 Sπ0γ, is a major peaking background. This decay is exclusively reconstructed and rejected if the candidate is found to satisfy the following requirements:VI. HELICITY ANGLE AND MASS DISTRIBUTIONS As the spin and invariant mass of the Kη system are not well known, we study Bþ → Kþηγ [65] assuming the isospin symmetry breaking to be small between B0 → K0ηγ and Bþ → Kþηγ [66]. The selections on Bþ → Kþηγ are the same as those on B0 → K0 Sηγ except for kaon selections. We define the helicity angle (θhel) as the angle between the K momentum and the opposite of the B-meson momentum in the Kη rest frame. The signal yields are extracted by fitting to ΔE and Mbc in bins of cos θhel and the Kþη invariant mass; later, the efficiency-corrected yield is obtained. We fit to the cos θhel distribution with spin-1 and spin-2 hypotheses, as a spin-3 resonance in B decays is only found in a B0 s decay and is highly suppressed compared to the spin-1 states [68]. Figures 1 and 2 show the background-subtracted and efficiency-corrected θhel and invariant-mass distributions for Bþ → Kþηγ. We find that the signal is concentrated in the region MKη < 2.1 GeV=c2 and has the signature of a spin-1 system. From these studies, we apply two selection criteria,VII. SIGNAL EXTRACTION We extract the signal yield with a three-dimensional extended unbinned maximum-likelihood fit to ΔE, Mbc, and O0 NB. For the signal ΔE–Mbc distribution, a twodimensional histogram is used as the two variables have 40% correlation due to the imperfect energy measurement for the prompt photon. The O0 NB distribution is modeled with the sum of two bifurcated Gaussian functions sharing a common peak position and right-side width. For the qq¯ background, the ΔE and Mbc distributions are parameterized by a second-order Chebyshev polynomial and an ARGUS function [69], respectively. The sum of a bifurcated Gaussian and a Gaussian function reproduces its O0 NB distribution. For background from B meson decays, the ΔE distribution is described by an exponential function; O0 NB is modeled with a bifurcated Gaussian function; the Mbc distribution is described by the sum of an ARGUS function and a Gaussian function. The fit results projected onto ΔE, Mbc and O0 NB are shown in Fig. 3. We obtain 69.5þ13.4 −12.4 and 22.4þ7.3 −6.4 signal events for the η2γ and η3π decay modes, respectively, with purities in the signal region of 28.4% and 22.5%.VIII. FLAVOR TAGGING The flavor of the Btag meson is determined from inclusive properties of particles in the ROE based on a multi-dimensional likelihood method. The algorithm for flavor tagging is described in detail elsewhere [70]. Two parameters, q defined in Eq. (2) and r, are used to represent the tagging information. The parameter r is an event-byevent MC-determined flavor tagging quality factor that ranges from 0 for no flavor information to 1 for unambiguously determined flavor. The data are sorted into seven intervals of r in which the fractions of wrongly tagged B flavor (wl, l ¼ 1; …; 7) as well as the differences between B0 and B¯ 0 (Δwl) are determined from self-tagged semileptonic and hadronic b → c decays. The total effective tagging efficiency, Σ½fl × ð1 − 2wlÞ2, where fl is the fraction of events in category l, is determined to be ð29.8  0.4Þ%. IX. VERTEX RECONSTRUCTION The vertex positions of signal-side decays of B0 → K0 Sη3πγ and B0 → K0 Sη2γ γ is determined from the charged tracks. For B0 → K0 Sη3πγ decays, we require at least one of the charged pions from η3π decays, which originate from the B decay position, to have at least one (two) hit in the SVD r − ϕ (z) layers. To improve the B-vertex resolution, we use an additional constraint from the transverse-plane beam profile at the IP (σbeam x ∼ 100 μm, σbeam y ∼ 5 μm) smeared with the finite flight length of the B0 meson in the x − y plane. The estimated uncertainty of the reconstructed vertex position in the z direction (σrec z ) determined with single (two) charged track is required to be less than 500 μm (200 μm) to ensure enough quality for time dependent analysis. For B0 → K0 Sη2γ γ decays, the K0 S trajectory, reconstructed from its pion daughters, is used to determine the vertex position with the aforementioned constraint on the smeared beam profile; this strategy is adopted since the decay vertex of the long-lived K0 S is displaced from the B decay vertex. To have good resolution of the K0 S trajectory, both pions daughters must satisfy SVD-hit requirements of at least one (two) hit in the r − ϕ (z) layers for SVD1, and at least two hits in both r − ϕ and z layers for SVD2. We apply a selection on the σrec z to be less than 500 μm. The vertex position of Btag is determined from well-reconstructed charged particles in the ROE [71]. The jΔtj is restricted to be less than 70 ps for further analysis. X. EVENT MODEL We determine S and A by performing an unbinned maximum-likelihood fit to the observed Δt distribution in the signal region. The probability density function (PDF) expected for the signal distribution, PsigðΔt; q; wl; Δwl; S; AÞ, is given by Eq. (2), modified to incorporate the effect ofincorrect flavor assignment. Two of the parameters in the PDF expression, τB0 and Δmd, are fixed to their world average [72]. The distribution is convolved with the proper-time resolution function, RsigðΔtÞ, which is a function of the event-by-event Δt uncertainties. The resolution function RsigðΔtÞ incorporates the detector resolution, contamination of nonprimary tracks in the vertex reconstruction of Btag, and the kinematic energy generated by the ϒð4SÞ decay. As in Ref. [73], universalRsig parameters are used for the vertex reconstruction for η3π and the long-lived K0 S. A detailed description can be found in Ref. [74]. The PDF for BB¯ background events (PBB¯ ) is modeled in the same way as for signal, but with different lifetime and CP violation parameters while using the same resolution function (RBB¯ ¼ Rsig). The effective lifetime of the BB¯ background is obtained from a fit to the MC sample for each η decay mode. The PDF for qq¯ background events, Pqq¯ , is modeled as the sum of exponential and prompt components, and is convolved with a double Gaussian representing the resolution function Rqq¯ . All parameters in Pqq¯ and Rqq¯ are determined by a fit to the Δt distribution of a backgroundenhanced sample in the ΔE–Mbc sideband. For each event i, the following likelihood function is calculated:where Pol is a broad Gaussian function that represents an outlier component with a small fraction fol [74]. The signal and background probabilities, fsig and fBB¯ , are calculated on an event-by-event basis from the function obtained by the same ΔE–Mbc–O0 NB fit used to extract the signal yield, and are then multiplied by a factor that depends on the flavor tagging r-bin. The r distributions of the signal and the qq¯ background are estimated by repeating the ΔE–Mbc–O0 NB fit procedure for each r interval with the three background shape parameters fixed to the full-range result. The BB¯ background distribution is estimated from MC samples and found to be small. XI. RESULTS The only free parameters in the final fit are S and A, which are determined by maximizing the likelihood function L ¼ ΠiPiðΔti; S; AÞ, where the product is over all events. We obtain S ¼ −1.32 and A ¼ −0.48 and find that the central values are outside of the physical boundary defined by S2 þ A2 ¼ 1. We extract the statistical uncertainties from the root-mean-square of the CP violation parameter distributions obtained using an ensemble test with input values of ðStrue; AtrueÞ ¼ ð−0.94;−0.34Þ, which is the closest point on the physical boundary to the fit result [75], as δS ¼ 0.77 and δA ¼ 0.41 [76]. The correlation between S and A is found to be 0.15. We define the raw asymmetry in each Δt interval as ðNq¼þ1 − Nq¼−1Þ=ðNq¼þ1 þ Nq¼−1Þ, where Nq¼1 is the number of observed candidates with the given q. The Δt distributions and raw asymmetries for events in the signalenhanced 0.5 < r ≤ 1.0 region for q ¼ 1 are shown in Fig. 4. XII. VALIDATIONS Various cross-checks are performed to confirm the validity of our procedure. The CP asymmetry fit to MC signal samples shows good linearity. Dedicated lifetime fits to Bþ → Kþηγ samples yield 2.0  0.3 ps and 2.3 0.4 ps for η2γ and η3π, respectively. A lifetime fit to B0 → J=ψK0 S using only K0 S to determine the signal vertex results in 1.528  0.027 ps. A CP asymmetry fit to the Bþ → Kþηγ control samples yields ðS; AÞ¼ð0.01 0.35; 0.06  0.29Þ and (0.2  0.6, 0.2  0.4) for η2γ and η3π, respectively. Lastly, a CP asymmetry fit to B0 → J=ψK0 S only using K0 S to determine the signal vertex position yields ðS; AÞ¼ð0.73  0.05; 0.00  0.03Þ. These results are consistent with either their world-average or expected values [58].XIII. SYSTEMATIC UNCERTAINTIES We calculate systematic uncertainties in the following categories by fitting the data with each fixed parameter being varied by its uncertainty: values of physics parameters such as Δmd and τB0 , effective lifetime and CP asymmetry of the BB¯ background, imperfect knowledge of the qq¯ background Δt PDF, the flavor-tagging determination, the signal and background fractions, and the resolution functions. A possible bias in the fit is checked by performing a large number of pseudo-experiments. The fit result is consistent with the input value within the statistical uncertainty. We quote this uncertainty as the possible fit bias. The uncertainty due to the vertex reconstruction is estimated by changing the requirements on the track quality. For the effect of SVD misalignment, we use the value from the latest sin 2ϕ1 measurement at Belle [77], which is estimated from MC samples by artificially displacing the SVD sensors in a random manner. Effects of tag-side interference [78] are estimated with a control sample of B → Dlν events. A detailed description of the evaluation of the systematic uncertainties is found in Ref. [79]. The dominant systematic contributions for S arisefrom the uncertainties in the resolution function and vertex reconstruction. The systematic uncertainty inA is dominated by the resolution function. These contributions are added in quadrature and summarized in Table I. XIV. CONFIDENCE LEVEL CONTOURS Figure 5 shows confidence intervals calculated using the Feldman-Cousins frequentist approach [80], incorporating a smearing by additional Gaussian functions to represent the systematic uncertainties discussed above. Our result is less than 2σ away from zero, and is consistent with the BABAR result [44] as well as the SM predictions [48–53] with the assumption that time-dependent CP asymmetries in B0 → K0γ and B0 → K0 Sηγ are the same. XV. CONCLUSION In summary, we have measured CP violation parameters in B0 → K0 Sηγ decays using a data sample of 772 × 106BB¯ pairs. The obtained parameters, S ¼ −1.32  0.77ðstatÞ  0.36ðsystÞ; A ¼ −0.48  0.41ðstatÞ  0.07ðsystÞ; are consistent with the null-asymmetry hypothesis within 2σ as well as with SM predictions [48–53]. Our measurement is dominated by statistical uncertainty. Therefore, with much higher statistics and also higher acceptance and reconstruction efficiencies, the forthcoming Belle II experiment should significantly improve upon the precision of this measurement.	We report a measurement of time-dependent CP violation parameters in B0 → K0 Sηγ decays. The study is based on a data sample, containing 772 × 106BB¯ pairs, that was collected at the ϒð4SÞ resonance with the Belle detector at the KEKB asymmetric-energy eþe− collider. We obtain the CP violation parameters of S ¼ −1.32  0.77ðstatÞ  0.36ðsystÞ and A ¼ −0.48  0.41ðstatÞ  0.07ðsystÞ for the invariant mass of the K0 Sη system up to 2.1 GeV=c2.
11	2017	Search for Lorentz and CPT violation using sidereal time dependence of neutrino flavor transitions over a short baseline		Search for Lorentz and CPT violation using sidereal time dependence of neutrino flavor transitions over a short baseline A class of extensions of the Standard Model allows Lorentz and CPT violations, which can be identified by the observation of sidereal modulations in the neutrino interaction rate. A search for such modulations was performed using the T2K on-axis near detector. Two complementary methods were used in this study, both of which resulted in no evidence of a signal. Limits on associated Lorentz and CPT-violating terms from the Standard Model extension have been derived by taking into account their correlations in this model for the first time. These results imply such symmetry violations are suppressed by a factor of more than 1020 at the GeV scale.I. INTRODUCTION While Lorentz invariance is a cornerstone of the Standard Model (SM) of particle physics, violations of this symmetry are allowed in a variety of models [1–3] at or around the Planck scale,mP ∼ 1019 GeV. At energies relevantto modern experiments, Lorentz invariance-violating (LV) processes are expected to be suppressed at least by ∼1=mP. Experimental observations of such phenomena would provide direct access to physics at the Planck scale, and precision tests have been performed to overcome this suppression (cf. Ref. [4] for a review). Neutrino oscillations can be used as a natural interferometer to probe even weak departures from this symmetry and have been studied with accelerator [5–10], reactor [11], and atmospheric [12,13] neutrinos. Lorentz and CPT symmetry violations can be described within the context of the Standard Model extension (SME) [14], an observer-independent effective field theory that incorporates all possible spontaneous LV operators with the SM Lagrangian. In general, the SME allows two classes of effects for neutrino oscillations, sidereal violations, in which the presence of a preferred spatial direction induces oscillation effects that vary with the neutrino travel direction, and spectral anomalies [15–17]. For a terrestrial fixed-baseline experiment, the rotation of the Earth induces a change in the direction of the neutrino target-detector vector relative to a fixed coordinate system such that a LV signal of the former type would manifest itself as a variation in the neutrino oscillation probability with sidereal time. This paper reports a search for evidence of siderealdependent νμ disappearance over an average baseline of 233.6 m using the T2K experiment. After introducing Lorentz invariance-violating oscillations within the SME and describing the T2K experiment, the selection of an analysis sample composed predominately of muon neutrinos inside the INGRID [18,19] detector is presented. Results of two complementary analyses of the data and concluding remarks follow thereafter. II. LV EFFECTS ON NEUTRINO OSCILLATIONS AT SHORT DISTANCES In this analysis, the LV is probed through νμ disappearance channel. In the SME framework, the disappearance probability of a νμ over short baselines is given by [16] where L is the distance travelled before detection. Equation (1) is valid as long as L ≪ Losc, where Losc is the typical distance of standard νμ → νb oscillations [20]. T⊕ is the local sidereal time, and ω⊕ ¼ 2π 23h56m4.0916s is the Earth’s sidereal frequency. Under a three-flavor neutrino hypothesis, oscillations of νμ to νe and ντ can occur. In general, the ten coefficients Cμb, ðAcÞμb, ðAsÞμb, ðBcÞμb, and ðBsÞμb (b ¼ e, τ) are functions of the neutrino energy E, the neutrino beam direction at the time origin (seebelow), and of 40 parameters within the SME which carry explicit Lorentz and CPT-violation information: ðaLÞα μb and ðcLÞ αβ μb (b ¼ e, τ) [21]. The ðaLÞα μb (ðcLÞ αβ μb) are constant coefficients associated with CPT-odd (even) vector (tensor) fields. It should be noted that the impacts of ðaLÞα ab and ðcLÞ αβ ab on the set of ten coefficients depend on the absolute direction of the neutrino baseline [21]. In the analysis to follow, a search for sidereal variations is performed relative to an inertial frame centered on the Sun, assuming it to be stationary during the data taking period. Other than the choice of the origin of the time coordinate, this frame is the same as in Ref. [22]. The time origin T ¼ 0 is chosen as January 1, 1970, 09∶00:00 Coordinated Universal Time. Data will be studied using the local sidereal phase (LSP), which is defined as LSP ¼ mod ðT⊕ω⊕=2πÞ. III. EXPERIMENTAL SETUP The T2K long-baseline neutrino experiment uses the collision of 30 GeV protons from the Japan Proton Accelerator Research Complex with a graphite target and focuses charged mesons produced in the subsequent interactions along the primary proton beam direction using a series of magnetic horns. Downstream of the production target is a 96-m-long decay volume in which these mesons decay to produce a beam of primarily muon neutrinos (99.3% νμ þ ν¯μ along the beam axis). This study is based on data accumulated from 2010 to 2013, divided into four run periods, and corresponds to 6.63 × 1020 protons on target (POT) exposure of the INGRID detector in the neutrino mode. The neutrino beam is defined by the beam colatitude χ ¼ 53.55087° in the Earth-centered frame with the same fixed axis as the Sun-centered frame. At the beamline location, a local frame is defined where the z axis corresponds to the zenith. The beam direction in this local frame is defined by the zenith angle θ ¼ 93.637°, and at the azimuthal angle, ϕ ¼ 270.319°. A more detailed description of the T2K experiment can be found in Ref. [18]. The INGRID detector is located 280 m downstream of the graphite target and is composed of 14 120 × 120 × 109 cm modules assembled in a cross-shaped structure. Each module holds 11 tracking segments built from pairs of orthogonally oriented scintillator planes interleaved with nine iron planes. The scintillator planes are built from 24 plastic scintillator bars connected to multipixel photon counters (MPPCs). Situated on the beam center, INGRID’s high event rate makes it well suited to a search for a sidereal variation in the νμ interactions. Although the νμ → νμ oscillation probability in Eq. (1) depends on the square of the neutrino flight length, the precise distance from creation to detection for each neutrino is unknown. Indeed, the neutrino’s parent meson may decay anywhere along the decay volume as shown in Fig. 1. As a result, the present analysis uses the mean of this distribution,Lave ¼ 233.6 m, as an effective distance travelled for all candidate events. Similarly, the mean neutrino energy of the flux at the INGRID detector, Eave ¼ 2.7 GeV, is used. IV. νμ EVENT SELECTION AND SYSTEMATIC UNCERTAINTIES A. INGRID νμ event selection To prevent LV oscillation-induced νe and ντ from washing out an LV effect on the νμ data, it is essential to select a sample with very high νμ purity. Since the ντ charged-current (CC) interactions have a 3.5 GeV production threshold, their cross section in the T2K energy range is very small. Their impact on the analysis was evaluated to be negligible. Consequently, no attempts were made to further reject them in the signal selection. Charged-current neutrino νμ interactions within INGRID are identified by a reconstructed track consistent with a muon originating in the detector fiducial volume and coincident in time with the expected arrival of neutrinos in the beam originated from a given proton bunch. In addition to a set of cuts to define a basic leptonlike sample [23], a likelihood function, hereafter referred to as muon confidence level (μCL), is used to further separate tracks produced by muons from showers produced by electrons or hadrons. This function is based on four discriminating variables: the number of active scintillator bars transverse to the beam direction averaged over the number of active planes, i.e., planes having at least one hit belonging to the track; the primary track’s length; the dispersion of the track’s energy deposition with distance; and the number of active scintillator bars close to the primary interaction vertex. The first three variables focus on the tendency for showers to have a broader transverse development and varying rate of energy deposition, whereas muons at T2K energies are minimum ionizingand are more longitudinally penetrating. The fourth variable is based on a region defined by only the two planes upstream and downstream of the event vertex and is useful for discriminating against showers with additional particles near the event vertex and proton-induced activity. Since the total neutrino flux is constant and the neutral current (NC) cross section is the same for each neutrino flavor, the NC event rate within INGRID is expected to be constant with sidereal time. Accordingly, no additional cuts to remove NC events are used. Figure 2 shows the μCL likelihood distribution for reconstructed data and Monte Carlo (MC) νμ CC, νe CC, and NC interactions. A cut on μCL ≥ 0.54 has been selected to ensure that the νe contamination of the final sample is smaller than the statistical error on the νμ component while maximizing the νμ statistics. After applying all analysis cuts, the νμ CC selection efficiency is ϵμ ¼ 44.0%. The corresponding νe efficiency, ϵe, has been reduced to 13.3%. There are 6.75 × 106 events remaining in the final sample, which provides an average statistical error of 0.22% in each of the 32 analysis bins (defined below). If an oscillation effect equivalent to three times the statistical error on the νμ component appears as νe in the final sample, the resulting contamination will be 0.2%. Assuming no oscillation due to LV effect, the final sample has 3.4% NC events. B. Timing corrections and systematic uncertainties The operation of the T2K beam is not constant in time and varies with the hour of the day and season of the year. The effect of time-dependent changes in the neutrino event rate must be corrected since they can mimic an LVoscillation signal or reduce the analysis sensitivity. Such effects can be separated into two distinct classes depending on whether they alter the neutrino beam itself or the INGRID detector. The first class consists of three timedependent corrections considered for the neutrino beam: (i) Beam center variations during each run.—Since the neutrino interaction rate itself is insufficient to estimate these variations, muons collected spill by spill with a muon detector just downstream of the decay volume [24] are used to estimate the beam center position. For each of the four run periods considered in this exposure, the beam center position as a function of LSP is estimated after correcting for tidal effects at the detector. These data are then used to extrapolate the position of the neutrino beam center, which is aligned with the muon direction, at INGRID. LSP-dependent corrections to the observed event rate at INGRID due to shifts in the neutrino beam center are estimated using MC. (ii) Beam center variation between runs.—Changes in the average beam center position between run periods are evaluated using the INGRID neutrino data, and a correction is estimated and applied as in the above. (iii) Beam intensity variation between runs and nonuniform POT exposure as a function of LSP.—A correction is applied to bring the event rate per POT in each LSP bin in line with the average for the entire run. The correction is applied for each event based on its run and sidereal phase. A further correction is applied to make the average event rate per POT of each run consistent with that of a reference run chosen to be near the end of the data taking period. The second class of effects consists of three additional corrections to account for changes in the response of INGRID: (i) Event pileup variations.—Typically, only single interactions in an INGRID module are reconstructed, and other interactions in the same data acquisition timing window (one for each neutrino bunch) are lost (pileup events). However, changes in the beam intensity affect the probability of multiple interactions within an INGRID reconstruction timing window. Accordingly, events at INGRID are corrected as a linear function of LSP to account for the variation in pileup events with variations in the beam intensity. The number of lost pileup events varies between 3% and 7% across the INGRID modules. (ii) Dark noise variations.—Variations in the temperature and humidity affect the MPPC dark rate, which in turn weakly affects the neutrino detection efficiency. The maximal variations of the dark rate with the sidereal time is 2%. A correction to account for this efficiency variation has been applied linearly with the dark rate.(iii) Variations in the photosensor gain.—The MPPC gain is influenced by environmental changes, and the scintillator gain might decrease over time. Gain changes impact both the reconstruction and the analysis selection and are corrected using a sample of beam-induced muon interactions in the rock upstream of INGRID. The effect of variations in the charge at the minimum ionization peak of these muons is simulated in MC and used to correct the neutrino event rate. The size of the correction varies with LSP and does not exceed 1%. The validity of the above corrections has been tested by separating the analysis data set into day and night subsamples. Though time-dependent differences are expected in the split samples due to, for instance, cooler temperatures at night or beamline maintenance during the day, the data should be consistent with one another when viewed in the LSP coordinate if the above corrections have been applied consistently. Figure 3 shows the day and night distributions as a function of LSP. The agreement between the day and night distribution is evaluated with a Pearsons chi-squared test, and a corresponding χ2=NDF ¼ 28.3=32 has been found. Data before and after all corrections also appear in the figure. Systematic errors for each of the corrections have been evaluated and are listed in Table I. The total systematic error is 0.08%, which is small when compared to the statistical error of the final sample, 0.22%. V. ANALYSIS METHODOLOGY AND RESULTS The analysis of the final data sample is performed in two stages. First, the compatibility of the data with a null signal is studied using a fast Fourier transform (FFT) method (Sec. VA). This method explicitly searches for a sidereal modulation and ultimately provides an estimate of the power of each Fourier mode from a potential signal. Then, constraints on the parameters appearing in Eq. (1) are extracted using a likelihood method (Sec. V B) that includes their correlations. Figure 4 shows examples of the expected LSP distribution for MC generated under three signal assumptions. A. Fast Fourier transform result Expanding Eq. (1) indicates that LV oscillations are described by four harmonic sidereal frequencies, fi ¼i·ω⊕, i ∈ ½1; 4, and a constant term. The FFT [25,26] method is most efficient for N ¼ 2L bins, and the sensitivity of the current analysis is found to be optimal when L ¼ 5. Data are therefore divided into 32 evenly spaced LSP bins for input into the FFT, and the magnitudes of the four Fourier modes, jFij, are then estimated. Note that the constant term is not considered in this study due to large uncertainties in the beam flux normalization. A 3σ detection threshold has been determined as the power in a Fourier mode for which 0.3% of MC experiments generated without LV effects shows higher power. For each mode, this threshold corresponds to jFij > 0.026. The results of the fit to the data are shown in Table II together with a p value estimating the likelihood that the observed power was produced by a statistical fluctuation of the null (no LV) hypothesis. All jFij are below the 3σ detection threshold and indicate no evidence for a LV signal. Constraints on the SME coefficients can be extracted with the FFT method [7,21] under the assumption that the parameters above are uncorrelated. However, since the data sets are reduced to the four amplitudes and the relatively large number of parameters in the oscillation function, correlations are expected. Figure 5 shows the probability for data without LV to yield more power in the Fourier modes than the average expected for a LV signal as a function of the SME coefficients ðaLÞX μe and ðcLÞTX μe . The parameters exhibit a high degree of anticorrelation, indicating that in the event of a null observation as above using the FFT method without considering these correlations may lead to an underestimation of the parameter limits. As the parameters in Eq. (1) are functions of these coefficients, they might be also expected to exhibit correlations. Accordingly, a likelihood method has been developed to fully incorporate these correlations when making parameter estimations. B. Likelihood analysis Because of the large number of SME parameters [21] relative to the number of observables, this analysis does not estimate the ðaLÞX ab and ðcLÞTX ab parameters but the Cμb, ðAcÞμb, ðAsÞμb, ðBcÞμb, ðBsÞμb (b ¼ e, τ) parameters from Eq. (1) using a likelihood method that fully incorporates their correlations and the experimental uncertainties. However, since the impact of systematic errors is negligible (cf. Table I), only the statistical uncertainty in each LSP bin is considered here. Further, each parameter is assumed to be real valued. Sensitivity studies without this assumption showed no significant constraint on the complex phases of these parameters with the present data. Under these conditions, a simultaneous fit for ten real parameters using the data and binning from the previous section has been performed. Since the parameters are highly correlated, the contours and limits are not estimated assuming a profiling method but instead using a likelihood marginalization which genuinely preserves their correlations [27]. This analysis assumes flat priors for all the parameters since no LV has been discovered so far. The results of the fit are shown in Table III. As expected from the FFT method, no indications of LV oscillations are found, and 2σ upper limits are set for each parameter. Those limits are compared with the sensitivity obtained by determining the parameter absolute values for which 5% of some MC experiments generated without LV effects shows higher absolute values. The contour limits are constructed following a constant Δχ2 method and are shown in Fig. 6 for the ðAcÞμe and ðAsÞμe parameters that show important anticorrelations. While correlatedparameter analyses have been performed elsewhere [22], this is the first search to do so using all ten parameters simultaneously. The five harmonics in Eq. (1) heavily correlate the ten parameters as shown in Fig. 6. VI. CONCLUSIONS The T2K experiment has performed a search for Lorentz and CPT-invariance violations using the INGRID on-axis near detector. Two complementary analysis methods have found no evidence of such symmetry violations for the energy, neutrino baseline, and data set used [28]. Not only are the data consistent with an LSP-independent event rate based on a FFT analysis, but a likelihood analysis incorporating parameter correlations has corroborated this finding and yielded constraints on ten SME parameters.	A class of extensions of the Standard Model allows Lorentz and CPT violations, which can be identified by the observation of sidereal modulations in the neutrino interaction rate. A search for such modulations was performed using the T2K on-axis near detector. Two complementary methods were used in this study, both of which resulted in no evidence of a signal. Limits on associated Lorentz and CPT-violating terms from the Standard Model extension have been derived by taking into account their correlations in this model for the first time. These results imply such symmetry violations are suppressed by a factor of more than 1020 at the GeV scale.
12	2017	Chern-Simons five-form and holographic baryons	Pak Hang Chris Lau1and Shigeki Sugimoto	Chern-Simons five-form and holographic baryonsIn In the top-down holographic model of QCD based on D4/D8-branes in type IIA string theory and some of the bottom-up models, the low energy effective theory of mesons is described by a five-dimensional Yang-Mills-Chern-Simons theory in a certain curved background with two boundaries. The fivedimensional Chern-Simons term plays a crucial role in reproducing the correct chiral anomaly in fourdimensional massless QCD. However, there are some subtle ambiguities in the definition of the Chern-Simons term for the cases with topologically nontrivial gauge bundles, which include the configurations with baryons. In particular, for the cases with three flavors, it was pointed out by Hata and Murata that the naive Chern-Simons term does not lead to an important constraint on the baryon spectrum, which is needed to pick out the correct baryon spectrum observed in nature. In this paper, we propose a formulation of a well-defined Chern-Simons term which can be used for the cases with baryons, and show that it recovers the correct baryon constraint as well as the chiral anomaly in QCD. I. INTRODUCTION The gauge/gravity duality provides a powerful method to study strongly coupled gauge theories using theories with gravity [1–3]. One of its surprising features is that the space-time dimensions of the gravity side are higher than those of the corresponding gauge theory. For this reason this type of duality is called holographic duality. It has been applied to QCD and there have been a lot of successes in revealing the properties of QCD and physics of hadrons.1 The holographic dual description of QCD (or QCD-like theory) is called holographic QCD. A common feature of the holographic models is that the meson effective action is given as a five-dimensional gauge theory embedded in a certain curved background. In this paper, our main focus is on the five-dimensional Chern-Simons (CS) termwhere C is a constant and ω5ðAÞ is the CS five-form that satisfies dω5ðAÞ ¼ trðF3Þ. The explicit form of the CS five-form isIt appears in the meson effective action in the top-down holographic model of QCD proposed in [5]3 and some of the bottom-up models (see, e.g., [7–10]). In these models, the effective theory of mesons is described by a fivedimensional UðNfÞ Yang-Mills-Chern-Simons (YM-CS) action on a curved space-time M5, where Nf is the number of massless quarks, and the coefficient of the CS term is related to the number of colors Nc byThe normalizable modes of the five-dimensional UðNfÞ gauge field A correspond to the degrees of freedom of a tower of vector and axial vector mesons (such as rho meson, omega meson, a1 meson, etc.) as well as the massless pions.4 It has been shown that the masses as well as coupling constants for low-lying mesons read off from the five-dimensional YM-CS theory turn out to be inprovide some predictions for the unknown parameters. The CS term plays crucial roles in many aspects in holographic QCD. First of all, the chiral anomaly in QCD is correctly reproduced due to the CS term. In fact, the five-dimensional expression of the Wess-ZuminoWitten (WZW) term in QCD [12–14] has a direct physical interpretation in terms of the five-dimensional CS term in holographic QCD [5]. Furthermore, some of the decay modes of the omega meson (ω → π0γ and ω → π0πþπ−) are induced by terms generated from the CS term. Surprisingly, the structure of the interaction terms for these decay modes predicted by holographic QCD agrees with that of the Gell-Mann–Sharp–Wagner model[15], which is a phenomenological model proposed to reproduce the experimental data of the omega meson decay [16] (see also [9]). The CS term is also important in the analysis of baryons. Due to the CS term, it can be shown that the baryon number is equal to the instanton number defined on a time slice [5]. When the vector (and axial-vector) mesons are integrated out, the five-dimensional YM-CS action reduces to the action of the Skyrme model [5,16]. The Skyrme model was proposed by Skyrme to describe baryons as topological solitons called Skyrmions [17]. The pion field in the soliton has a nontrivial winding number representing an element of the homotopy group π3ðUðNfÞÞ ≃ Z. The relation between the instanton number for the five-dimensional gauge field and the winding number carried by the pion field is precisely that proposed by Atiyah and Manton [18] in an attempt to obtain approximate Skyrmion solutions by using instanton solutions. However, there are some subtle ambiguities in the definition of the CS term. In the explicit expression of the CS term in (1.1) with (1.2), we have implicitly assumed that the gauge field A is a globally well-defined one-form on the five-dimensional space-time M5. This is, however, not always possible when the gauge configuration with a given boundary condition is topologically nontrivial, including the cases with baryons. In such cases, it is necessary to cover the five-dimensional space-time M5 by multiple patches on which the gauge field is well defined. One might naively think that the CS term can be defined as just a sum of the CS term defined on each patch. However, this approach does not work, because it depends on the choice of the gauge, and some additional terms are needed to make it well defined. Related to this issue, a problem was pointed out by Hata and Murata in [19]. They tried to analyze the spectrum of baryons in the case with Nf ¼ 3, generalizing the analysis for Nf ¼ 2 in [20], and claimed that a constraint needed to get the correct baryon spectrum [see (2.29)] cannot be obtained by using the naive CS term. They proposed a new CS term that gives the correct constraint, but it does not reproduce the chiral anomaly of QCD. Our main goal is to propose a welldefined CS term that solves all these problems. The paper is organized as follows. We start with reviewing the problems in more detail while fixing our notation in Sec. II. Our proposal for the well-defined CS term is given in Sec. III. In Sec. IV, we revisit the analysis of the effective action for the collective coordinates of the soliton solution representing baryons and show that the correct constraint is obtained from the new CS term. Section V gives a summary and outlook. II. PUZZLE A. The model Our starting point is the five-dimensional UðNfÞ YM-CS action given bywhere κ is a constant and * is the Hodge star in fivedimensional space-time M5. Although the details of the metric on M5 are not important in our main purpose, we use the following form of the metric for explicit calculations: B. Problems of the CS term In order to illustrate the problem clearly, let us compactify the time and x1∼3 directions, and consider the case that the topology of the space-time is equivalent toThis expression inevitably vanishes if we impose the boundary condition Ajz→∞ ¼ 0. Therefore, if we adopt the identification Aˆ  ¼ Ajz→∞ in the previous subsection, the globally well-defined gauge field A can describe only the nB ¼ 0 sector of the gauge configuration, when the external gauge fields Aˆ  are turned off. This is clearly restricting the gauge configurations too much. As usual in gauge theory, we should include the gauge configurations defined on topologically nontrivial gauge bundles. In order to describe gauge configurations with nonzero baryon number, we cover the space-time manifold M5 with two patches asThe question now is how to define the CS term in this setup. While the CS term is supposed to give the correct chiral anomaly, we should make sure that it is invariant (up to a 2π shift) under the gauge transformations with gˆ ¼ 1 that act trivially at the boundary. One can immediately see that a naive expression likedoes not work. This is one of the reasons that the naive CS term has to be modified. Another approach is to insist on a globally well-defined gauge field A, and modify the relation between the boundary values of the gauge field and the external gauge field associated with the chiral symmetry. This can be achieved from the above description by the gauge transformation (2.19) with g ¼ h satisfying hþhh−1 − ¼ 1 on M− 5 ∩ Mþ 5 . Then, the gauge field A defined aswhere hˆ  ≡ hjz→∞. It is important to note that a gauge configuration is specified by the pair ðA; hˆ Þ. Two gauge configurations with the same gauge field ðA; hˆ Þ and ðA; hˆ0 Þ can be physically inequivalent when hˆ  and hˆ0  are different. It is easy to see that, with the identification (2.23), the expressions for the baryon number (2.14) and (2.20) are identical. When the external gauge fields are turned off, the boundary values of the gauge field are given by Ajz→∞ ¼ hˆ dhˆ−1  and the baryon number (2.14) is given by the difference of the winding number carried by hˆ þ and hˆ − aswith a UðNfÞ-valued function g on M5. This gauge transformation does not act on the external gauge fields Aˆ  and hence the gauge configurations ðA; hˆ Þ and ðAg; ghˆ Þ are physically equivalent. The problem is thatω5ðAÞ andω5ðAgÞ are not equal [see (A16)] and it is not clear which one we should use. Moreover, the naive CS term (1.1) does not reproduce the expression (2.7) for the chiral anomaly. Because of the boundary condition (2.23), the relation between the boundary values of the gauge function g in the gauge transformation A → Ag and the gauge function for the gauged chiral symmetry gˆ is modified asThen, the transformation ðA;hˆÞ→ðAg;hˆÞ induces Aˆ  →Aˆ gˆ  as desired. For the infinitesimal gauge transformation with g ≃ 1 − Λ and gˆ ≃ 1 − Λˆ , (2.26) gives Λˆ  ¼ ðhˆ−1  Λhˆ Þjz→∞ and hence the infinitesimal gauge transformation of the naive CS term (1.1) iswhich does not agree with (2.7) in general. In addition to these issues, there is a more practical problem of the CS term pointed out by Hata and Murata in [19]. They studied the spectrum of baryons in holographic QCD with Nf ¼ 3. The analysis is similar to that for the three-flavor Skyrme model. In the Skyrme model, baryons are represented as topological solitons called Skyrmions in a theory of pions. There are collective coordinates corresponding to the SUð3Þ rotation (for Nf ¼ 3) of the Skyrmion solution, which are denoted by a ∈ SUð3Þ. (See Sec. IV C.) It has been shown that the WZW term giveswhere M6 is a six-dimensional manifold with ∂M6 ¼ M5. Although they succeeded in recovering the correct constraint by using this new CS term, it is also problematic. First, as emphasized above, M5 has boundaries and the meaning of “∂M6 ¼ M5” is not clear, because ∂M5 ¼ ∅ is a necessary condition to have such M6. Furthermore, this term is manifestly gauge invariant and it does not recover the chiral anomaly (2.7). III. PROPOSAL In this section, we propose a new CS term that solves all the problems discussed in the previous section. A. Proposal for the CS term Using the notation introduced in Sec. II B, our proposal for the CS term is given byUseful formulas for the CS five-form ω5ðAÞ and the fourform α4ðV;AÞ can be found in Appendix A 3. Note that the last term in (3.1) can be replaced withusing (A19). The third and fourth terms in (3.1) are added to the naive expression (2.21). The motivation for adding these terms will soon become clear. A few comments are in order. In (3.1), we have assumed  is a disk satisfying ∂D ¼ S1, and then h~ exists because the image of h, as a map from S1 to SUðNfÞ at each point in S3, is contractible in SUðNfÞ. The choice of Nð0Þ 5 and h~ does not matter, due to the standard argument for the WZW term [13]. This new CS term has the following desired properties: 1. It reduces to (1.1) when h is topologically trivial. 2. It is invariant (up to a 2πZ shift) under the gauge transformation (2.19) with gjz→∞ → 1. 3. It reproduces the correct chiral anomaly in QCD  Fig. 2 for the picture in the limit ϵ → 0). Let us show these properties one by one. 1. When h is topologically trivial, i.e. h can be continuously deformed to h ¼ 1, there exists a  Under the gauge transformation (2.19), the CS term (3.1) is transformed as where h0 ≡ gþhg−1 − and h~0 are UðNfÞ-valued functions on M− 5 ∩ Mþ 5 and Nð0Þ 5 , respectively, satisfying h~0 j ∂Nð0Þ 5 ¼ h0 jz¼0. Note that since gjz¼0 are topologically trivial due to the boundary conditions gjz→∞ → 1, there exist UðNfÞ-valued functions where G ≡ dg−1  g and G~  ≡ dg~−1  g~. The first and second lines are Snew CS defined in (3.1). The third and forth lines can be omitted because they take values in 2πZ. 3. Here, we consider the infinitesimal gauge transformation with gˆ ≃ 1 − Λ. 13 In this case, gjz¼0 is again topologically trivial and it suffices to show property 3 for the cases with g ¼ 1 on M− 5 ∩ Mþ 5 , because of the property 2 shown above. Then, since the third and fourth terms in (3.1) do not change under the gauge transformation, the proof of (2.7) is the same as that reviewed in Sec. II A. 4. Using the relations ∂M 6 ¼ M 5 ∪ ðNð0Þ 5 Þ and the Stokes’ theorem, we obtainB. Other useful expressions It is often more useful to use the globally well-defined gauge field A defined in (2.22) to describe the CS term. A similar analysis as in (3.6)–(3.7) shows that the new CS term (3.1) can be rewritten asC. Pion field The relation between the UðNfÞ-valued pion field UðxμÞ in the chiral Lagrangian and the five-dimensional gauge field was proposed in [5,7,18]:D. Equations of motion and current For later use, let us write down the equations of motion and currents with our new CS term. Since the additional terms in our new CS term do not affect these equations, the results in this subsection are not new. Nevertheless, it will be instructive to show them explicitly. The action (2.1) is replaced withHere, we use the expression (3.12) for the CS term Snew CS . Using (A22) and (A26), an infinitesimal variation of the action is computed as15 IV. APPLICATION TO BARYONS In this section, we analyze the effective action for the collective coordinates of the soliton solution corresponding to baryons. We show that the term (2.28) needed to obtain the correct constraint (2.29) is reproduced by using the CS term proposed in the previous section. This statement was already shown in [19] using (2.31) for the nB ¼ 1 case. As we have seen in Sec. III B that our CS term reduces to (2.31) when Aˆ ¼ 0, we should recover their result. In our derivation, we will not use an explicit solution corresponding to a baryon so that it can be generalized to the cases with nB > 1. A. Collective coordinates In this subsection, we work in the A0 ¼ 0 gauge. We assume there exists a solution of the equations of motion (3.31) with nonzero baryon number nB, denoted aswhere “cl” refers to a classical solution and M ¼ 1, 2, 3, z is the spatial index. We also assume that this gauge field is globally well defined and regular everywhere in M5. Here, we consider the cases with Aˆ  ¼ 0. Then, for a finite energy solution, the gauge field approaches a pure gauge configuration near the boundary asBecause of the condition A0 ¼ 0, hcl  are time independent. Without loss of generality, we can assume hcl −jz→−∞ ¼ 1 and hcl þjz→þ∞ ≡ h0ðx⃗Þ, where h0 is a UðNfÞ-valued function on the S3 parametrized by x⃗ ¼ ðx1; x2; x3Þ satisfying with a globally well-defined SUðNfÞ-valued function V. 17 The idea is as follows. If V is time independent, it can be regarded as the collective coordinates (coordinates of the instanton moduli space) corresponding to the global gauge rotation, since AM is again a classical solution with the same energy. A standard procedure of the moduli space quantization method18 is to promote the collective coordinates to be time-dependent variables and reduce the system to a quantum mechanics of these variables. To this end, one should also make a compensating gauge transformation so that the gauge configuration satisfies the Gauss law equation, which is the equation of motion for A0:V in (4.4) contains both the collective coordinates and the compensating gauge transformation, and it can depend on the five-dimensional space-time coordinates. We assume that the initial value of V is 1 and hence its value at a fixed time is connected to V ¼ 1 by a continuous deformation. With this choice of the gauge configuration, the asymptotic value of the gauge field isB. Effective action To obtain the effective action for aðtÞ, it turns out to be more convenient to make a gauge transformation (2.25) using g ¼ V−1. Then, the configuration in (4.4) is mapped toHere, Nðþ∞Þ 5 is assumed to be Nðþ∞Þ 5 ≃ D × S3, and h0 and aþ are extended to be functions on it. We can choose h0 and aþ to be constant along the D and S3 directions, respectively. Using the relation (A28), one can show that (4.18) is equivalent toAlthough it is a bit more tedious, it is also possible to derive (4.19) directly from (4.10) by using (A16) with g ¼ V−1. 19 The first term on the right-hand side of (4.19) can be evaluated as follows. The relation (A22) with δA ¼ Φdt impliesC. Relation to Skyrmions The action of the Skyrme model is written in terms of the pion field UðxμÞ discussed in Sec. III C. The classical solution corresponding to the baryon carries nonzero winding number as an element of π3ðUðNfÞÞ ≃ Z. In the standard approach for Nf ¼ 3, the ansatz for the field configuration iswhere Uclðx⃗Þ ∈ SUð3Þ is a classical solution representing a baryon and aðtÞ ∈ SUð3Þ is the collective coordinates corresponding to the SUð3Þ rotation. The classical solution is assumed to be of the formwhere U0ðx⃗Þ is the Skyrmion solution for Nf ¼ 2. The form of the solution (4.35) is natural in the sense that exciting the components of the mesons with a strange quark costs more energy than those with only up and down quarks, when we include the mass term to the Lagrangian. The pion field (3.25) for our gauge configuration (4.16) is given byThese equations can be consistently truncated by restricting FSUð2Þ 0M ¼ 0 and FUð1Þ MN ¼ 0 for M, N ¼ 1, 2, 3, z. In this case, only the Uð1Þ part of the gauge field contributes in (4.32) and the classical quark number matrix ncl Q for Nf ¼ 2 is proportional to the unit matrix. When the solution for Nf ¼ 2 is embedded into the Uð3Þ gauge field, ncl Q is of the formV. CONCLUSION AND OUTLOOK In this paper, we reexamined a puzzle concerned with the CS term in the five-dimensional meson effective theory ofholographic QCD. We proposed a modified CS term and demonstrated that the new action successfully reproduces the required baryon constraint as well as the chiral anomaly. Although we obtained a CS term that can be used for the topologically nontrivial gauge configurations corresponding to baryons, our construction is not completely general. For example, the expression (3.12) is applicable only when N5 and h can be constructed and the gauge field can be treated as a globally well-defined one-form field on M5. For the expression (3.22), we have to assume the existence of M6 and N5 as well as an extension of the gauge fields to these spaces. (See the footnote on p. 12 for further comments.) It would be interesting to investigate an expression of the CS term that works for more generic situations, as it was done in [33] for the three-dimensional CS term. The main motivation for the present work is to solve a puzzle concerned with baryons in holographic QCD with Nf ¼ 3 and make it applicable to the physics of baryons including strange quarks. In order to be more realistic, it would be important to include the mass of the strange quark. There are already some works along this direction. (See, e.g., [19,34–38].) We hope our work removes possible concerns on the validity of the formulation and provides some new insight into application of holographic QCD to hyperons. 	In the top-down holographic model of QCD based on D4/D8-branes in type IIA string theory and some of the bottom-up models, the low energy effective theory of mesons is described by a five-dimensional Yang-Mills-Chern-Simons theory in a certain curved background with two boundaries. The fivedimensional Chern-Simons term plays a crucial role in reproducing the correct chiral anomaly in fourdimensional massless QCD. However, there are some subtle ambiguities in the definition of the Chern-Simons term for the cases with topologically nontrivial gauge bundles, which include the configurations with baryons. In particular, for the cases with three flavors, it was pointed out by Hata and Murata that the naive Chern-Simons term does not lead to an important constraint on the baryon spectrum, which is needed to pick out the correct baryon spectrum observed in nature. In this paper, we propose a formulation of a well-defined Chern-Simons term which can be used for the cases with baryons, and show that it recovers the correct baryon constraint as well as the chiral anomaly in QCD.
13	2016	Entropy production from chaoticity in Yang-Mills field theory with use of the Husimi function	Hidekazu Tsukiji,1,Hideaki Iida,2 Teiji Kunihiro,2 Akira Ohnishi,1 and Toru T. Takahashi3	Entropy production from chaoticity in Yang-Mills field theory with use of the Husimi function We investigate possible entropy production in Yang-Mills (YM) field theory by using a quantum distribution function called the Husimi function fHðA; E; tÞ for the YM field, which is given by a coarse graining of the Wigner function and non-negative. We calculate the Husimi-Wehrl entropy SHWðtÞ ¼ −TrfH log fH defined as an integral over the phase space, for which two adaptations of the test-particle method are used combined with Monte Carlo method. We utilize the semiclassical approximation to obtain the time evolution of the distribution functions of the YM field, which is known to show chaotic behavior in the classical limit. We also make a simplification of the multidimensional phase-space integrals by making a product ansatz for the Husimi function, which is found to give a 10–20% overestimate of the HusimiWehrl entropy for a quantum system with a few degrees of freedom. We show that the quantum YM theory does exhibit the entropy production and that the entropy production rate agrees with the sum of positive Lyapunov exponents or the Kolmogorov-Sinai entropy, suggesting that the chaoticity of the classical YM field causes the entropy production in the quantum YM theoryI. INTRODUCTION Thermalization or entropy production in an isolated quantum system is a long-standing problem. The entropy of a quantum system may be given by von Neumann entropy SvN ¼ −Trρ log ρ with ρ being the density matrix [1], and taking into account that the time evolution of the quantum system is described by a unitary transformation UðtÞ ¼ e−iHt=ℏ, von Neumann entropy SvN is shown to remain unchanged in time, which is an absurd consequence in contradiction to reality. One possible way to avoid this puzzle is to assume that there is no isolated quantum system because any quantum system is surrounded by the environment composed of quantum fields described by, say, QED; the partial trace with respect to the environment would lead to a density matrix of a mixed state due to the entanglement [2]. For thermalization of a macroscopic quantum system, the old idea of von Neumann was recently rediscovered, and since then a lot of related works and developments have been made [3,4]; see Ref. [5] and references cited therein. It might be worth mentioning that the entanglement entropy of a quantum system may have a geometrical interpretation as is clearly shown by Ryu-Takayanagi’s formula [6]. In this work, we do not intend to develop a master theory to describe thermalization or entropy production of a generic quantum system. Instead, we concentrate on entropy production in quantum systems of which the classical counterparts are chaotic and the semiclassical approximation is valid. There are many physical systems satisfying these characteristics [7]: among them, we have in mind the problem of the early thermaliztion in high-energy heavy-ion collisions (see the review [8] and recent studies[9–25]) at the Relativistic Heavy-Ion Collider in the Brookhaven National Laboratory [26–29] and the Large Hadron Collider at CERN [30]. Chaotic classical systems are characterized by the sensitive dependence of the trajectory on the initial condition, and trajectories starting from adjacent initial conditions with the difference δXð0Þ in the phase space deviate exponentially jδXðtÞj ¼ expðλtÞjδXð0Þj from each other; the exponent λ is called a Lyapunov exponent. Then, one can readily imagine that the chaotic behavior makes the phase-space distribution fðq; pÞ so complicated that it generates a finite amount of entropy via a coarse graining in the classical Hamiltonian system. In this respect, it is interesting that the sum of positive Lyapunov exponents coincides with the Kolomogorov-Sinai entropy (see references in Ref. [31]) or the production rate of entropy [32]. Indeed, these have been demonstrated for a discrete classical system [33], where an explicit calculation of the Boltzmann-like entropy SB ¼ −Trf log f was made with the distribution function fðq; pÞ as obtained by a coarse graining of the phase space of the discrete system. A natural extension of the above interesting work to a quantum systems might be done with the application of the quantum mechanical distribution function, i.e., the Wigner function fWðq; pÞ derived as a Weyl transform of the density matrix ρ [34]. However, since fW is a mere Weyl transform of ρ, it cannot describe an entropy production ofa pure quantum system, even apart from the fact that fW is not positive definite. To circumvent this well-known difficulty, let us recall that one cannot distinguish two phase-space points in a unit cell in quantum mechanics, and smearing in the phasespace volume of ð2πℏÞD may be allowed, where D is the degrees of freedom (DOF) of the system. For such a smeared distribution function, we adopt the Husimi function fH [35], which is obtained by a Gaussian smearing of the Wigner function and is semipositive definite. Then, we can define the entropy in terms of fH as SHW ¼ −TrfH log fH, where Tr means the integral over the phase space. This entropy was first introduced and called the classical entropy by Wehrl [36], and we call it Husimi-Wehrl (HW) entropy [37,38]. It should be noted that the HW entropy is a kind of microscopic entropy, and does not necessarily increase in time. Indeed, it may show an oscillatory behavior in time depending on the initial configuration as shown for quantum mechanical systems [38]; we will find that it is also the case for the Yang-Mills field. Nevertheless, it is noteworthy that the HW entropy in the thermal equilibrium tends to coincide with the von Neumann entropy at high temperatures [37]. In the previous work [38], the present authors examine thermalization of isolated quantum systems by using the HW entropy evaluated in the semiclassical approximation. It was shown that the semiclassical treatment works well in describing the entropy-production process of a couple of quantum mechanical systems of which the classical countersystems are known to be chaotic. Two novel methods were also proposed to evaluate the time evolution of the HW entropy, the test-particle method and the two-step Monte Carlo method, and it was demonstrated that the simultaneous application of the two methods ensures the reliability of the results of the HW entropy at a given time. In this article, we extend the previous work [38] to the Yang-Mills (YM) field, which is known to be chaotic and has a macroscopic number of positive Lyapunov exponents [18]. We investigate the possible entropy production by constructing the Husimi function and calculating the HW entropy of the YM field in the semiclassical approximation. The initial condition we adopt for the equation of motion (EOM) of the YM field is motivated by the early stage of relativistic heavy-ion collisions [7,39]. There is, however, a caveat against this simple prescription that works for quantum mechanical systems with a few degrees of freedom because of the large number of the degrees of freedom in the field theory. Thus, we also take a simple ansatz for the Husimi function, where we construct it by a product of the Husimi function for each degree of freedom, although the classical EOM itself is solved numerically with the fully included nonlinear couplings. When applied to a quantum mechanical system with 2 degrees of freedom, the ansatz gives a 10–20% overestimate of the HW entropy. We also develop a novel efficient numerical method for calculating the HW entropy, which is a modification of the test-particle method. We calculate the HW entropy in YM field theory in a semiclassical way for the first time and show that the entropy production rate agrees with the sum of positive Lyapunov exponents or Kolmogorov-Sinai (KS) entropy. II. HUSIMI-WEHRL ENTROPY ON THE LATTICE We consider the SUðNcÞ YM field on a L3 lattice. In the temporal gauge, the Hamiltonian in noncompact formalism is given bywith Fa ij¼∂iAa j ðxÞ−∂jAa i ðxÞþPb;cfabcAb i ðxÞAc j ðxÞ. ND ¼ 3L3ðN2 c−1Þ is the total DOF. We take the dimensionless gauge field A and conjugate momentum E normalized by the lattice spacing a throughout this article. The coupling constant g is also included in the definitions of A and E. The Husimi-Wehrl entropy of the YM field is obtained as a natural extension of that in quantum mechanics by regarding ðAðxÞ; EðxÞÞ as canonical variables. First, we define the Wigner function (referred to as the Wigner functional [40]) in terms of AðxÞ and EðxÞ,III. NUMERICAL METHODS We calculate the time evolution of the HW entropy by two methods: the test particle (TP) method and the parallel test particle (pTP) method. The TP method is developed in Ref. [38]. The pTP method, an alternative method for the two-step Monte Carlo method, requires less numerical cost and gives almost the same results as the two-step Monte Carlo method. We have demonstrated that the HW entropy in some quantum mechanical systems is successfully obtained in these two methods, which are reviewed in the following. In the test particle method (TP) for the Yang-Mills field theory, the “test particles” represent the gauge field amplitude A and its canonical conjugate E, and in terms Note here that the integral over ðA; EÞ has a support only around the positions of the test particles ðAiðtÞ; EiðtÞÞ due to the Gaussian function for each i, and we can effectively perform the Monte Carlo integral. We generate random numbers ðAi;k; Ei;kÞðk ¼ 1; …; NMCÞ with zero mean and standard deviations of ð ffiffiffiffiffiffiffiffiffiffiffi ℏ=2Δ p ; ffiffiffiffiffiffiffiffiffiffiffi ℏΔ=2 p Þ, with NMC being the total number of Monte Carlo samples. Then, we obtain the HW entropy as shown in the second line of Eq. (9). IV. NUMERICAL PROCEDURE Our numerical procedure is summarized as follows: (1) According to the initial state density matrix, an ensemble of field configurations referred to as test particles is generated. (2) Subsequently, the classical equations of motions are solved for each test particle.where NTP is the total number of the test particles. The initial conditions of the test particles are ðAið0Þ; Eið0ÞÞ ði ¼ 1; 2; …; NTPÞ, which are chosen so as to sample well fW½A; E; 0. The time evolution of the coordinates ðAiðtÞ; EiðtÞÞ is determined so that it reproduces the EOM for fW½A; E; t, which is reduced to the canonical EOM (4) in the semiclassical approximation. With the test particle representation of theWigner function, Eq. (7), the Husimi function is readily expressed asIt is noteworthy that the Husimi function here is a smooth function in contrast to the corresponding Wigner function in Eq. (7). Substituting the Wigner function (8) into Eq. (6), the HW entropy in the test particle method is finally given as(3) The Husimi function can be estimated according to Eq. (8), and the HW entropy takes the form as given in the first line of Eq. (9). (4) We perform the functional integral by using a Monte Carlo method as shown in the second line of Eq. (9), where ðAi;m; Ei;mÞ are Gaussian random numbers. V. TEST IN QUANTUM MECHANICAL SYSTEM In the parallel test particle method, we prepare two independent sets of test particles in and out of the logarithm in Eq. (9), while they are the same samples in TP. Figure 1 shows the numerical results of the HW entropy in the twodimensional quantum mechanical system, the Hamiltonian of which is given byThis system is called a modified quantum Yang-Mills (mqYM) model in Ref. [38]. We set the initial condition of the Wigner function as a Gaussian corresponding to the coherent state; fWðp; q; t ¼ 0Þ ¼ 4 expð−ωððp1 − p1;0Þ2þ ðp2 − p2;0Þ2Þ=ℏ − ωðq2 1 þ q2 2Þ=ℏÞ. In this calculation, we set ℏ ¼ ω ¼ Δ ¼ 1 and p1;0 ¼ p2;0 ¼ 10. With increasing test particle number, the HW entropy is found to converge from below (above) in the TP (pTP) method, and then it is possible to give upper and lower limits of the entropy and to guess the converged value by comparing the results in the two methods. VI. PRODUCT ANSATZ AND EXAMPLE IN A TWO-DIMENSIONAL QUANTUM MECHANICAL SYSTEM While the extension to the field theory on the lattice is straightforward, the DOF are so large and numerical-cost demanding in quantum field theories that we need to invoke some approximation scheme in practical calculations. We adopt the product ansatz here to avoid this difficulty. In the ansatz, we construct the Husimi function as a product of that for 1 degree of freedom,where fðiÞ H ¼ R Q j≠idAjdEj=2πℏfH½A; E; t. By substituting this ansatz into Eq. (6), we obtain the HW entropy as a sum of the HW entropy for 1 degree of freedomTo check the variety, we apply it to the mqYM model previously discussed. Figure 2 shows the numerical results of the HW entropy with the product ansatz (SðPAÞ HW ) as well as the full entropy (SHW), which can be found in Fig. 1. While SðPAÞ HW slightly overestimates SHW, the difference is small enough to confirm entropy production. The HW entropy with the product ansatz is found to agree with that without the ansatz within 10–20% error in a few-dimensional quantum mechanical system. We also find that numerical results with the ansatz converge with smaller Monte Carlo samples; then, it is much more efficient from the viewpoint of numerical-cost reduction. VII. ENTROPY PRODUCTION IN YANG-MILLS FIELD THEORY We apply the above-mentioned framework to the SU(2) Yang-Mills field theory. The initial condition of the Winger function is set to be a Gaussian distribution,fW½A; E; t ¼ 0 ¼ 2ND expð−ωA2=ℏ − E2=ωℏÞ, which corresponds to a coherent state. The Wigner-function evolution is obtained by solving the classical EOM, and the HW entropy with the product ansatz is calculated by using the TP and pTP methods. We take the parameter set, ℏ ¼ Δ=ω ¼ 1. Calculating the HW entropy needs NMCN2 TP times summation as found in Eq. (9) and is numerically demanding, so we show here the results on the relatively small lattices, 43; 63, and 83. In Fig. 3, we show the time evolution of the HusimiWehrl entropy per DOF with the product ansatz in the TP and pTP methods. We find that the HW entropy per DOF is independent of the lattice size, and the extensive nature of entropy is confirmed. The dependence on the number of test particle number is the same as that in quantum mechanics; with increasing Monte Carlo samples, SðPAÞ HW converges from below and above in the TP and pTP methods, respectively. The results in the TP and pTP methods approach each other with increasing NTP, and we can guess that the converged value lies between these curves. The oscillatory behavior of the HW entropy around ωt ¼ 0.5 is caused by a collective motion of the phasespace distribution starting from the initial distribution that is different from the equilibrium one. After the oscillation around ωt ¼ 0.5, the HW entropy increases in a monotonic way, and its growth rate decreases. This means that the collective motion in the earliest stage damps, and at later times, the entropy production rate becomes smaller, which might suggest that the system approaches a quasistationary nonequilibrium state [42]. Although this is an interesting possibility, an exploration of the possibility is beyond the scope of the present paper. The straight lines in Fig. 3 show the KS entropy rate, which is given in Ref. [18]. The upper and lower lines show the sum of positive local and intermediate Lyapunov exponents (LLE and ILE), respectively. The LLE are obtained as the eigenvalues of the second derivative matrix of the Hamiltonian, and the ILE show the exponential growth rate in some time duration. Since the classical YM fields are conformal, the KS entropy rate should be proportional to ε1=4 where ε is the energy per site. The coefficients are evaluated in Ref. [18] as RLLE KS =L3 ≃ 3 × ε1=4 and RILE KS =L3 ≃ 2 × ε1=4 for the sum of positive LLE and ILE (local and intermediate KS entropy rates), respectively. These findings show that the local KS entropy rate characterizes the growth rate of the HW entropy in the early time and the intermediate KS entropy rate agrees with the average entropy growth after the initial stage. Let us mention here the gauge dependence of the HW entropy. The present result shows the HW entropy rate is consistent with the KS entropy rate, which is shown to be gauge invariant [21]. We also note that the gauge degrees of freedom do not contribute to the chaoticity and instability [21,23]. For these reasons, we expect that the gauge dependence of the HW entropy production is not serious. VIII. CONCLUSION In summary, we have developed a novel numerical formulation and have calculated the time evolution of the Husimi-Wehrl entropy in Yang-Mills field theory in the semiclassical approximation for the first time. We have shown that the HW entropy is produced and the growth rates roughly agree with Lyapunov exponents. It should be noted that the time reversal invariance is kept in the present framework, in the time evolution of the Wigner function as well as in measuring the entropy. The produced entropy mainly comes from the complexity of the phase-space distribution. The entropy growth will contribute to the thermalization process in relativistic heavy-ion collisions. The setup in this article is motivated by the initial stage dynamics in relativistic heavy-ion collisions. For more realistic analysis, we should choose the initial condition like the one given by the McLerran-Venugopalan model [7].	We investigate possible entropy production in Yang-Mills (YM) field theory by using a quantum distribution function called the Husimi function fHðA; E; tÞ for the YM field, which is given by a coarse graining of the Wigner function and non-negative. We calculate the Husimi-Wehrl entropy SHWðtÞ ¼ −TrfH log fH defined as an integral over the phase space, for which two adaptations of the test-particle method are used combined with Monte Carlo method. We utilize the semiclassical approximation to obtain the time evolution of the distribution functions of the YM field, which is known to show chaotic behavior in the classical limit. We also make a simplification of the multidimensional phase-space integrals by making a product ansatz for the Husimi function, which is found to give a 10–20% overestimate of the HusimiWehrl entropy for a quantum system with a few degrees of freedom. We show that the quantum YM theory does exhibit the entropy production and that the entropy production rate agrees with the sum of positive Lyapunov exponents or the Kolmogorov-Sinai entropy, suggesting that the chaoticity of the classical YM field causes the entropy production in the quantum YM theory
14	2016	Fate of the conformal fixed point with twelve massless fermions and SU(3) gauge group	Zoltan Fodor,1 Kieran Holland,2 Julius Kuti,3 Santanu Mondal,4 Daniel Nogradi,4 and Chik Him Wong5	Fate of the conformal fixed point with twelve massless fermions and SU(3) gauge group We report new results on the conformal properties of an important strongly coupled gauge theory, a building block of composite Higgs models beyond the Standard Model. With twelve massless fermions in the fundamental representation of the SU(3) color gauge group, an infrared fixed point (IRFP) of the β-function was recently reported in the theory [A. Cheng, A. Hasenfratz, Y. Liu, G. Petropoulos, and D. Schaich, J. High Energy Phys. 05 (2014) 137] with uncertainty in the location of the critical gauge coupling inside the narrow ½6.0 < g2  < 6.4 interval and widely accepted since as the strongest evidence for a conformal fixed point and scale invariance in the theory with model-building implications. Using the exact same renormalization scheme as the previous study, we show that no fixed point of the β-function exists in the reported interval. Our findings eliminate the only seemingly credible evidence for conformal fixed point and scale invariance in the Nf ¼ 12 model whose infrared properties remain unresolved. The implications of the recently completed 5-loop QCD β-function for arbitrary flavor number are discussed with respect to our work.I. INTRODUCTION AND MOTIVATION Investigations of strongly coupled gauge theories with massless fermions in the fundamental or two-index symmetric (sextet) representation of the SU(3) color gauge group serve considerable theoretical interest with added relevance as important building blocks of composite Higgs theories beyond the Standard Model (BSM). Two complementary aspects of the composite Higgs paradigm are investigated in this large class of theories: (1) a nearconformal and unexpectedly light scalar particle, perhaps dilatonlike with mass at the electroweak scale or (2) a parametrically light pseudo Nambu-Goldstone boson (PNGB) combined with partial compositeness for fermion mass generation to avoid the flavor problem. Both paradigms are based on strongly coupled gauge dynamics to address important aspects of conformal and chiral symmetries and their symmetry breaking patterns in BSM theories. The precise determination of near-conformal or conformal behavior of SU(3) gauge theory with twelve flavors is relevant for both paradigms. (1) Light scalar, perhaps dilatonlike? Near-conformal strong dynamics with spontaneous chiral symmetry breaking (χSB) is focused on its emergent light scalar with 0þþ quantum numbers of the σ-meson, perhaps with dilatonlike properties.With early results reviewed in [1], this paradigm is very different from scaled up quantum chromodynamics (QCD) which was the prototype of old Higgs-less technicolor. Comparing near-conformal models, with details explained in Fig. 1, a light composite scalar of the massless SU(2) flavor doublet in the sextet fermion representation of SU(3) color was reported in [1,2] whereas the Nf ¼ 8 light scalar with fermions in the fundamental representation was discovered in [3] and confirmed recently [4]. The sextet model β-function, with the minimal flavor doublet required for the composite Higgs mechanism, indicates the closest position to the lower edge of the conformal window (CW) among recently investigated SU(3) gauge theories, exhibiting the lightest scalar accordingly. The β-function of the sextet theory with three massless flavors has a weakly coupled conformal fixed point close to the upper end of the CW [5] with apparent crossing into the CW between two and three flavors. In contrast, uncertainties in crossing into the CW with fermions in the fundamental representation appear to extend into the wider Nf ¼ 8–12 flavor range. For example, it is not known if for more than eight flavors the theory gets very close to the CW with a much lighter scalar mass than at Nf ¼ 8. Based on the findings of [6] and a similar zero in the β-function reported earlier [7,8], the Nf ¼ 12 model has been investigated as a composite Higgs model built on a conformal fixed point inside the CW [9]. The importance of the question warrants independent determination. (2) PNGB with partial compositeness? Challenges for the near-conformal light scalar paradigm to generate fermionmasses and Yukawa couplings motivates the alternate PNGB scenario with a massless scalar boson emerging from vacuum misalignment of χSB as reviewed recently [14]. Model studies with a parametrically light Higgs based on Nf ¼ nf þ νf fermion flavors in the fundamental representation of the SU(3) color gauge group could address the hierarchy problem and fermion mass generation with partial compositeness, if Nf is large enough to bring the theory inside the CW before mass deformations of conformal symmetries are turned on [14–16]. For the simple choice nf ¼ 4, the global flavor symmetry SUð4Þ × SUð4Þ is broken to the diagonal SU(4) flavor group and a Higgs-like scalar state is identified in the PNGB set via χSB. The custodial SO(4) symmetry of the Standard Model remains protected [15,16] while a large enough νf is required to bring the theory close to a strongly coupled IRFP with expectations of large baryon anomalous dimensions as the key ingredients of partial compositeness. The Nf ¼ 12 choice with nf ¼ 4 and νf ¼ 8 for this PNGB paradigm is discussed in [9] building on the conformal fixed point of twelve flavors, warranting again independent confirmation. II. LATTICE IMPLEMENTATION OF THE STEP β-FUNCTION The gradient flow based diffusion of the gauge fields of lattice configurations from Hybrid Monte Carlo (HMC) simulations became the method of choice for studying renormalization effects with great accuracy [17–23]. In particular, we adapted the method and introduced the scaledependent renormalized gauge coupling g2ðLÞ where the scale is set by the linear size L of the finite volume [10,24]. This implementation is based on the gauge invariant trace of the non-Abelian quadratic field strength, EðtÞ ¼ −1 2 TrFμνFμνðtÞ, renormalized as a composite operator at gradient flow time t on the gauge configurations and measured from the discretized lattice implementation, as in [20]. Following [10,24], we define the one-parameter family of renormalized nonperturbative gauge couplings for strongly coupled gauge theories built on the SU(N) color group with Nf massless dynamical fermions,for the gauge fields in all four directions. The origin of the third Jacobi elliptic function ϑ in Eq. (2) was explained in [10] including the treatment of zero modes from periodic gauge fields in finite volumes [25–29]. A scale-dependent renormalized gauge coupling g2ðLÞ was introduced earlier to probe the step β-function, defined as ðg2ðsLÞ − g2ðLÞÞ=logðs2Þ for some preset finite scale change s in the linear physical size L of the fourdimensional volume in the continuum limit of lattice discretization [30,31]. The gauge coupling g2ðLÞ for the determination of the step β-function is identified in our case with the definition in Eq. (1) as we drop the preset label c in the notation and tðLÞ is simply replaced by L. The renormalization scheme with the preset choice c ¼ 0.2 and the preset scale factor s ¼ 2 in our work is identical to the one of the previous study [6] including the boundary conditions on gauge fields and fermion fields. In the continuum limit, the monotonic function g2ðLÞ implies in any of the volume-dependent schemes that a selected value of the renormalized gauge coupling sets the physical size L measured in some particular dimensionful physical unit. Fixed physical size L on the lattice is equivalent to holding g2ðLÞ fixed at some selected value as the lattice spacing a is varied and the fixed physical length L is held by the variation of the dimensionless linear scale L=a as the bare lattice coupling is tuned without changing the selected fixed value of the renormalized gauge coupling. The continuum limit at fixed g2ðLÞ is obtained by a2=L2 → 0 extrapolation of the residual cutoff dependence in the step β-function at the target gauge coupling. In the convention we use, asymptotic freedom in the UV regime corresponds to a positive step β-function given by the perturbative loop expansion for small values of the renormalized coupling. In the infinitesimal derivative limit s → 1 the step β-function turns into the conventional one. If the conventional β-function of the theory possesses a fixed point, the step β-function will have a zero at the same critical gauge coupling g2  as well. The scale-dependence of the gauge coupling g2ðLÞ can be determined from repeated application of the step β-function starting at some scale L0 set by the initial gauge coupling g2ðL0Þ we choose. III. BSM MODELS CLOSE TO THE CONFORMAL WINDOW The effect of near-conformal behavior on the light scalar mass is shown in Fig. 1, if the size of the nonperturbative β-function is used at strong coupling as an indicator for the approach to the CW in the fundamental and sextet representations of massless fermions. The mass of the light σ-like 0þþ scalar particle, as a composite Higgs candidate when coupled to the electroweak sector, is displayed in units of the Goldstone decay constant F in the massless fermion limit of χSB as determined from spectroscopy in each model. The striking trend of decreasing scalar mass iswell established as the CW is approached. In BSM applications F ¼ 250 GeV sets the scale in physical units [1]. The sextet model has the smallest nonzero β-function relative to the other theories in the fundamental representation, together with the lightest scalar. The possibility of the Nf ¼ 12 model being even closer to the CW with an even lighter scalar is open, if the model is near-conformal without IRFP. Our goal is an independent determination of the fate of the Nf ¼ 12 IRFP reported earlier [6]. IV. Nf = 12 SIMULATIONS WITH TARGETED RUN SETS The algorithmic details of our new Nf ¼ 12 simulations are similar to [10,11]. Periodic boundary conditions already defined on the gauge fields, the fermion fields are chosen to be antiperiodic in all four directions. We utilize the staggered fermion action with massless fermions and four steps of stout smearing with stout parameter ϱ ¼ 0.12 on the gauge links [17]. The gauge action is the tree-level improved Symanzik action [32,33]. The evolution along a trajectory of the Hybrid Monte Carlo algorithm [34] is implemented with multiple time scales [35] and Omelyan integrator [36]. For integration along the gradient flow we use the tree-level improved Symanzik action based discretization scheme. The observable EðtÞ is discretized as in [20]. The final 28 runs of Table I ranged in length between 5000 and 20,000 time units of molecular dynamics. The statistical analysis of the renormalized gauge coupling of each run followed [37] and used similar software. Autocorrelation times were measured for each run in two independent ways, using estimates from the autocorrelation function of each run, and from jackknifed blocking procedure. Errors on the renormalized couplings were consistent from the two procedures and the one from autocorrelation functions is listed in Table I. Each run went through thermalization and these segments were not included in the analysis. For detection of residual thermalization effects the replica method of [37] was used in the analysis. All 28 runs passed Q value tests when mean values and statistical errors of the replica segments were compared for thermal and other variations. We targeted the step β-function at three preselected values of the renormalized gauge coupling to cover the interval where the IRFP was reported [6]. In Table I results are shown for gauge ensembles from the three target groups A, B, C of the final run sets. The 28 runs were grouped into 14 steps of pairs where the lower L=a value was precisely tuned to the target value of the renormalized gauge coupling. The higher L=a volume at the doubled physical size determined the step β-function at finite lattice spacing. The first group with 4 steps is target A at g2ðLÞ ¼ 5.979ð2Þ with L=a ¼ 16 → 32; 18 → 36; 20 → 40; 24 → 48. Both target B at g2ðLÞ ¼ 6.185ð2Þand target C at g2ðLÞ ¼ 6.393ð2Þ have an added fifth step of L=a ¼ 28 → 56 for more robust continuum extrapolation. Precise tuning for g2 0 of the 14 steps of the three targets eliminated the largest systematic uncertainty in the step β-function from model-dependent interpolation in the bare gauge coupling. Figure 2 shows the remarkable accuracy of tuning for the three targets at better than per mille accuracy level, like for the entries of Table I. V. CONTINUUM EXTRAPOLATION OF THE STEP β-FUNCTION Cutoff effects have to be removed from the step β-functions at finite lattice spacing. The leading cutoff effects are a2=L2 corrections in each L=a → 2L=a pair for the step β-function at the targeted renormalized couplings. Linear fits to the lattice step functions in a2=L2 allows continuum extrapolation to the a2=L2 → 0 limit, as shown in Fig. 3. For all three targets linear four-point fits of the step functions were used with consistently good χ2 results. The final results of our continuum step β-function are shown in Fig. 4 with overwhelming statistical evidence against the IRFP of [6] in the targeted interval. Leaving open the existence of the IRFP in [6], a new study of the β-function appeared recently in a different renormalization scheme of the model and without our targeted goal [38]. VI. NEW DEVELOPMENTS AND CONCLUSIONS Originally the zero of the β-function for twelve flavors was reported at a somewhat lower value of g2 using the Schrödinger functional (SF) based scheme in agreement with its 3-loop step β-function [8], as shown in Fig. 4 (cyan color). In comparison, the dashed red line is the 3-loopprediction of the MS scheme within the simulation error of the IRFP. The 4-loop MS result only slightly shifts the prediction and is closer to [6]. Although in two different schemes, tantalizing agreement of the simulations and the loop expansion lead to the widely held view that twelve massless fermion flavors in QCD bring the theory inside the CW. In a significant new development, the first MS calculation of the 5-loop β-function was completed for arbitrary flavor number in QCD [39]. Based on the new 5-loop results, it was immediately recognized that the zero in the β-function turns complex and the IRFP disappears for twelve flavors [40], consistent with the plot in Fig. 4. It was also shown that two fixed points appear in the β-function for thirteen flavors like in the intriguing scenario of [41], with shifting estimates for the lower edge of the CW and for the flavor dependence of the mass anomalous dimension [40]. Five loop MS predicts two real zeros at g2 ¼ 5.11 and g2 ¼ 6.52 for thirteen flavors, as shown in Fig. 4. It did not escape our attention that new lattice studies of the running coupling with thirteen flavors would be within easy reach of the 5-loop MS predictions. Credible proof of conformal behavior based on the β-function requires two necessary steps in strongly coupled gauge theories. First, the critical gauge coupling g2  has to be determined where the scheme-dependent β-function vanishes and signals the location of the conformal IRFP. The slope of the β-function at the fixed point is a schemeindependent scaling exponent ω which controls the leading conformal scaling corrections to fermion mass deformations close to the IRFP [1,42–44]. The choice in scheme dependence can move the position of the conformal IRFP but cannot destroy its existence, or change the universal scaling exponent ω. These are very demanding criteria, unmatched in lattice simulations while reporting zeros in the β-function.	We report new results on the conformal properties of an important strongly coupled gauge theory, a building block of composite Higgs models beyond the Standard Model. With twelve massless fermions in the fundamental representation of the SU(3) color gauge group, an infrared fixed point (IRFP) of the β-function was recently reported in the theory [A. Cheng, A. Hasenfratz, Y. Liu, G. Petropoulos, and D. Schaich, J. High Energy Phys. 05 (2014) 137] with uncertainty in the location of the critical gauge coupling inside the narrow ½6.0 < g2  < 6.4 interval and widely accepted since as the strongest evidence for a conformal fixed point and scale invariance in the theory with model-building implications. Using the exact same renormalization scheme as the previous study, we show that no fixed point of the β-function exists in the reported interval. Our findings eliminate the only seemingly credible evidence for conformal fixed point and scale invariance in the Nf ¼ 12 model whose infrared properties remain unresolved. The implications of the recently completed 5-loop QCD β-function for arbitrary flavor number are discussed with respect to our work.
15	2016	Search for a dark vector gauge boson decaying to πþπ− using η → πþπ−γ decays	We report a search for a dark vector gauge boson U0 that couples to quarks in the decay chain Dþ → D0πþ; D0 → K0 Sη; η → U0 γ, U0 → πþπ−. No signal is found and we set a mass-dependent limit on the baryonic fine structure constant of 10−3 − 10−2 in the U0 mass range of 290 to 520 MeV=c2. This analysis is based on a data sample of 976 fb−1 collected by the Belle experiment at the KEKB asymmetricenergy eþe− collider.	Search for a dark vector gauge boson decaying to πþπ− using η → πþπ−γ decays We report a search for a dark vector gauge boson U0 that couples to quarks in the decay chain Dþ → D0πþ; D0 → K0 Sη; η → U0 γ, U0 → πþπ−. No signal is found and we set a mass-dependent limit on the baryonic fine structure constant of 10−3 − 10−2 in the U0 mass range of 290 to 520 MeV=c2. This analysis is based on a data sample of 976 fb−1 collected by the Belle experiment at the KEKB asymmetricenergy eþe− collider. The standard model (SM) of particle physics cannot explain the nature of dark matter that is understood to have mostly gravitational effects on visible matter, radiation, and the large-scale structure of the Universe [1–4]. The dark matter can be naturally explained by the introduction of a * Corresponding author. eunil@hep.korea.ac.kr. E. WON et al. PHYSICAL REVIEW D 94, 092006 (2016) 092006-2 weakly interacting particle predicted in the supersymmetric extension of the SM [5]. The absence of observation of any supersymmetric particles in hadron collider experiments [6] motivates studies of new classes of models, commonly referred to as dark models, which introduce new gauge symmetries [7] and predict the existence of new particles that couple weakly to SM particles. Most accelerator-based experiments have focused on the dark photon or dark particles coupling to the SM photons [8], though many dark models suggest a new gauge boson that could couple predominantly to quarks [9,10]. This new dark boson (hereafter referred to as the U0 boson, instead of B as is originally proposed in Ref. [9], to avoid confusion with the SM B meson) can be produced from light SM meson decays through P → U0 γ or V → U0 P, where P refers to a pseudoscalar meson (e.g., π0; η; η0 ) and V to a vector meson (e.g., ω; ϕ). Two recent experimental limits on searches for a dark photon A0 via π0 → A0 γ; A0 → eþe− [11] and ϕ → A0 γ; A0 → eþe− [12] can be applied to the U0 boson search in a model-dependent way to constrain the baryonic fine structure constant αU0 ≡ g2 U0=ð4πÞ, where gU0 is the universal gauge coupling between the U0 boson and the quarks [10]. There are also limits from η → π0γγ and ϕ → ηπ0γ decays based on their total rate, as well as from the analysis of hadronic ϒð1SÞ decays [10]. We search for U0 bosons decaying to πþπ− pairs using η → πþπ−γ decays, where η is produced in the decay chain Dþ → D0πþ, D0 → K0 Sη [13]. The kinematics here allows us to suppress the combinatorial background significantly. The decay U0 → πþπ− is expected to have a relatively small branching fraction of 2%–4% [10] but nevertheless provides a very clean signature for a possible dark vector gauge boson. The dominant decay modes are π0γ at low U0 mass and πþπ−π0 at higher U0 mass; however they suffer from higher combinatorial background and therefore are not used in the analysis. We use the decay η → πþπ−π0 to validate our event reconstruction by measuring the branching fraction of η → πþπ−γ relative to that of η → πþπ−π0. The data used in this analysis were recorded at the ϒðnSÞ resonances (n ¼ 1; …; 5) and 60 MeV below the ϒð4SÞ resonance with the Belle detector [14] at the eþe− asymmetric-energy collider KEKB [15]. The sample corresponds to an integrated luminosity of 976 fb−1. We generated two million Monte Carlo (MC) events [16] each for η → πþπ−γ, η → πþπ−π0, and η → U0 γ → πþπ−γ at a particular U0 mass selected in the range from 280 to 540 MeV=c2 in steps of 10 MeV=c2 (i.e., 58 million events in all). The lifetime of the U0 is assumed to be negligible. The U0 samples are used to determine the Mðπþπ−Þ resolution. The U0 signal shape parameters for intermediate U0 mass values are determined using spline interpolation. Except for tracks from K0 S decays, we require that the charged tracks originate from the vicinity of the interaction point (IP) with impact parameters along the beam direction (z axis) and perpendicular to it of less than 4 and 2 cm, respectively. All such charged tracks are required to have at least two associated hits in the silicon vertex detector (SVD), both in the z and perpendicular directions. Such charged tracks are identified as pions or kaons by requiring that the ratio of particle identification likelihoods, LK=ðLK þ LπÞ, constructed using information from the central drift chamber (CDC), time-of-flight scintillation counters, and aerogel threshold Cherenkov counters, be larger or smaller than 0.6, respectively. For both kaons and pions, the efficiencies and misidentification probabilities are 86% and 14%, respectively. For photon selection, we require the energy of the candidate photon to be greater than 60 MeV (100 MeV) when the candidate photon is reconstructed in the barrel (end cap) calorimeter that covers 32° < θ < 130° (12° < θ < 32° or 130° < θ < 157°) in the polar angle θ with respect to the þz axis. To reject neutral hadrons, the ratio of the energy deposited by a photon candidate in the 3 × 3 and 5 × 5 calorimeter arrays centered on the crystal with the largest signal is required to exceed 0.85. Candidate π0 mesons are reconstructed from pairs of γ candidates; we require Mγγ ∈ ½120; 150 MeV=c2 and refit γ momenta with the π0 mass constraint. Candidate K0 S → πþπ− mesons are reconstructed from two tracks, assumed to be pions, using a neural network technique [17] that uses the following information: the K0 S momentum in the laboratory frame; the distance along z between the two track helices at their closest approach; the K0 S flight length in the transverse plane; the angle between the K0 S momentum and the vector joining the K0 S decay vertex to the IP; the angles between the pion momenta and the laboratory-frame direction in the K0 S rest frame; the distances of closest approach in the transverse plane between the IP and the two pion helices; and the pion hit information in the SVD and CDC. We also require that the πþπ− invariant mass be within 9 MeV=c2 (about 3σ in resolution [18]) of the nominal K0 S mass [19]. For the η → πþπ−γ candidates, we require that the photon not be associated with a π0 candidate and its transverse momentum be greater than 200 MeV=c to remove Dþ → Dþð→ K0 Sπþπ−πþÞγ background. For both η → πþπ−γ and η → πþπ−π0 candidates, we perform a vertex fit with the two charged pions and require the reduced χ2 to be less than 10. The efficiency of this requirement is 94%. We require the reconstructed mass of each η candidate to be in the range ½500; 600 MeV=c2 and refit momenta of its daughters with the constraint of the nominal η mass. Combinations of a K0 S candidate and η candidate are fit to a common vertex and their invariant mass is required to be within 40 MeV=c2 of the nominal D0 mass. The D0 and πþ combinations are fitted to the IP, and the mass difference validateused for the combinatorial background PDF. The feeddown contribution is described by a Gaussian with shape parameters fixed from the MC simulation. The confidence level (p-value) of the fit is 12% and the η → πþπ−γ signal yield is Nη ¼ 2974  90 events. The feed-down yield agrees well with the expectation. As a cross-check, we measure the ratio of branching fractions Bðη → πþπ−γÞ=Bðη → πþπ−π0Þ. The fit to the πþπ−π0 invariant mass distribution is similar to the one described above, except that the combinatorial background is described by a second-order polynomial and there is no feed-down background. The reconstruction efficiencies, determined from the MC simulation, are εðπþπ−γÞ ¼ 5.1% and εðπþπ−π0Þ ¼ 4.8%. The measured ratio of branching fractions, 0.185  0.007, where the uncertainty is statistical only, is in good agreement with the worldaverage value of 0.184  0.004 [19]. We define the η signal region as Mðπþπ−γÞ ∈ ½535.5; 560.5 MeV=c2, and the sideband regions used for background subtraction as Mðπþπ−γÞ∈½520.0;532.5 or ½563.5; 576.0 MeV=c2 . The Mðπþπ−Þ distribution for the background-subtracted η signal is shown in Fig. 3. To describe the Mðπþπ−Þ distribution, we use an expression of the differential decay rate based on lowenergy quantum chromodynamics phenomenology [20,21] using a combination of chiral perturbation theory and dispersive analysis,(s in GeV2=c4). The numerical values and the uncertainties of the expansion coefficients of jPðsÞj and jFVðsÞj are taken from fits to data of ηð0Þ → πþπ−γ decays. We multiply the dΓ=ds expression from Eq. (1) by the reconstruction efficiency. The efficiency as a function of Mðπþπ−Þ is approximately flat but drops to 0 at the kinematic limit of mη. The fit results are presented in Fig. 3. Equation (1) describes the Mðπþπ−Þ distribution well, and the confidence level of the fit is 95%. We add the U0 signal to the above fit function and perform fits while fixing the U0 mass at a value between 290 and 520 MeV=c2 in steps of 1 MeV=c2. The U0 signal is described by the sum of two Gaussians. The signal resolution of the core Gaussian is about 1 MeV=c2 near the 2mπ threshold and 2 MeV=c2 at the mη kinematic limit. An example of the U0 signal with the mass of 400 MeV=c2 and arbitrary normalization is shown in Fig. 3. We do not find a significant U0 signal at any mass value. The typical uncertainty in the U0 yield NU0 is Oð1–10Þ events. We express the baryonic fine structure constant αU0 using the equation for the partial width ratio Γðη → U0 γÞ=Γðη → γγÞ from Ref. [10] aswhere α is the electromagnetic fine structure constant. The first factor in Eq. (2), which is purely theoretical, contains the phase space, the form factor Fðm2 U0Þ, and the branching fraction of U0 → πþπ− decay. The branching fraction is about 2%–4%, as computed from formulas provided in Ref. [10] and references therein. The second factor is obtained from the latest measurements [19]. The third factor is determined from the η and U0 yields and reconstruction efficiencies ðNU0=εðη → U0 γ → πþπ−γÞÞ= ðNη=εðη → πþπ−γÞÞ. To estimate the systematic uncertainties in the η → πþπ−γ and η → U0 γ → πþπ−γ yields, we change the parametrization of the combinatorial background in the Mðπþπ−γÞ fit from a first- to a second-order polynomial and account for the background nonlinearity while subtracting the sidebands. The change in the η yield is at the 1% level, while the change in the U0 yield is negligible. The systematic effect due to the uncertainties of the expansion coefficients in jPðsÞj and jFVðsÞj is negligible in the U0 yield. The systematic uncertainty in the ratio of the reconstruction efficiencies εðη → U0 γ → πþπ−γÞ=εðη → πþπ−γÞ is conservatively estimated to be 4% (1% per track and 3% per photon). The total systematic uncertainties are estimated by adding the above contributions in quadrature. Using Eq. (2), we set a 95% confidence level upper limit on αU0 using the Feldman-Cousins approach [22], adding the statistical and systematic uncertainties in quadrature. The upper limit as a function of the U0 boson mass is shown in Fig. 4. Considering other results in this mass region, we find that our limit is stronger than that from a modeldependent analysis [10] of the ϕ → eþe−γ decays [12] for mU0 > 450 MeV=c2, but weaker than the limit based on the η → π0γγ total rate [10]. Recently, we learned that the data set in Ref. [23] contains many more η → πþπ−γ decays and can provide a more stringent limit on αU0 in future. To conclude, we perform a search for a dark vector gauge boson U0 that couples to quarks [10], using the decay chain Dþ → D0πþ, D0 → K0 Sη, η → U0 γ, U0 → πþπ−. Our results limit the baryonic fine structure constant αU0 to below 10−3–10−2 at 95% confidence level over the U0 mass range 290 to 520 MeV=c2. This is the first search for U0 in the πþπ− mode. We find that our limit is stronger than that from a model-dependent analysis [10] of the ϕ → eþe−γ decays [12] for mU0 > 450 MeV=c2, but weaker than the limit based on the η → π0γγ total rate [10].	We report a search for a dark vector gauge boson U0 that couples to quarks in the decay chain Dþ → D0πþ; D0 → K0 Sη; η → U0 γ, U0 → πþπ−. No signal is found and we set a mass-dependent limit on the baryonic fine structure constant of 10−3 − 10−2 in the U0 mass range of 290 to 520 MeV=c2. This analysis is based on a data sample of 976 fb−1 collected by the Belle experiment at the KEKB asymmetricenergy eþe− collider. 
16	2014	Independent measurement of the neutrino mixing angle θ13 via neutron capture on hydrogen at Daya Bay	F. P. An,1 A. B. Balantekin,2 H. R. Band,2 W. Beriguete,3 M. Bishai,3 S. Blyth,4 I. Butorov,5 G. F. Cao,6 J. Cao,6 Y. L. Chan,7 J. F. Chang,6 L. C. Chang,8 Y. Chang,9 C. Chasman,3 H. Chen,6 Q. Y. Chen,10 S. M. Chen,11 X. Chen,7 X. Chen,6 Y. X. Chen,12 Y. Chen,13 Y. P. Cheng,6 J. J. Cherwinka,2 M. C. Chu,7 J. P. Cummings,14 J. de Arcos,15 Z. Y. Deng,6 Y. Y. Ding,6 M. V. Diwan,3 E. Draeger,15 X. F. Du,6 D. A. Dwyer,16 W. R. Edwards,16 S. R. Ely,17 J. Y. Fu,6 L. Q. Ge,18 R. Gill,3 M. Gonchar,5 G. H. Gong,11 H. Gong,11 W. Q. Gu,19 M. Y. Guan,6 X. H. Guo,20 R. W. Hackenburg,3 G. H. Han,21 S. Hans,3 M. He,6 K. M. Heeger,2,22 Y. K. Heng,6 P. Hinrichs,2 Y. K. Hor,23 Y. B. Hsiung,4 B. Z. Hu,8 L. M. Hu,3 L. J. Hu,20 T. Hu,6 W. Hu,6 E. C. Huang,17 H. Huang,24 X. T. Huang,10 P. Huber,23 G. Hussain,11 Z. Isvan,3 D. E. Jaffe,3 P. Jaffke,23 K. L. Jen,8 S. Jetter,6 X. P. Ji,25 X. L. Ji,6 H. J. Jiang,18 J. B. Jiao,10 R. A. Johnson,26 L. Kang,27 S. H. Kettell,3 M. Kramer,16,28 K. K. Kwan,7 M. W. Kwok,7 T. Kwok,29 W. C. Lai,18 K. Lau,30 L. Lebanowski,11 J. Lee,16 R. T. Lei,27 R. Leitner,31 A. Leung,29 J. K. C. Leung,29 C. A. Lewis,2 D. J. Li,32 F. Li,18,6 G. S. Li,19 Q. J. Li,6 W. D. Li,6 X. N. Li,6 X. Q. Li,25 Y. F. Li,6 Z. B. Li,33 H. Liang,32 C. J. Lin,16 G. L. Lin,8 P. Y. Lin,8 S. K. Lin,30 Y. C. Lin,18 J. J. Ling,3,17 J. M. Link,23 L. Littenberg,3 B. R. Littlejohn,26 D. W. Liu,30 H. Liu,30 J. L. Liu,19 J. C. Liu,6 S. S. Liu,29 Y. B. Liu,6 C. Lu,34 H. Q. Lu,6 K. B. Luk,28,16 Q. M. Ma,6 X. Y. Ma,6 X. B. Ma,12 Y. Q. Ma,6 K. T. McDonald,34 M. C. McFarlane,2 R. D. McKeown,35,21 Y. Meng,23 I. Mitchell,30 J. Monari Kebwaro,36 Y. Nakajima,16 J. Napolitano,37 D. Naumov,5 E. Naumova,5 I. Nemchenok,5 H. Y. Ngai,29 Z. Ning,6 J. P. Ochoa-Ricoux,38,16 A. Olshevski,5 S. Patton,16 V. Pec,31 J. C. Peng,17 L. E. Piilonen,23 L. Pinsky,30 C. S. J. Pun,29 F. Z. Qi,6 M. Qi,39 X. Qian,3 N. Raper,40 B. Ren,27 J. Ren,24 R. Rosero,3 B. Roskovec,31 X. C. Ruan,24 B. B. Shao,11 H. Steiner,28,16 G. X. Sun,6 J. L. Sun,41 Y. H. Tam,7 X. Tang,6 H. Themann,3 K. V. Tsang,16 R. H. M. Tsang,35 C. E. Tull,16 Y. C. Tung,4 B. Viren,3 V. Vorobel,31 C. H. Wang,9 L. S. Wang,6 L. Y. Wang,6 M. Wang,10 N. Y. Wang,20 R. G. Wang,6 W. Wang,21,33 W. W. Wang,39 X. Wang,42 Y. F. Wang,6 Z. Wang,11 Z. Wang,6 Z. M. Wang,6 D. M. Webber,2 H. Y. Wei,11 Y. D. Wei,27 L. J. Wen,6 K. Whisnant,43 C. G. White,15 L. Whitehead,30 T. Wise,2 H. L. H. Wong,28,16 S. C. F. Wong,7 E. Worcester,3 Q. Wu,10 D. M. Xia,6 J. K. Xia,6 X. Xia,10 Z. Z. Xing,6 J. Y. Xu,7 J. L. Xu,6 J. Xu,20 Y. Xu,25 T. Xue,11 J. Yan,36 C. C. Yang,6 L. Yang,27 M. S. Yang,6 M. T. Yang,10 M. Ye,6 M. Yeh,3 Y. S. Yeh,8 B. L. Young,43 G. Y. Yu,39 J. Y. Yu,11 Z. Y. Yu,6 S. L. Zang,39 B. Zeng,18 L. Zhan,6 C. Zhang,3 F. H. Zhang,6 J. W. Zhang,6 Q. M. Zhang,36 Q. Zhang,18 S. H. Zhang,6 Y. C. Zhang,32 Y. M. Zhang,11 Y. H. Zhang,6 Y. X. Zhang,41 Z. J. Zhang,27 Z. Y. Zhang,6 Z. P. Zhang,32 J. Zhao,6 Q. W. Zhao,6 Y. Zhao,12,21 Y. B. Zhao,6 L. Zheng,32 W. L. Zhong,6 L. Zhou,6 Z. Y. Zhou,24 H. L. Zhuang,6 and J. H. Zou6 (Daya Bay Collaboration)	Independent measurement of the neutrino mixing angle θ13 via neutron capture on hydrogen at Daya Bay A new measurement of the θ13 mixing angle has been obtained at the Daya Bay Reactor Neutrino Experiment via the detection of inverse beta decays tagged by neutron capture on hydrogen. The antineutrino events for hydrogen capture are distinct from those for gadolinium capture with largely different systematic uncertainties, allowing a determination independent of the gadolinium-capture result and an improvement on the precision of the θ13 measurement. With a 217-day antineutrino data set obtained with six antineutrino detectors and from six 2.9 GWth reactors, the rate deficit observed at the far hall is interpreted as sin22θ13 ¼ 0.083  0.018 in the three-flavor oscillation model. When combined with the gadolinium-capture result from Daya Bay, we obtain sin22θ13 ¼ 0.089  0.008 as the final result for the six-antineutrino-detector configuration of the Daya Bay experiment. Neutrino oscillations are described by the three angles ðθ13; θ23; θ12Þ and phase (δ) of the Pontecorvo-MakiNakagawa-Sakata matrix [1,2]. Recent results [3–7] have established that θ13 is nonzero, as had been indicated by accelerator and reactor neutrino experiments [8–14]. Accurate and precise knowledge of θ13 is essential to forthcoming experiments to determine the neutrino mass hierarchy and to search for CP violation in the lepton sector [15]. Definite θ13 results were obtained by measuring the changes of reactor antineutrino rates and spectra at multiple sites via the inverse-beta decay (IBD) reaction, ν¯e þ p → eþ þ n, in which the prompt eþ signal is tagged by the delayed ∼8 MeV γ-cascade signal from neutron capture on gadolinium (nGd) [3–6]. In this paper, with comparable statistics as the nGd case, a new measurement obtained by tagging the delayed 2.2 MeV γ from neutron capture on hydrogen (nH) [14,16,17] at Daya Bay is presented. New analysis approaches have been developed to meet the challenges associated with the higher background, longer neutron capture time (∼200 μs), and a lower energy γ ray from neutron capture for nH IBD events. This nH analysis provides an independent measurement of sin22θ13, and leads to an improved precision on the θ13 mixing angle when combined with the nGd result obtained from the same period of the six-antineutrino-detector (AD) configuration [6]. The inclusion of nH capture results will improve the ultimate precision of Daya Bay for both θ13 and the ν¯e mass-squared difference jΔm2 eej [6]. Optimization of the nH analysis method will be applicable to future reactor neutrino experiments that address the reactor antineutrino anomaly [18–21] and determine the neutrino mass hierarchy [22–25]. A detailed description of the Daya Bay experiment can be found in Refs. [26,27]. The ongoing experiment consists of two near experimental halls, EH1 and EH2, and one far hall, EH3. The power-weighted baselines to the six commercial power reactors are ∼500 m and ∼1.6 km for the near and far halls, respectively. In this analysis, EH1, EH2, and EH3 have two, one, and three ADs, respectively. All ADs are submerged in water pools consisting of optically separated inner (IWS) and outer water shields (OWS), which also function as Cherenkov detectors to tag cosmic-ray muons. All ADs utilize an identical three-zone design with 20 tons of Gd-loaded liquid scintillator (GdLS) in the innermost zone, 22 tons of liquid scintillator (LS) in the middle zone to detect γ’s escaping from GdLS, and 40 tons of mineral oil in the outermost zone where photomultiplier tubes (PMTs) are installed. Unlike the nGd events, nH capture can occur in both the LS and GdLS regions, resulting in more nH than nGd events before event selection. The trigger threshold for each AD was set at ∼0.4 MeV based on the logical OR of the number of overthreshold PMTs and the analog sum of their signals [28]. The vertex and energy were reconstructed utilizing the charge topological information collected by the PMTs. For a 2.2 MeV γ, the vertex resolutions were ∼8 cm in the x-y plane and ∼13 cm in the z direction in a Cartesian coordinate system with the origin at the AD center and the þz axis pointing upwards. Detector simulation was based on GEANT4 [29] with the relevant physical processes validated [26]. All data from December 24, 2011 to July 28, 2012 were used for this analysis. The live time of each AD is listed in Table I. All triggered events at each site were sequenced according to their time stamps after removing an instrumental background resulting from spontaneous light emission of PMTs [3,5]. Because of the latency between detectors, events with time separations less than 2 μs in the same hall were grouped together for identifying cosmic-ray muons. A water-pool muon was defined as an event with the number of over-threshold PMTs > 12 in the IWS or > 15 in the OWS, while an AD (shower) muon had a visible energy greater than 20 MeV (2.5 GeV) in an AD. Table I lists the total muon rate per AD, Rμ, which was stable over the entire data-taking period. Due to the long lifetimes of muon spallation products, the AD events were required to occur at least 400 μs, 800 μs, or 1 s after a water-pool, AD, or shower muon, respectively. The visible energy for each AD event was also required to be greater than 1.5 MeV to reject the low-energy background. The surviving AD events were denoted as “good” events for further study. Coincident events were identified within a 399 μs time window, Tc, beginning at 1 μs after each prompt signal candidate [30]. This procedure classified all good events into singlecoincidence, double-coincidence (DC), and multicoincidence categories. Events in the latter category account for ∼2% of the total and were not included for further analysis. Since the DC events were dominantly accidentally coincident background, especially in the far hall, a maximum distance of 50 cm between the prompt and delayed vertices was required, rejecting 98% of this background at the cost of a 25% acceptance loss. This cut was one of the major differences between the nH and the nGd analyses. Figure 1(a) shows the prompt energy vs the delayed energy for all the DC events after this cut in the far hall. The IBD bands are clearly seen for both the 2.2 MeV nH and the 8 MeV nGd cases. The measured nH peak was around 2.33 MeV with a resolution of 0.14 MeV. The offset from the true peak value arose from the nonlinear and nonuniform energy response, which was pegged to the nGd capture peak in the reconstruction. The γ’s from 40K and 208Tl decays are observed around 1.5 and 2.6 MeV, respectively, and the continuous bands from 1.5 to 3 MeV are from the decay products of 238U and 232Th. The nH IBD candidates were obtained by requiring the prompt energy to be less than 12 MeV and the delayed energy to be within 3σ of the measured nH peak in each AD. The numbers of the candidates are listed in Table I. The four identified backgrounds in the selected sample are accidental coincidences, cosmogenically produced fast neutrons and 9Li=8He, and neutrons from the retracted 241Am-13C calibration source. The delayed signals of the latter three are all from correlated neutron captures. The following procedure was adopted for removing the accidental coincidence background. An accidental background sample (ABS) consisting of NABS-tot events was first generated by pairing two single events separated by at least 10 hours. The same distance and energy cuts were then applied to the ABS events, resulting in NABS-cut events. As shown in Fig. 1(b), the ABS describes well the pattern of the low-energy region in Fig. 1(a). The spectra of correlated events dominated by IBD, NIBDðξÞ, were then obtained by subtracting the accidental background from the DC events, NDC:an AD was installed in water and became stable after about 4 months. The slow variation of Rs was taken into account by performing the accidental subtraction [Eq. (1)] on a run-by-run basis, with each run lasting about 2 days. Figure 1(c) shows the delayed energy spectra for the DC events in the near and far halls after subtracting the accidental background. Very similar spectra, clearly showing the nH and nGd peaks, were observed for all ADs. The procedure of accidental background subtraction was validated by checking the distribution of distance between the prompt and delayed vertices, as shown in Fig. 2. Simulation studies indicated IBD events rarely occurred with the prompt and delay vertices separated beyond 200 cm. Figure 2 shows a flat distribution consistent with zero for the region beyond 200 cm. The distribution of the difference of the delayed and prompt times after all other cuts is shown in Fig. 3 to further validate the accidental subtraction and justify the 399 μs Tc cut. The accidentalbackground-subtracted spectra are consistent with no events of coincidence time longer than 1.5 ms. The procedures for evaluating the 9Li=8He, fast neutron, and 241Am − 13C backgrounds follow those in Ref. [3], except for three different selection cuts: the delayed energy cut, the distance cut, and an additional cut, E > 3.5 MeV, on the prompt energy to suppress the accidental background. The fast-neutron background is significantly higher than in the nGd case because the LS region is more accessible to the externally produced fast neutrons. The other two backgrounds are also slightly different due to detector geometry configuration. All background rates are listed in Table I. The number of predicted IBD events, N, summed over various detector volumes v (GdLS, LS, and acrylic vessels) is given aswhere ϕ is the antineutrino flux, which was modeled as in Ref. [6], and Np, σ, and f are the number of protons, the IBD cross section, and the hydrogen capture fraction, respectively. The efficiency εμ is the efficiency of the muon veto and εm is the efficiency of the multiplicity cut for the DC selection [30]. The efficiency εep (εed) is the prompt (delayed) energy cut efficiency, and εt (εd) refers to the efficiency of the time (distance) cut. The θ13 analysis is based on relative rates, as in Refs. [3,5], such that uncertainties that are correlated among ADs largely cancel and the uncorrelated uncertainties give the dominant contributions. The central values of εep and εed were evaluated from the simulation. The prompt energy cut at 1.5 MeV caused about 5% inefficiency in εep for GdLS and LS events and a much higher loss in the acrylic. The slight variations in energy scale and resolution among different ADs introduced an uncorrelated uncertainty of 0.1%. For εed, the 3σ energy cut around the nH capture peak made the efficiency largely insensitive to the small variations of energy calibration and resolution. The efficiency εed also included a small contribution from the low-energy tail of nGd capture events. The uncertainty in εed was determined by using a spallation neutron sample. Since the spallation neutron fluxes for neighboring ADs were nearly identical and the relative nGd acceptance in the GdLS region was accurately measured [3,5], a comparison of the spallation neutron rates between nH and nGd captures gave an uncertainty of 0.5%. Simulations of IBD events in different ADs with as-built dimensions were also consistent with this uncertainty estimate. The central value of εt was also evaluated with the simulation. The sources of the uncorrelated uncertainty include the number densities of various isotopes in LS and GdLS, the neutron elastic and capture cross sections, and the precision of time measurements. A chemical analysis showed that the density difference among the ADs is less than 0.1% and that the weight fractions of carbon and hydrogen among the ADs differed by less than 0.3%, limited by the instrumental precision. The uncertainty in number densities introduced a 0.1% uncorrelated uncertainty in εt. The precision of the timing measurement was studied using β-α coincident events from the decay chain of 214Bi-214Po-210Pb originating from the 238U cascade decays. With the same procedure of accidental subtraction applied, a comparison of the measured lifetime of 214Po with the known value (237 μs) verified that the uncertainty on the timing precision due to the electronics was at the level of 0.1%. In total, the uncorrelated uncertainty was taken as 0.14%. A study of a clean nH IBD sample with the prompt energy > 3.5 MeV for the ADs in the two near halls also confirmed this conclusion. The central value of εd was directly measured from the distribution of the distance between the prompt and delayed vertices (see Fig. 2). The uncorrelated uncertainty, caused by the slight variations in the vertex reconstruction bias and resolution, was estimated to be 0.4%. The value and uncertainty of Np in GdLS were discussed in Ref. [26]. The proton number Np in the LS region was determined in the same way and its uncorrelated uncertainty of 0.13% was dominated by the uncertainty of the Coriolis-mass-flow meter. The H-capture fraction, f, was less than unity due to neutron capture on Gd and C, and was estimated by the simulation to be 96% in the LS region and 16% in the GdLS region. The relative difference among ADs is negligible [5]. The selected nH IBD sample was about 65% of the size of the nGd IBD sample [6]. The total uncorrelated uncertainty per AD was 0.67%, as summarized in Table II. The nH/nGd ratios among ADs 1, 2, and 3 agreed within 0.6%, as shown in Table I, which provided a strong confirmation of the uncorrelated uncertainty per AD. Figure 4 shows a comparison of the prompt spectra of the far hall and the near halls weighted by the near-to-far baseline ratio, along with the ratio of the measured-topredicted rates as a function of baseline. Clear evidence for electron antineutrino disappearance is observed. A χ2 with pull terms for nuisance parameters as in Refs. [3,5] is minimized to extract sin22θ13 from the detected nH IBD rate deficit. The value of jΔm2 31j is taken from MINOS [31]. The best fit is sin22θ13 ¼ 0.083  0.018 with χ2 ¼ 4.5 for four degrees of freedom. The increase in χ2 is 20 when θ13 is set to zero, ruling out this null assumption at 4.6 standard deviations. The expected far/ near ratio based on the best-fit sin22θ13 value is compared to data in Fig. 4. The nH result is an independent measurement of θ13 and provides a strong confirmation of the earlier measurement using nGd [6]. Currently both the nH and nGd [6] uncertainties are statistics dominated. With only statistical uncertainties considered in the nH fit, the uncertainty of sin22θ13 is 0.015, about 70% of the total uncertainty when uncertainties are added in quadrature, which is the same for the nGd analysis. The dominant systematic uncertainties are also independent of the nGd analysis. For example, the delayed-energy cut is uncoupled (uncorrelated) because the impact of the relative energy-scale difference on the fixed-energy threshold in the nGd analysis [3,5,6] is avoided with the data-driven 3σ cut. Further couplings are noted in Table II. With all uncoupled uncertainties included in the nH fit, the uncertainty of sin22θ13 is 0.017 (90% of the total uncertainty in quadrature). By conservatively taking all coupled quantities to be fully coupled, the correlation coefficient is about 0.05, indicating an essentially independent measurement of θ13. The weighted average of nH and nGd [6] results is 0.089  0.008, improving the nGd result precision by about 8%. In summary, with an nH sample obtained in the six-AD configuration, by comparing the rates of the reactor antineutrinos at the far and near halls at Daya Bay, we report an independent measurement of sin22θ13 which is in good agreement with the one extracted from the minimally correlated nGd sample. By combining the results of the nH and nGd samples, the precision of sin22θ13 is improved. In general, with different systematic issues, results derived from nH samples will be important when the nGd systematic uncertainty becomes dominant in the future. It is also expected that nH analysis	A new measurement of the θ13 mixing angle has been obtained at the Daya Bay Reactor Neutrino Experiment via the detection of inverse beta decays tagged by neutron capture on hydrogen. The antineutrino events for hydrogen capture are distinct from those for gadolinium capture with largely different systematic uncertainties, allowing a determination independent of the gadolinium-capture result and an improvement on the precision of the θ13 measurement. With a 217-day antineutrino data set obtained with six antineutrino detectors and from six 2.9 GWth reactors, the rate deficit observed at the far hall is interpreted as sin22θ13 ¼ 0.083  0.018 in the three-flavor oscillation model. When combined with the gadolinium-capture result from Daya Bay, we obtain sin22θ13 ¼ 0.089  0.008 as the final result for the six-antineutrino-detector configuration of the Daya Bay experiment.
17	2014	Status of Higgs couplings after run 1 of the LHC		Status of Higgs couplings after run 1 of the LHC  We provide an update of the global fits of the couplings of the 125.5 GeV Higgs boson using all publicly available experimental results from run 1 of the LHC as per summer 2014. The fits are done by means of the new public code LILITH 1.0. We present a selection of results given in terms of signal strengths, reduced couplings, and for the two-Higgs-doublet models of type I and II.I. INTRODUCTION The properties of the observed Higgs boson with mass around 125 GeV [1,2] have been measured with unforeseeable precision already during run 1 of the LHC at 7–8 TeV center-of-mass energy [3,4]. This is a consequence of the excellent operation of the LHC and of the wealth of accessible final states for a 125 GeV Standard Model (SM)-like Higgs boson. Indeed, many distinct signal strengths, defined as production × decay rates relative to SM expectations, μi ≡ ðσ × BÞi=ðσ × BÞSM i , have been measured and used to obtain information about the couplings of the Higgs boson to electroweak gauge bosons, fermions of the third generation, and loop-induced couplings to photons and gluons. (See [5] for a thorough discussion of the use of signal strengths μi.) Fits to various combinations of reduced Higgs couplings, i.e., Higgs couplings to fermions and gauge bosons relative to their SM values, have been performed by the experimental collaborations themselves, e.g., in [3,4]. Moreover, theorists combine the results from ATLAS and CMS in global fits (see, e.g., [6,7] and references therein) in order to test consistency with SM expectations and to constrain models with modified Higgs couplings. In particular, the couplings of the observed Higgs boson could deviate from the SM predictions due to the presence of other Higgs states mixing with the observed one and/or due to new particles contributing to the loop-induced couplings. In [6], a comprehensive analysis of the Higgs signal strengths and couplings and implications for extended Higgs sectors was performed based on the experimental results as per Spring 2013. Since then, a number of new measurements or updates of existing ones were published by the experimental collaborations. From ATLAS, the VH, H → bb¯ and the H → ττ results were updated with full luminosity [8,9]. Moreover, significantly improved measurements in the H → γγ [10] and H → ZZ [11] channels were released, and the search for invisible decays in the ZH → ll þ invisible channel was updated [12]. There were also significant news from CMS, in particular updates of the H → ZZ → 4l [13] and H → WW [14] results, and—most importantly—the long-awaited final results for the H → γγ channels [15]. Furthermore, CMS published new results for H → ττ [16] and H → invisible [17]. Finally, in both ATLAS and CMS a special effort was made for probing the production of a Higgs boson in association with a pair of top quarks (ttH). From ATLAS, ttH results are available for H → γγ [10] and H → bb¯ [18], while CMS published ttH results for H → γγ, bb¯, ττ, ZZ and WW [19]. We therefore think that an update of the global coupling fits, combining ATLAS and CMS results, is timely and interesting for the high-energy-physics community in general, even more so as this will likely define the status of the Higgs couplings until the first round of Higgs results will become available from LHC Run-2 (or until an official combination of the run 1 results is done by ATLAS and CMS). Hence, in this short communication, we provide such an update for (i) the combined signal strengths, (ii) the most important reduced coupling fits, and (iii) two Higgs doublet models of type I and type II by means of a new public code, LILITH 1.0 [20]. LILITH stands for “LIght LIkelihood fiT for the Higgs.” It is a light and easy-to-use PYTHON tool to determine the likelihood of a generic Higgs boson with mass around 125 GeV from the latest experimental data, and can conveniently be used to fit the Higgs couplings and/or put constraints on theories beyond the SM. The experimental results used are the signal strengths in the primary Higgs production modes [21] as published by the ATLAS and CMS experiments at the LHC and by the Tevatron experiments. All experimental data are stored in a flexible XML database which is easy to maintain. LILITH 1.0 has been validated extensively against the ATLAS and CMS coupling fits, see [20]. A quick user guide is also available from [20]; a manual providing a complete description of the code is in preparation.II. COMBINED SIGNAL STRENGTHS We begin by showing in Fig. 1 contours of constant confidence level (C.L.) for the combined signal strengths in the μðggF þ ttHÞ versus μðVBF þ VHÞ plane for different Higgs decay modes. The left panel shows the bosonic channels H → γγ, WW, ZZ as well as VV, where VV ≡ ZZ, WW; the right panel shows the fermonic channels bb¯, ττ as well as bb¯ ¼ ττ. The combination of the ZZ and WW decay modes is justified by custodial symmetry, which implies that the HZZ and HWW couplings are rescaled by the same factor with respect to the SM. The combination of the bb¯ and ττ decay modes is justified, in principle, in models where one specific Higgs doublet has the same couplings, with respect to the SM, to down-type quarks and leptons, although QCD corrections can lead to deviations of the reduced Hbb and Hττ couplings from a common value. All results show an excellent agreement with the SM. Compared to [6], uncertainties have been significantly reduced for the fermionic channels, particularly for H → bb¯ in ttH production. As for H → γγ, while previously small excesses were observed in ggF by ATLAS and in VBF þ VH by both ATLAS and (to a lesser extent) CMS, updated results point to a more SM-like behavior. At the same time, the slight deficit previously seen by CMS in ggF is no longer present. Overall, this leads to a central value only slightly larger than unity. A comment is in order here. In the latest experimental papers, only the 68% and 95% C.L. contours are displayed in the μðggF þ ttHÞ versus μðVBF þ VHÞ plots, or in other two-dimensional projections. In order to use this information, one is forced to make assumptions on the likelihood functions—typically this means assuming normally distributed signal strengths, and this is also the approach we have adopted here. However, this is not fully satisfactory and sometimes reproduces the contours rather poorly, as in the case of ATLAS H → ZZ. (See [5] for a detailed discussion.) In the previous round of Higgs results, CMS had provided a temperature plot for the H → γγ result [15], while ATLAS had gone a step further and digitally published the two-dimensional likelihood grids for the bosonic channels [22–24] corresponding to the results of [25]. This was a boon for interpretation studies, as it rendered the Gaussian approximation unnecessary at least for these channels. We strongly hope that such likelihood grids (or digitized temperature plots) will again be made available in the future by both ATLAS and CMS. In the Gaussian approximation, we can derive a simple expression for the χ2 for each decay mode j in the form of ellipses [6]where the upper indices ggF and VBF stand for ðggF þ ttHÞ and ðVBF þ VHÞ, respectively, and μˆ ggF j and μˆVBF j denote the best-fit points obtained from the measurements. The parameters μˆ ggF, μˆVBF, a, b, and c for Eq. (1) (and, for completeness, the correlation coefficient ρ) resulting from our fit are listed in Table I. Approximating the χ2 in this form can be useful for applications that aim at a quick assessment of the compatibility with the experimental data without invoking the complete likelihood calculation. In the fits presented below, we will apply the full machinery of LILITH 1.0.III. FITS TO REDUCED HIGGS COUPLINGS Let us now turn to the fits of reduced couplings. To this end, we definewhere the CI are scaling factors for the couplings relative to their SM values, introduced to test possible deviations in the data from SM expectations. We set CW, CZ > 0 by convention; custodial symmetry implies CV ≡ CW ¼ CZ. In addition to these tree-level couplings, we define the loop-induced couplings Cg and Cγ of the H to gg and γγ, respectively. With the BEST-QCD option in LILITH 1.0, the contributions of SM particles to Cg and Cγ (as well as the corrections to VBF production) are computed at NLO QCD from the given values for CU, CD, CW, and CZ following the procedure recommended by the LHC Higgs Cross Section Working Group [26] (using grids generated from HIGLU [27], HDECAY [28], and VBFNLO [29]). Alternatively, Cg and Cγ can be taken as free parameters. Finally, invisible or undetected branching ratios can also be included in the fit. Deviations from SM expectations can be divided into two categories: (i) modifications of the tree-level couplings, as in extended Higgs sectors or Higgs portal models, and (ii) vertex loop effects from new particles beyond the SM, modifying in particular Cg and/or Cγ. We first discuss the former. Figure 2 shows results for a three-parameter fit of CU, CD, CV, assuming custodial symmetry and taking CU, CD > 0. We note that at 95.4% C.L. in two dimensions, CU and CV are constrained within roughly 20%; the uncertainty on CD is about twice as large. Although not shown in Fig. 2, CU < 0 is excluded at more than 2σ, while the sign ambiguity in CD remains. (See [30] for a discussion of wrong-sign Yukawa couplings.) The fact that μˆ ggF γγ , μˆVBF γγ , and μˆVBF VV lie somewhat above one (cf. Fig. 1 and Table I) leads to a slight preference for CV > 1. The best fit is obtained for CU ¼ CD ¼ 1.01 and CV ¼ 1.05, resulting in Cg ¼ 1.01 and Cγ ¼ 1.06. All these reduced couplings are however consistent with unity at the 1σ level. In one dimension, i.e., profiling over the other parameters, we find CU ¼ ½0.91; 1.11 ([0.82,1.22]), CD ¼ ½0.85; 1.16 ([0.70,1.32]), and CV ¼ ½0.97; 1.13 ([0.89,1.20]) at 68.3% (95.4%) C.L.; requiring CV < 1, we get CV > 0.96 (0.88). To test possible deviations from custodial symmetry, we next define CWZ ≡ CW=CZ and perform a four-parameter fit of CU, CD, CZ, CWZ. In one dimension, we find CWZ ¼ ½0.83; 1.02 ([0.75,1.16]) and CZ ¼ ½1.0; 1.24 ([0.89,1.35]) at 68.3% (95.4%) C.L. (The corresponding 68.3% and 95.4% C.L. intervals for CW are [0.95,1.11] and [0.87, 1.19].) Current Higgs data hence provide a significant constraint on deviations from custodial symmetry. So far, we considered deviations of the tree-level reduced couplings from unity, but no extra loop contributions to the effective couplings to gluons and/or photons. If instead we fix CU;D;V but allow Cg and Cγ to vary freely, corresponding to loop contributions ΔCg, ΔCγ from new physics, we obtain the result shown in Fig. 3. (In this case, Cg;γ ¼ C¯ g;γ þ ΔCg;γ , with C¯ g;γ the contribution from SM particles.) The left panel corresponds to the case where CU ¼ CD ¼ CV ¼ 1; here the best-fit point has ΔCg ¼ −0.01 and ΔCγ ¼ 0.09, as expected from Fig. 1. The right panel shows the situation when CU, CD, CV are fixed to the best-fit values previously obtained: CU ¼ CD ¼ 1.01, CV ¼ 1.05; in this case the best-fit point has ΔCg ¼ −0.04 and ΔCγ ¼ 0.06. In both cases, the SM solution ΔCg ¼ ΔCγ ¼ 0 lies within the 1σ contour.IV. TWO-HIGGS-DOUBLET MODELS In view of the discussion above it is clear that models with an extended Higgs sector will be significantly constrained by the data. In particular, it is interesting to consider the simplest such extensions of the SM, namely two-Higgs-doublet models (2HDMs) of type I and type II. The basic parameters describing the couplings of the neutral Higgs states to SM particles are only two: the CP-even Higgs mixing angle α and the ratio of the vacuum expectation values, tan β ¼ vu=vd. The couplings, normalized to their SM values, of the Higgs bosons to vector bosons (CV) and to up- and down-type fermions (CU and CD) are functions of α and β as given in Table II; see, e.g., [31] for details. The type I and type II models are distinguished only by the pattern of their fermionic couplings. To investigate the impact of the current Higgs data on 2HDMs, we vary α ¼ ½−π=2; þπ=2 and β ¼ ½0; π=2½. (Note that this results in CU > 0 in our convention, while CV can be negative). We implicitly assume that there are no contributions from non-SM particles to the loop diagrams for Cγ and Cg. In particular, this means our results correspond to the case where the charged Higgs boson, whose loop might contribute to Cγ , is heavy. The results of the 2HDM fits are shown in Fig. 4 for the case that the observed state at 125.5 GeV is the lighter CP-even h. In the case of the type I model, we note a broad valley along the SM limit of cosðβ − αÞ ¼ 0, which is rather flat in tan β. For tan β ≳ 2, at 95.4% C.L. j cosðβ − αÞj can be as large as ≈0.4; only for tan β ≪ 1, one is forced into the decoupling/alignment regime. The situation is quite different for the type II model. Here we observe two narrow valleys in the tan β versus cosðβ − αÞ plane. The first one lies along the SM solution cosðβ − αÞ ¼ 0; the largest deviation here occurs around tan β ≈ 1, where cosðβ − αÞ ≈ 0.13 is allowed at 95.4% C.L.; for both tan β ≫ 1 and tan β ≪ 1 one is forced into the decoupling/alignment regime. The second minimum is a banana-shaped valley with tan β ≳ 3 (5) and cosðβ − αÞ ≲ 0.35 (0.5) at 68.3% (95.4%) C.L. This corresponds to the degenerate solution with CD ≈ −1. In one dimension, the 68.3% (95.4%) C.L. limits are j cosðβ − αÞj < 0.19 (0.34) for type I and cosðβ − αÞ¼½0; 0.29 (½−0.05; 0.47) for type II; the latter shrinks to cosðβ − αÞ¼½0; 0.07 (½−0.05; 0.11) when demanding CD > 0. Constraints on and future prospects for 2HDMs in light of the LHC Higgs signal (status spring 2013) were discussed in detail in [32] taking into account all relevant theoretical and experimental constraints. The results of that paper will be somewhat modified by the new constraints presented here; this is presently under study. V. CONCLUSIONS We presented a brief update of the global fits of the 125.5 GeV Higgs boson using all publicly available experimental results as per summer 2014. The fits were done with LILITH 1.0, a new user-friendly public tool for evaluating the likelihood of an SM-like Higgs boson in view of the experimental data. Our results can be summarized as follows:(1) The latest ATLAS and CMS results for the H → γγ decay mode now point to a very good agreement with the SM; concretely we get μˆ ggFþttH γγ ¼ 1.25 0.24 and μˆVBFþVH γγ ¼ 1.09  0.46 with a correlation of ρ ¼ −0.30. (2) In the CU, CD, CV reduced coupling fit, we found CU ¼ 1.01  0.1, CD ¼ 1.01  0.16 and CV ¼ 1.05  0.08; in terms of the loop-induced couplings this corresponds to Cg ¼ 1.01  0.11 and Cγ ¼ 1.06  0.11 (in one dimension). (3) Custodial symmetry can also be tested. We found CWZ ¼ 0.92  0.1, hence compatibility with custodial symmetry at the 1σ level. (4) Assuming SM-like couplings, the limit for invisible decays is Binv < 0.12 at 95.4% C.L. This changes to Binv < 0.34 when CU, CD, CV (or even CU, CD, CV, Cg, Cγ ) are allowed to vary. (5) In the context of 2HDMs, barring loop contributions from the charged Higgs, the 95.4% C.L. limits in one dimension are sinðβ − αÞ > 0.94 in type I and sinðβ − αÞ > 0.90 in type II. As mentioned, one of the limitations of these fits is the use of the Gaussian approximation. This could easily be avoided if the experimental collaborations published the two-dimensional likelihood grids in addition to the 68% and 95% C.L. contours. Another limitation is induced by the combination of production modes, typically ggF þ ttH and VBF þ VH, in the experimental results. This could be overcome if the collaborations provided the signal strength likelihoods beyond two-dimensional projections—the optimum would be to have the signal strengths as functions of mH separated into all five production modes ggF, ttH, VBF, ZH, and WH, as recommended in [5]. We hope that this way of presentation (in digital form) will be adopted for Higgs results at run 2 of the LHC. The structure of LILITH is well suited to make use of such extended experimental results.	We provide an update of the global fits of the couplings of the 125.5 GeV Higgs boson using all publicly available experimental results from run 1 of the LHC as per summer 2014. The fits are done by means of the new public code LILITH 1.0. We present a selection of results given in terms of signal strengths, reduced couplings, and for the two-Higgs-doublet models of type I and II.
18	2019	Testing collapse models with levitated nanoparticles: Detection challenge	A. Vinante, A. Pontin, M. Rashid,1 M. Toroš,2 P. F. Barker,2 and H. Ulbricht	Testing collapse models with levitated nanoparticles: Detection challenge  (Received 20 March 2019; published 16 July 2019) We consider a nanoparticle levitated in a Paul trap in ultrahigh cryogenic vacuum, and look for the conditions which allow for a stringent noninterferometric test of spontaneous collapse models. In particular we compare different possible techniques to detect the particle motion. Key conditions which need to be achieved are extremely low residual pressure and the ability to detect the particle at ultralow power. We compare three different detection approaches based, respectively, on an optical cavity, an optical tweezer, and an electrical readout, and for each one we assess advantages, drawbacks, and technical challenges. DOI: 10.1103/PhysRevA.100.012119 I. INTRODUCTION Spontaneous wave-function collapse (or dynamical reduction) models (CMs) [1–5] have been proposed to reconcile the linear and deterministic evolution of quantum mechanics with the nonlinearity and stochasticity of the measurement process. According to CMs, random collapses in space (i.e., localizations) of the wave function of any system occur spontaneously, independently of measurement processes, leading to a progressive spatial localization. The collapse rate scales with the size of the system, leading to rapid localization of any macroscopic system, while giving no measurable effect at the microscopic level, where standard quantum mechanics holds. Importantly, CMs lead to a natural solution of the measurement problem, by predicting the emergence of welldefined outcomes in any experiment in agreement with the Born rule. The most general model described in literature is the continuous spontaneous localization (CSL) model [2,3], a refined version of the earliest collapse model proposed by Ghirardi, Rimini, and Weber (GRW) [1]. Other more specific models have been proposed in literature, the most notable being the Diósi-Penrose gravity-induced collapse model [6–8]. In this paper we will focus on the CSL model only, as other models can be usually regarded as special cases, or slight variations, of CSL. CSL is characterized by two phenomenological constants, a collapse rate λ and a characteristic length rC, which characterize, respectively, the intensity and the spatial resolution of the spontaneous collapse process. λ and rC are free parameters which should be derived, or bounded, by experiments. The standard conservative values suggested by GRW are λ  10−16 s−1 and rC = 10−7 m [1,2] and are sufficient to guarantee almost instantaneous localization of macroscopic objects. A strongly enhanced value for the collapse rate has been suggested by Adler [9], motivated by the requirement of making the wave-function collapse effective at the level *A.Vinante@soton.ac.uk †a.pontin@ucl.ac.uk of latent image formation in the photographic process. The values of λ suggested by Adler are ≈109±2 times larger than the GRW values at rC = 10−7 m, and ≈1011±2 times larger at rC = 10−6 m. Several precision experiments have been recently exploited to set significant bounds on the CSL model parameters. The direct effect of collapse models such as CSL is to suppress quantum superpositions, resulting in a loss of coherence in interferometric matter-wave experiments [10]. On top of that, the noise field associated with the collapse implies a violation of the energy conservation. So called noninterferometric tests have been proposed to look for these effects, which include spontaneous emission of x rays [11,12], force noise in mechanical systems [13–24], and spontaneous heating of bulk matter [25] or ultracold atoms [26,27]. At present, noninterferometric tests set by far the strongest bound on CSL parameters, which are summarized in Fig. 1. Here, we consider noninterferometric tests based on mechanical systems. In this approach, one looks for the universal force noise which is predicted to be induced by CSL in any massive mechanical system. So far, experiments based on ultracold cantilevers [19,20] and gravitational wave detectors [21–24] are setting the strongest bounds. It has been suggested that levitated nanoparticles or microparticles would be a nearly ideal platform to perform more sensitive tests [28,29]. Ideally, one needs to work with the lowest possible temperature and dissipation in order to minimize thermal noise. We consider a possible experimental implementation based on ion trap techniques and point out that, for the extremely low thermal noise required by this experiment, the nanoparticle detection becomes a very crucial issue. We consider three possible detection techniques and discuss potential advantages and drawbacks. II. PHYSICAL SYSTEM AND THE BASIC MODEL A. CSL noise Our goal is to monitor a levitated nanoparticle in order to detect or place strong upper bounds on the universal force noise predicted by spontaneous collapse models such as CSL. The CSL force noise power spectral density (PSD) acting on a homogeneous sphere can be written as [17–19]. where ρ is the particle mass density, R is the particle radius, and m0 is the nucleon mass [30]. Note that Eq. (1) grows as R2 for R  rC, scaling therefore as a surface noise, and as R6 for R  rC. So, if the main background is surface force noise, as for instance due to gas collisions, the signal to noise ratio (SNR) is almost constant for R > rC. If the main background force noise scales with the volume (as in the case of material-dependent losses) the SNR, according to Eq. (1), will feature a shallow maximum at R  2rC. Therefore, if we wish to probe the standard value rC = 100 nm, as a general rule the radius of the particle should be at least of the order of 200 nm. A similar analysis can be performed also for other collapse models. For example, for the Diósi-Penrose (DP) model the force noise power spectral density is given by Sf f,DP = 2 √π 9 GhR¯ 3ρ2( a rDP ) 2, where G is the gravitational constant, R (ρ) is the radius (density) of the nanoparticle, a is the lattice constant of the material, rDP is the DP cutoff parameter [31], and we have assumed rDP  a [17,32]. In particular, the results in the following sections can be reinterpreted in terms of the DP parameter rDP by exploiting the relation Sf f,DP(rDP) = Sf f,CSL(λ,rC ), i.e., one has a simple relation between rDP and λ (rc) at a fixed value of rc (λ). B. Particle For simplicity we will consider a spherical nanoparticle, although comparable results could be achieved by other geometries [24]. We will take SiO2 as standard material. Although it is not the optimal choice for CSL, because of the relatively low density, it is the most used material when optical detection is involved. In general, optical detection requires low absorption dielectric materials, such as silicon or silica. In principle a much larger variety of materials could be used in the case of electrical detection. C. Paul trap As we wish to work in an ultracold and ultraisolated environment, standard optical trapping is not viable because of the strong heating of the nanoparticle. Alternative levitation methods which avoid this problem are Paul traps [33] and superconducting magnetic traps [34]. Both methods cause very small heat dissipation in the levitated particle, so they are expected to be compatible with a cryogenic environment. We will assume here a Paul trap approach, as related technology has been pushed to quite an advanced level by the ion trapping community. For the sake of simplicity we will not go here into technical details on the geometry of the trap. We will only assume that the secular resonant frequency of one relevant translational mode can be set at f0 = 1 kHz. Such a value can be obtained by a Paul trap with electrode effective distance of the order of 1 mm or smaller, voltage bias of some tens of volts, and a charge on the particle of 10–1000 e. These parameters should be also readily compatible with a cryogenic operation. Among possible sources of noise and decoherence related to the trap, we can mention surface losses in the electrodes and voltage noise in the driving ac and dc bias. Bias voltage noise is a known effect in ion traps, and can be a potentially limiting factor here because the trap potentials have to be kept continuously active. For an order-of-magnitude estimation, let us assume an electronics voltage noise Sv = 10 nV/ √Hz at the secular frequency. This value corresponds to a dynamic range larger than 109, and should be readily achievable, although it seems nontrivial to improve much over this value. Then, for a trapped charge of q = 30 e, and an effective electrode distance of 500 μm, one estimates a force noise Sf  Svq/d = 9.6×10−23 N/ √Hz. As shown in Fig. 3, this noise would be comparable with the effect predicted by the CSL model for the standard length parameter rC = 10−7 m and the collapse rate λ = 10−10 Hz. Therefore, a heavy suppression of electronic noise would be needed in order to probe the CSL collapse rate much lower than the latter value. In addition, we note that this noise contribution would increase for larger charge. D. Environment The environment of the particle leads to noise and decoherence through many different channels. A minimal list of sources includes scattering with gas particles and scattering, absorption, and emission of thermal photons. In addition, any trapping mechanism typically involves some kind of decoherence. For a Paul trap, besides voltage noise in the driving electrodes, there will be interaction with the electrode surface, as well as electrical losses if an electrical detection circuit is coupled to the trap. Ambient vibrational noise (seismic or acoustic noise) has to be eventually considered. Collision with gas particles and emission of blackbody radiation will significantly affect the particle dynamics; not only do they represent a noise source but they are also the main mechanisms for the thermalization of the particle. Thus, the residual gas pressure Pg and temperature Tg, together with the steady-state power Wabs absorbed by the particle will determine its equilibrium bulk temperature T as discussed in the following section. 1. Thermal equilibrium The heat flow from a hot nanoparticle to a cold surrounding gas in the molecular regime can be calculated using the formula [35] where  α is a thermal accommodation factor, vt = 8kBTg/(πm) is the gas thermal velocity with m the molecular mass, and γs is the specific-heat ratio. Here all parameters can be easily determined, except for the accommodation factor 0 <α< 1. Typical values of order 0.4 are reported in literature for specific experimental situations [35]. The heat flow by blackbody radiation is described by the expression [36] where V is the particle volume and ζ (5) ≈ 1.04 is the Riemann zeta function. The dependence on T 5 is typical of a subwavelength nanoparticle. Here, the flow is controlled by the absorption coefficient abs = Im[(BB − 1)/(BB + 2)] where BB is the blackbody emissivity. For typical situations, as for instance silica at 100 K, this term can be taken of the order of 0.1. By setting Q˙BB + Q˙ gas + Wabs = 0 one can estimate the equilibrium internal temperature T of the particle as a function of the environmental conditions and the input power. Figure 2 illustrates the dependence of the equilibrium temperature of the nanosphere on the gas pressure Pg and absorbed power Wabs in a relevant region of the parameter space. Two regimes are clearly visible. For low pressure and high power the particle is thermalized by radiation, and the equilibrium temperature is independent of pressure. For high pressure and low power the particle is thermalized by the gas and the equilibrium temperature depends solely on the ratio Wabs/Pg. 2. Thermal force noise To estimate the thermal force noise due to the gas in a hot-particle scenario we follow the model of Ref. [37], i.e., we separately consider the contributions due to the impinging and emerging gas particles. The model assumes two different baths with temperature Ti = Tg for the impinging molecules and Te = Ti + a(T − Ti) for the emerging molecules where a is another phenomenological accommodaFIG. 2. Internal temperature of the nanosphere as a function of the gas pressure and the absorbed power. The particle is a silica nanosphere with R = 200 nm, the residual gas is helium at Tg = 300 mK, the thermal accommodation factor has been set to α = 0.4, and the emissivity has been set to abs = 0.1. tion factor 0 < a < 1. The underlying idea is that the scattering of a gas molecule off the nanoparticle is not elastic: the particle is assumed to partially thermalize with the nanoparticle before being reemitted. This model of scattering is abundantly supported by experimental literature [37]. The two baths lead to different mechanical damping rates: where ms is the mass of the particle. The force noise can then be calculated as There will also be, even in the absence of detection, a force noise due to recoil from emission of blackbody radiation. Following calculations similar to the one leading to Eq. (2), one arrives at the formula [36] It is easy to check that this contribution is exceedingly small compared to the one of the gas for any realistic set of parameters, except under the condition of extremely low pressure below 10−15 mbar and relatively high power. The reason is that photons can remove efficiently energy but at the same they carry very little momentum. An even smaller contribution arises from the scattering and absorption of blackbody radiation coming from the cold environment at temperature TgFigure 3 shows the thermal force noise as a function of gas pressure for different values of the absorbed power. Clearly, going to sufficiently low pressure will eventually suppress thermal noise below any detectable level. For instance, for the lower value of the pressure reported in literature 10−17 mbar [38] and absorbed power 10−18 W, the thermal noise would compare to the extremely tiny effect of CSL according to the GRW values. Unfortunately, other effects become dominant in this regime, in particular, electrical noise in the Paul trap and backaction noise from the detection. 3. Ambient vibrational noise In the following we will assume that ambient vibrational noise is negligible. Because of the smallness of the particle considered here, this assumption is rather reasonable. To give a rough idea, let us consider a typical figure for standard seismic noise at level of Sa = 10−6 m/s2/ √Hz. For a silica nanosphere with R = 200 nm, the corresponding force noise would be Sf = 7×10−23 N/ √Hz at the resonance frequency, roughly equivalent to the CSL noise at rC = 10−7 m and λ = 10−11 Hz. This suggests that a moderate and feasible mechanical isolation of the order of 60 dB would be enough to suppress seismic noise well below the CSL noise considered in this paper. However, seismic as well as acoustic noise could influence the experiment in other ways. For instance, the need to lock an optical cavity would impose additional and stricter requirements on the total noise rms at low frequency, but its effect depends drastically on how the cavity is engineered. As such, it will not be taken into account quantitatively in the rest of the paper. III. DETECTION SCHEMES Here we come to the key issue we wish to study in this paper, namely, how to choose and to optimize the detection of the particle. We will consider three detection options. An optical cavity readout is discussed in Sec. III B, an optical tweezer is considered in Sec. III C, and an electrical readout based on a superconducting quantum interference device (SQUID) is considered in Sec. III D. Before going into the details of the three techniques, we will analyze in Sec. III A the general features of two different measurement strategies, a stationary continuous one and a stroboscopic reheating, finding that they are in principle equivalent. Based on this conclusion we will focus in the following sections on the continuous measurement strategy. A. Continuous and stroboscopic measurement In this section we will try to compare continuous and stroboscopic measurements. Some relevant considerations can be done regardless of the specific detection technique. In a steady-state approach the position of the trapped mechanical harmonic oscillator is continuously measured. The acquired signal is fast Fourier transformed (FFT) and periodograms are averaged to provide an estimation of the PSD. An example of application of this method in the context of testing collapse models is given by recent cantilever experiments [20]. In general, there will be two contributions to the PSD, a wide-band position measurement noise Sxx and the true oscillator noise with PSD given by |χ(ω)| 2Sf f wher is the Lorentzian mechanical susceptibility. For high-Q systems, and provided Sxx is low enough, the oscillator noise will be dominant around the resonant frequency ω0, over a given bandwidth  f = ω/2π which depends on the actual values of Sxx and Sf f . If we define  f = | f2 − f1|, where f1,2 = ω1,2/2π are the frequencies at which Sf f |χ(ω)| 2 = Sxx, we find that The estimation of Sf f is inferred from the data available within this bandwidth, for instance, by fitting the estimated PSD. The relative uncertainty on Sf f will be of the order of ≈1/ √tm f where tm is the measurement time. For instance, for a single FFT with acquisition time tm the frequency resolution is 1/tm and therefore there will be about n = tm f independent samples for the determination of Sf f . The same dependence on tm is obtained if FFT averaging is implemented. This simplified argument can be made more rigorous by means of Wiener filter theory [39]. Now, let us assume that the detection is Heisenberg limited, i.e., Sf f Sxx = h¯2 [40], and that thermal noise is negligible. In this ideal case the force noise is completely determined by the measurement backaction, and therefore by the strength of the measurement. By using the Heisenberg condition to eliminate  We can connect this ideal measurement bandwidth with the phonon heating rate γ due to Sf f , which is generally defined by the relation By comparing Eqs. (10) and (11) we can conclude that  f = 2γ/π  γ , i.e., the effective measurement rate  f is set by the phonon heating rate γ . In the nonideal case other force noise sources are present. We may define a factor N such that Sf f Sxx = N2h¯2 and performing a similar analysis we conclude that  f = 2γ/(πN)  γ/N. We turn now to the stroboscopic reheating strategy, which is described, for instance, in Refs. [28,41]. Here one works in nonstationary conditions. In a first step the trapped particle is monitored with high sensitivity, and feedback cooling is applied to prepare the system in a state as cold as possible with mean phonon number n1. Subsequently, feedback is switched off and the oscillator will reheat due to the force noise Sf f . A possible advantage of this approach is that during the free reheating evolution one is allowed to switch off the detection, therefore avoiding completely measurement backaction. After a given evolution time t, the system energy is measured again and the heating rate γ is inferred through the relation where n2 is the mean phonon after the free evolution. How fast can a measurement of γ and hence Sf f be? Again, let us start with an ideal Heisenberg limited detector, in the absence of other noise sources. Under these ideal conditions one may in principle perform feedback cooling to the ground state, so that in the initial state n1  0 with uncertainty σn1 = 1/2. After a free evolution time t the energy is measured again, giving n2 = γ t with uncertainty σn2  √n2. The relative uncertainty on the estimation of γ will be From a statistical point of view we can thus interpret γ t as an effective number of independent samples for the experimental estimation of γ , so that γ can be interpreted as an effective measurement rate. We have thus arrived essentially at the same expression of the continuous measurement case, apart from constants of order 1, meaning that the time required to estimate Sf f or equivalently γ with a given accuracy is the same. In the case of nonideal detection, we proceed as before by defining a factor N, and the initial state and the final state will be affected by a larger uncertainty. As in the continuous case, this leads to a reduction of the measurement rate to γ/N. In conclusion, the two strategies appear to be roughly equivalent from a fundamental point of view. The choice of one instead of the other one will depend mostly on technical implementation aspects. Continuous strategies are in principle easier to implement, being based on a stationary FIG. 4. Simplified scheme of the experiment. A Nd:Yag laser at 1064 nm is locked on the fundamental mode of an optical cavity by means of a standard Pound-Drever-Hall technique. A silica bead is held at the center of the cavity field by a linear Paul trap. Measurement of the particle dynamic is obtained by homodyning the transmitted cavity field. state. However, measurements at very low coupling could be challenging for technical reasons and require high stability of the trap frequencies. Stroboscopic measurements do not suffer from the last problem but require the ability to deal with the transients associated to switching detection on and off, which can be extremely challenging in practice. B. Optical cavity Here we consider an optical cavity exploited as a pure displacement sensor. Contrary to the tweezer approach, the cavity is sensitive mainly to one degree of freedom of the particle motion. The requirements to achieve a meaningful measurement in the CSL context are quite stringent and will drive the design of the detection in a direction that is quite different from the typical optomechanical framework. We show in Fig. 4 a schematic view of the experiment. A nanoparticle is held at the center of an optical cavity by a Paul trap. The transmitted cavity field is then analyzed by homodyne detection. In general, the presence of a dielectric in an optical cavity causes the optical resonance to be downshifted in frequency. For a spherical Rayleigh particle this is simply given by [36,42] δω = go cos[φ] 2,where φ = kx is the position of the particle in the cavity standing wave, with k the wave number, and go is the characteristic optomechanical coupling strength. Thus, the particle dynamics can be monitored by an optical phase sensitive detection. The highest sensitivity is obtained at φ = π/4 (maximum of ∂δω ∂x ) where, to first order, the transduction is linear. At the same time, however, dipole forces provide a trapping potential with a trap frequency at the antinode (φ = 0) given by [43] ωt = 2¯hk2go ms nc. (14) Here, nc is the intracavity photon number. This potential is exploited in many applications [44–46]; here, however, it represents an unwanted perturbation that can easily dominate over the Paul trap potential. This very simple description already allows us to formulate two stringent requirements. First, assuming that the Paul trap and the cavity can be accurately aligned so that φ = π/4, the rms displacement needs to be small enough to keep the cavity transduction in the linear regime. This displacement has to include both the thermal secular motion and the driven micromotion. Second, the optical potential needs to be negligible compared to the Paul trap potential. At the optimal position for the detection the optical potential exerts a force displacing the steady-state position of the particle moving it away from the center of the Paul trap potential. As a consequence we need ωt < ω0. This requirement strongly limits the maximum intracavity power allowed, pointing to a low finesse cavity as the best choice. Another critical aspect is the backaction introduced by the detection which will ultimately limit the force noise sensitivity. The main sources of backaction are radiation pressure shot noise and recoil heating [47]; however, cavity dynamical effects must be accounted for since a nonvanishing detuning will introduce optical spring and damping. In the following we are going to show the expected sensitivity assuming an ideal homodyne detection, which is the best case scenario due to technical aspects. Laser frequency noise represents a major technical limitation. For a resonant optical drive, it can be considered as an additive noise limiting the sensitivity far above the shot noise [48]. In order to leave unaltered the particle signal in the homodyne, the cavity lock bandwidth has to be much smaller than f0. This imposes an extremely demanding requirement in terms of displacement noise of the cavity mirrors, especially considering the cryogenic environment. We consider an asymmetric low finesse (≈1000) cavity undercoupled on the injection side. The cavity length is L = 15 mm, has a waist of wst = 62 μm, and is coupled to a 200-nm radius silica nanoparticle. Assuming a 1064-nm laser driving the cavity, we have a single-photon optomechanical coupling g/2π = xzpf kgo/2π  5 Hz. We consider optical input powers ranging from 0.1 to 20 μW for which we can estimate the power absorbed by the particle. Assuming a pressure of 10−13 mbar, which should be achievable in an ultracryogenic environment [49], the particle equilibrium temperature is expected to be between 20 and 70 K following a W 1/5 abs power law since it is completely determined by blackbody radiation, as shown in Sec. II D. We show in Fig. 5 the expected PSD of the homodyne detection of the transmitted beam normalized to the shot noise for an input power of 10 μW along with all main contributions. Despite an effective bath temperature of ≈20 K, the total PSD is dominated by photon recoil. This remains true for all input powers considered. Also shown in Fig. 5 is the estimated limit to sensitivity due to laser frequency noise. Typical spectra for high stability Nd:Yag lasers give Sνν (f ) 4×108/ f 2 Hz2/Hz. In order to compare the sensitivity to CSL, we calculate the total force noise acting on the particle and express it in terms of collapse rate λ assuming a characteristic length rC = 10−7 m. Therefore, the collapse rate that will give a signal to noise ratio of 1 is summarized in Fig. 6, where we plot the imprecision noise as a function of the collapse rate for both shot-noise limited and frequency noise limited detection. For the former case, imprecision noise decreases for an increasing power as is typical for a shot-noise limited detection. For the latter case, the imprecision noise is relatively constant. However, the CSL noise sensitivity is always reduced for an increasing power since the system is dominated by photon recoil. −20 −10 0 10 20 10−1 101 103 105 107 Sout/Shot FIG. 5. Cavity output homodyne spectrum normalized to shot noise. Black is the total noise, blue (long-dashed) is the quantum noise, red dot-dashed is the thermal noise, and red dashed is the photon recoil. Finally, the (dashed) horizontal green line represents the typical frequency noise that could hinder the particle detection. Another important parameter is the measurement bandwidth since it directly impacts measurement time and as a consequence the requirements on stability on all other parameters. This is shown in Fig. 6 as well. For a frequency noise limited detection this turns out to be ≈150 mHz for the smaller power and up to ≈2 Hz for the highest. For shot-noise limited readout the situation would be much more favorable with a bandwidth varying from ≈400 mHz to ≈60 Hz. Of course, achieving this seems rather challenging especially considering the cryogenic environment. Indeed, for the configuration considered, the shot-noise level would correspond to a relative displacement noise of the cavity mirrors of the order of ≈10−33 m2/Hz. So far we have assumed a rather optimal scenario; here we discuss some criticalities. First, as stated before, the rms displacement of the particle needs to be sufficiently small to remain in the linear transduction region of the cavity, that is, x = (kBTeff/mω2 0 ) 1/2  λ/16. For the parameters  considered here we have x  0.4λ meaning that additional active feedback is required in order to reduce the particle fluctuations. To meet this requirement the mechanical quality factor needs to be reduced by a factor 100. This seems reasonable considering that its value is expected to be ≈109 at Pg = 10−10 mbar and increases inversely proportional to pressure. The optical spring near resonance has a linear dependence on detuning; we can impose a shift in frequency equal to the mechanical linewidth         tot from which we can get a limit on the detuning for given cavity parameters, that is, where k is the wave number, κ is the cavity half linewidth, and κin is the contribution to it due to the input port. For our parameters and an input power of 10 μW this is roughly   10−7κ  0.4 Hz, which is an extremely demanding requirement. Just to give a reference the maximum shift is ≈1 Hz. Since we are in the deeply bad cavity regime the same kind of limit imposed using the optical damping is much less stringent. The situation becomes more relaxed if we assume some kind of active feedback increasing the mechanical linewidth. However, this increase needs to be by a factor ≈1000 at least. At the same time, any blue detuning will give rise to dynamical instability. Thus a small but finite detuning seems rather necessary. Some final considerations are required concerning the optical power considered. The plots in Fig. 6 summarizing the performance in terms of sensitivity to CSL noise consider input powers ranging from 0.1 to 20 μW. The upper bound to the input power is due to the optical potential as previously discussed. The lower bound is somewhat more free but will ultimately be set by technical considerations concerning cavity locking. Indeed, a simple locking scheme as depicted in Fig. 4 is quite challenging to implement with the minimum power considered. However, more complicated locking schemes could be implemented. For example, the cavity could be locked to a higher-order cavity mode (i.e., a TEM01) which has a node along the cavity axis and is thus not coupled to the particle to first order, while a second laser is offset locked to be resonant to the cavity fundamental mode. A detailed study of the optimal optical setup is not the main focus of this paper and will be the topic of future research. As a conclusion to this section we list in Table I the important parameters and we summarize here some key aspects and requirements necessary to implement the cavity detection of the nanoparticle motion. (a) (b) FIG. 7. (a) Detection using a paraboloidal mirror and (b) detection using a dual counterpropagating beam setup. (1) The cavity and Paul trap need to be accurately aligned so that φ = π/4. (2) The intracavity power has an upper bound due to the gradient force trapping potential, i.e., ωt < ω0. (3) The particle thermal motion needs to be reduced to guarantee xth  λ/16. (4) The accuracy and stability of the detuning of the optical field need to be   10−7κ. (5) The cavity stability and displacement noise have to be controlled to allow a lock bandwidth much less than ω0. C. Optical tweezer We consider two different tweezer-based detection setups depicted in Fig. 7. Specifically, we consider a nanoparticle in a Paul trap with trap frequency ω0 which is monitored either continuously or stroboscopically using a tweezer. The trapped nanoparticle scatters light which is collected using optical elements and then directed towards the detector; similarly as in the cavity case we again consider the situation where the scattered light interferes with a local oscillator in a homodyne scheme [50]. For concreteness, we consider a laser wavelength λ = 1550 nm and assume a spherical silica (SiO2) nanoparticle of radius R = 200 nm. The Paul trap potential, UPaul, can be significantly perturbed by the optical potential, Uoptical, as well as by the effective potential Uscatt ∝ Fscattx generated by the nonconservative scattering force Fscatt oriented in the propagation direction x of the laser beam [51]. The presence of the optical potential Uopt ∝ ω2 optical ∝ P results in a change of the trap frequency, i.e., ω2 0 + ω2 opt, where ωopt and P denote the optical frequency and laser power, respectively. On the other hand, the effective scattering potential Uscatt displaces the origin of the combined Paul and optical trap; if the displacement is too large the nanoparticle can leave the harmonic region of the trap, possibly even resulting in particle loss. For the considered nanoparticle of size R = 200 nm, noting that Uoptical ∝ R3 and Uscatt ∝ R6, we find the strongest constraint coming from the requirement |∂xUscatt|  |∂xUPaul|; specifically, assuming a Paul trap frequency ω0 = 2π×1 kHz we are limited to powers of P  1 nW. To avoid the problem related to the scattering potential Uscatt one can, however, consider two counterpropagating beams which could be implemented using the lens setup shown in Fig. 7(b); requiring Uoptical < UPaul we then find a much less restrictive condition on the laser power, i.e., P  1 mW. Ideally we would like to set the laser power P to the value which minimizes the sum of the backaction and imprecision noise [40]. However, the optimal value of P is relatively low and sources of dark noise present in physical detectors must be taken into account. From the laser power P and the particle position x one can then estimate the photocurrent impinging on the detector; we quantify it by the total efficiency η, and denote the detected power by Pdet. In particular, adopting a simple semiclassical calculation we can find the conversion factor between the detected power and the particle position [50]. We compare Pdet to the detector’s dark noise, which can be estimated by considering the noise equivalent power (NEP) of the detector [52]. The NEP noise floor, although not an intrinsic limitation of the tweezer setup, is expected to be the dominant contribution to the noise floor (see Fig. 8). The fundamental noise floor in a tweezer setup, similarly as in the cavity setup, is given by the shot noise. The nanoparticle motion is affected by noise from gas collisions, photon recoil, and backaction. In Fig. 8, we show the expected homodyne spectrum for the tweezer case. We are assuming a pressure of 10−13 mbar, a numerical aperture NA = 0.6 limited by reasonable trap geometries, and an input power of 500 nW at a wavelength of 1550 nm. With this power we estimate a particle equilibrium temperature of T  60 K, quite similar to the cavity case shown in Fig. 5. Already at such low power the particle dynamics is dominated by quantum noise. To summarize the performance and allow an easy comparison, we show, in Fig. 9, the imprecision noise and measurement bandwidth as a function of detectable λ, in analogy to Fig. 6. The main advantage of the tweezer lies in the much wider range in acceptable power. Contrary to the cavity case, the maximum sensitivity is obtained in the position of maximum intensity of the optical field. This allows one to fully exploit the canceling forces due to the counterpropagating beams. However, the resulting displacement sensitivity is lower with a direct impact to the attainable measurement bandwidth. The most critical aspect of the tweezer approach lies in the requirements on the positioning accuracy of the two objective lenses. In order for the suppression of the scattering force to be effective, the position and size (i.e., nonidealities) of the waist of the two lenses needs to be close to a fraction of FIG. 9. Imprecision noise and measurement bandwidth as a function of the collapse rate that would provide a signal to noise ratio of 1 given the estimated total force noise acting on the particle and assuming rC = 10−7 m. Continuous lines refer to a NEP noise limited detection while dashed lines refer to a shot-noise limited one. the nominal waist, which is of the order of ≈2 μm (i.e., half the radius of the core of a telecom single mode fiber). While this is standard practice at room temperature, maintaining the alignment in an ultracryogenic environment is a completely different endeavor. As for the previous section we list in Table II important parameters and summarize the key aspects and requirements of the tweezer approach. (1) The tweezer and Paul trap need to be aligned with an accuracy better than the optical waist. (2) The counterpropagating configuration is required. (3) Any misalignment of the two beams will result in a dramatic increase of the scattering force. D. Electrical readout with a SQUID Instead of detecting the oscillating particle using an optomechanical setup, it has been proposed to use a direct electrical detection [53]. To this end, a pair of electrodes, for instance, two endcaps, are used to detect the motion of the particle along the axis orthogonal to them. The electrical signal induced in the electrodes could be eventually read out by a SQUID current sensor. SQUIDs are the best electrical amplifiers available in a wide range of frequencies, and are a natural choice when working at low temperature and low frequency. Unfortunately the electromechanical coupling of the above scheme is very weak for a current-based detection at low frequency. As shown in Refs. [53,54], the motion x of the particle with charge q will induce a current in the circuit connected to the electrodes:where d the effective gap between the electrodes and ω is the oscillation frequency. Furthermore, it was shown that the mechanical oscillator, as seen from the electrical circuit, is dynamically equivalent to a RLC oscillator with effective inductance Lm = ms/β2 and capacitance Cm = 1/ω2 0Lm, where β = q/d. For a reasonable set of parameters envisaged for the experiment, ms = 6×10−17 kg (standard 200-nm-radius SiO2 particle), q = 103 e, d = 300 μm, and ω0/2π = 1 kHz, one obtains effective LC inductance and capacitance Lm = 2×108 H and Cm = 10−16 F. For comparison a typical SQUID sensor has an input inductance Li  10−6 H, which means a huge impedance mismatch by 14 orders of magnitude. In fact, with a typical input current noise Si  0.1 pA/ √Hz, a direct coupling of the electrodes to a SQUID would lead to a very poor displacement resolution Sx  30 μm/ √Hz. One of the reasons for such a significant mismatch is that, according to Eq. (16), the displacement to the current transfer function is proportional to ω, which makes this scheme very suboptimal at low frequency. This situation is analogous to Faraday detection of magnetic fields. To overcome this problem, one can explore different strategies. The first is to reduce impedance mismatch, and the second is to resort to other types of amplifier, such as standard field-effect transistor (FET) transimpedance amplifiers or single-electron transistors. 1. SQUID with untuned transformer The simplest way to reduce impedance mismatch to a SQUID is to interpose a superconducting transformer between the electrodes and the SQUID, as shown in Fig. 10. This increases the effective input inductance to a value of the order of the inductance Lp of the primary coil. Low-loss superconducting transformers with inductance up to 10 H, Q  106, and a volume of around 1 L have been used to match a SQUID to the capacitive transducer of resonant bar gravitational wave detectors [55]. Larger values are in principle achievable but are hardly compatible with standard cryostats. Let us consider as a maximum realistic choice Lp = 10 H, and assume the transformer geometrical coupling k = M2/(LpLs) to be close to the maximum allowed value of 1. For optimal matching the secondary coil inductance should be Ls  Li, where Li is the SQUID input inductance. The SQUID can be modeled as a current amplifier with an imprecision current noise, with spectral density SII, and a conjugate backaction voltage noise with spectral density SVV . Neglecting cross correlations, a rough approximation for the spectral densities is SII  2¯h/LiNh¯ and SVV  2¯hω2LiNh¯. This way a single noise parameter Nh¯ is singled out. The best available SQUIDs have shown effective noise Nh¯  10 which is not far from the Heisenberg limit Nh¯ = 1. Besides SQUID noise, one has to consider the Nyquist electrical thermal noise of the transformer. At kHz frequencies the best reported electrical quality factor is Q  106 [56]. This implies a voltage noise referred to the primary coil, SVV el = 4kBTgω0Lp/Q. Taking into account the transformer coupling, Nyquist noise is equivalent to a SQUID backaction noise with equivalent noise number N  h¯  2kBTg/(¯hω0Q). It happens that for Tg = 300 mK and Q  106 electrical noise is slightly larger than SQUID backaction noise at 1 kHz. Thermal noise is dominant for f0 < 1 kHz, and becomes negligible for f0  1 kHz. Having defined the parameters, we can now simulate the spectrum of a levitated nanoparticle coupled to a SQUID through a transformer. Figure 11 shows a spectrum based on the representative set of parameters discussed above. The spectrum shows that the coupling would be so low that both Nyquist noise and backaction noise are much smaller than the gas collision noise, even at a pressure Pg = 10−12 mbar. Assuming the particle to be thermalized, as there is no direct dissipation of energy, the force noise on resonance would allow one to test the CSL model down to λ < 10−13 Hz. The drawback is that because of low coupling the bandwidth would be extremely narrow,  f < 1 mHz. For the sake of comparison with the optical cavity and tweezer approaches, we construct a bandwidth vs detectable λ in analogy to Fig. 6. In contrast with the optical case, the backaction noise here is negligible and the position noise is fixed by the circuital parameters, so that there is no way to tune the measurement coupling (as changing power in the optical case). We vary the thermal force noise by varying the gas pressure P, and calculate the corresponding minimum detectable λ and bandwidth. In this way we obtain the plot in Fig. 12. Clearly, the striking feature of a SQUID-based detection of a nanoparticle in a Paul trap is the extremely narrow bandwidth. In principle such an experiment would not be unfeasible, as it would be possible to acquire tens of independent points in one day. However, this would require the trap frequency to be extremely stable within better than 1 mHz over such a time scale. This is not trivial for an actively controlled trap. In addition, note that the experimental parameters are already quite tight. Lower charge, higher electrode distance, or lower primary coil inductance will all have the effect of further reducing the bandwidth. Despite the very narrow bandwidth, a SQUID-based detection has the very attractive feature of being effective with any type, material, or size of the charged particle. It would then be possible to levitate much heavier particles, for instance, made of gold or other heavy elements such as platinum or osmium, which would strongly increase the coupling to CSL. For instance, with an osmium particle with R = 400 nm and the same other parameters of Fig. 11, one would reach a force noise of λ  3×10−16 Hz, close to the GRW limit. Unfortunately, in this case the measurement bandwidth would reach the extremely small value of  f  2μHz. It is fair to say that, besides the very long measurement time, we have completely ignored here the voltage noise in TABLE III. Main parameters considered for the SQUID readout with untuned superconducting transformer. Parameter Value q 30 e Q 106 Lp 10 H Nh¯ 10 ¯h the trap bias line. This is actually expected to become a big issue especially for large charge. To make this contribution negligible in the configurations discussed above, it would be necessary to suppress voltage noise in the bias line to SV < 10 pV/ √Hz. Furthermore, we have to consider another nontrivial technical issue. The SQUID electronics must be able to handle the signal due to crosstalk from the ac bias line, which will likely be huge. While this crosstalk can be in principle suppressed by proper cancellation schemes, this may be not so easy to do in practice. As for the optical detection methods, we list in Table III important parameters and summarize the key aspects and requirements of the SQUID readout. (1) There is no direct heating from the SQUID, thus the internal temperature can be very close to the bath temperature. (2) The coupling and measurement bandwidth (<1 mHz) are very low, setting very stringent requirements on the trap stability. (3) The coupling depends on the particle charge, which needs to be as large as possible. (4) A massive superconducting inductor, with high quality factor, is required. (5) The Paul trap bias couples a direct signal into the SQUID, which likely needs to be actively suppressed. 2. SQUID with a transformer and LC To further improve the impedance matching, one can tune a LC resonance to the resonance of the trapped particle. Note that a similar strategy at comparable frequency has been pursued in the readout of resonant bar gravitational wave detectors [55]. Besides the increased technical difficulty, it has been shown that this tuned LC system is really advantageous in terms of bandwidth only if the electrical quality factor is comparable or better than the mechanical quality factor. While this condition was met in the case of Ref. [55], it is definitely not valid in our case, as the largest electrical quality factors at kHz frequencies are of the order of 106, while the mechanical quality factor in the configurations here considered is expected to be of the order of 1011. To assess whether this intuitive argument is correct we have performed simulations of a circuit with the same parameters as in the transformer setup, but now the capacitor C shown in Fig. 10 in parallel to the primary inductor is used to tune the LC frequency of the circuit to the mechanical frequency. The simulation is performed by explicitly writing the Kirchoff equations and the coupled mechanical equation, using as independent variable the mechanical displacement x, the currents across capacitor and inductor IC and IL, and the current in the SQUID ISQ and including all relevant noise terms. The total output noise is then divided by the force to the SQUID current transfer function to compute the effective total force noise.   	We consider a nanoparticle levitated in a Paul trap in ultrahigh cryogenic vacuum, and look for the conditions which allow for a stringent noninterferometric test of spontaneous collapse models. In particular we compare different possible techniques to detect the particle motion. Key conditions which need to be achieved are extremely low residual pressure and the ability to detect the particle at ultralow power. We compare three different detection approaches based, respectively, on an optical cavity, an optical tweezer, and an electrical readout, and for each one we assess advantages, drawbacks, and technical challenges.
19	2019	Maximum advantage of quantum illumination	Shannon Ray, James Schneeloch, Christopher C. Tison, and Paul M. Alsing	Maximum advantage of quantum illumination  (Received 14 March 2019; published 17 July 2019) Discriminating between quantum states is a fundamental problem in quantum information protocols. The optimum approach saturates the Helstrom bound, which quantifies the unavoidable error probability of mistaking one state for another. Computing the error probability directly requires complete knowledge and diagonalization of the density matrices describing these states. Both of these fundamental requirements become impractically difficult to obtain as the dimensions of the states grow large. In this paper, we analyze quantum illumination as a quantum channel discrimination protocol and circumvent these issues by using the normalized Hilbert-Schmidt inner product as a measure of distinguishability. Using this measure, we show that the greatest advantage gained by quantum illumination over conventional illumination occurs when one uses a Bell state. DOI: 10.1103/PhysRevA.100.012327 I. INTRODUCTION One of the main limitations to sending classical information using quantum states is the receiver’s ability to distinguish the states carrying said information. If these states do not have orthogonal support, there is an unavoidable probability that the receiver will mistake one state for another; this creates error in the message. Therefore, it is necessary to have a measure that quantifies the probability of making an error, or a measure of distinguishability when analyzing which states are optimal for sending information. In 1969, Helstrom’s work [1] on the problem of discriminating between states 0 and 1 that are, respectively, sent with probabilities p0 and p1 established the Helstrom bound min {i} pE = 1 2 (1 − ||p00 − p11||1 ) (1) as the standard for quantifying the unavoidable error of mistaking one state for another. Indeed, Eq. (1) is the minimization of the error probability pE = p0 Tr[01] + p1 Tr[10] (2) with respect to a set of positive operator value measures {i  0, i = 0, 1} where 0 = 1ˆ − 1 and 1ˆ is defined as the identity operator. In Eq. (1), the trace norm || • ||1 is defined as ||ρ||1 ≡ Tr[ ρ†ρ] (3) where ρ is an arbitrary operator and ρ† is its Hermitian transpose. Because the Helstrom bound is the standard for quantifying unavoidable error, most quantum information protocols that have a distinguishing process need to compute the trace norm, which requires diagonalization in general. This can be difficult to work with when conducting an analysis especially as the dimension of the state becomes large. One such class of protocols that require diagonalization is quantum channel discrimination (QCD). The focus of this paper is on the optimization of a specific QCD protocol. In QCD, one sends an input state (in) through a quantum channel which performs one of two operations on the state given by {Ei, i = 0, 1}. They then receive the output state (out) i = Ei((in)) which is used to determine which operator acted on (in). Of course, some input states will work better than others depending on the distinguishability of (out) 0 and (out) 1 . Here, the probability of mistaking one operation for another is quantified by the Helstrom bound p E = min  where it is assumed that an optimal measurement scheme is used. In this context, QCD can be understood as the problem of finding the input state that minimizes Eq. (4) over the space of all (in). Moreover, extending the space of input states to higher dimension (including joint entangled states (in) q ) can further reduce the error probability [2,3]. If one partitions the joint system into a signal subsystem and an idler subsystem, where the signal subsystem is sent as a probe, and the idler system is held in a local memory, when the signal returns, a joint measurement can be made; this changes Eq. (4) to  where 1ˆI is the identity operator on the idler subsystem. In this paper, we analyze a postselected model of quantum illumination (QI) as a QCD protocol where Eq. (5) is minimized in the space of all (in) q . Recently, the fundamental limits of QCD were established [4]. This limit can be less than the minimum of Eq. (5) in the more general setting of adaptive discrimination protocols. In Lloyd’s seminal paper [5] on QI, the experimenter uses a biphoton d-mode Bell state to enhance the detection of a potential surface in a noisy background (see Fig. 1 for a diagram). Formulating the problem with the simplest possible mathematical treatment, Lloyd assumes that a single photon is detected per trial if anything is detected at all. This detection may be due to a returning signal or surrounding noise. In our treatment of QI as a QCD problem, we denote the scenario of receiving a mixture between signal and noise by the operation (E0 ⊗ 1ˆI )q, and the operation where the surface is not present and only noise is detected as (E1 ⊗ 1ˆI )q. Here, q is an arbitrary biphoton entangled state that is not necessarily a Bell state. To remove the restriction of single-photon detection, it was suggested that a full Gaussian-state analysis of QI should be conducted; such an analysis was completed by Tan et al. [6]. Using M copies of signal and idler beams obtained from continuous-wave spontaneous parametric down-conversion in the absence of pump depletion, they demonstrated an improvement in reducing the upper bound of the unavoidable error probability over a strictly coherent source. Our analysis is restricted to the single-photon discrete-variable setting where it is easier to develop arguments based solely on dimension and quality of entanglement without choosing a specific state. The relationship between our discrete-variable analysis to the continuous-variable setting will be a focus of future research. An example of QI in the more realistic continuous-variable setting is given in Ref. [7]. To avoid the problem of diagonalization when computing Eq. (5) for the analysis of QI, we use the Hilbert-Schmidt (HS) inner product, Tr[ρ†σ], to define a measure of distinguishability. Since the HS inner product only requires the trace of a matrix product to compute, it significantly reduces the difficulty of analysis. One of the main goals of this paper is to demonstrate the efficacy of the HS inner product as a tool for discrimination. Given that the HS inner product significantly simplifies our analysis of QI (as we shall show), it may yet be used to simplify the analysis of other quantum information protocols. This is a reasonable assumption since QI is a QCD protocol, and QCD is a fundamental root of quantum sensing [8]. The approach of using the HS inner product was used in [9] as a measure of fidelity between a Bell state and its teleported counterpart, and it was used in [10] to avoid the trace norm when quantifying the average distance between two states. Although the HS inner product satisfies Josza’s axioms [11] of a fidelity measure, it does not increase monotonically under general quantum operations [11,12]. This is important, where the action of a quantum channel on a pair of quantum states cannot increase their distinguishability (or decrease their fidelity). Fortunately, for the class of states considered in the model of QI considered here, we show that the normalized HS inner product is monotonic with respect to its parametrization. In this paper, we analyze a modified model of Lloyd’s original QI formulation. Not only do we seek the states (in) q that minimize Eq. (5) for this model, we also show that the d-dimensional Bell state, defined as a maximally entangled state with equal-dimension subsystems, gives the greatest advantage of QI over conventional illumination (CI). Conventional illumination uses the same input signal as the entangled case, but there are no idlers held to increase its effective brightness; the advantage is defined as the difference in distinguishability between signal and noise as given by QI versus CI. This paper is structured in the following way. In the next section, we present some background on QI and the mathematical framework used to conduct our analysis. After that, we introduce the HS distinguishability measure and show that it reduces the analysis of QI as a QCD protocol entirely in terms of dimensional arguments and the purity of the ancilla/idler subsystem. After that, we present the result that the d-dimensional Bell state gives the greatest advantage over CI for any other choice of (in) q . This agrees with the recent results of De Palma and Borregaard [13] where they used asymmetric hypothesis testing [14] to show that the twomode squeezed state gives the greatest advantage of QI. These results are consistent since the two-mode squeezed state is the continuous-variable analogue to the d-dimensional Bell state. Finally, we conclude with a discussion on the advantages of using a Hilbert-Schmidt based measure to address the problem of discrimination and its possible applications to quantum information protocols beyond QI. II. QUANTUM ILLUMINATION In this section, we describe the model used to analyze QI. To do this, we present the original formulation of QI by Lloyd [5]. Then we discuss our model, which is the postselected model used by Weedbrook et al. [15]. In Lloyd’s original formulation of QI, only single-photon events are considered. In this setting, the signal consists of a single photon in dS possible modes. It also assumes that the detector can distinguish between dS modes and its detection window is set to only detect a single photon per trial. In this formulation, Lloyd chooses q to be the d-mode Bell state, which is given by q = |φBellφBell| where |φBell = d−1/2 d k=1 |1k S|1k I. Here, |1k S is the state with exactly one photon in the signal mode k and no photons in the other dS − 1 modes. A similar description is given for |1k I. Next we describe how noise is modeled in this setting. When the signal is lost due to the target being absent, the remaining state is given by where λ is the average number of noise photons received over many trials, |vacS represents the vacuum where no photons are found in any of the signal modes, 1ˆ S/dS is the state representing the detection of a random mode from the surrounding noise, and I = TrS[q] is the idler subsystem held in local memory. If the surface is present, the returning signal is a mixture between signal and noise, which is given bywhere η, the average number of signal photons received over many trials, represents the degradation of the signal due to noise. In the paper by Weedbrook et al. [15], they use a simplified model of Lloyd’s original formulation that assumes the detector always receives a photon from either the signal or surrounding noise. They also complete their analysis using a Bell state, though they show that their result is independent of the state chosen in their Appendix. Their formulation of QI corresponds to a postselected model where λ = 1 in Eq. (6); this simplifies the remaining state to be Using this simplified model, they argue that quantum discord [16] explains the underlying advantage of QI. For computational clarity, we will also be working with this postselected model. In the next section, we use the normalized HS inner product to show that the advantage of this postselected model can be understood in terms of the purity of the idler subsystem given by Tr[2 I ]. In fact, a pure idler subsystem (i.e., Tr[2 I ] = 1) is necessarily uncorrelated from the signal, making the protocol using such states equivalent to CI. Any value of Tr[2 I ] < 1 implies an advantage gained by using a QI protocol. Where the minimum value of the purity for any density operator is d−1, when Tr[2 I ] = d−1 I , the maximum advantage has been gained; this is equivalent to minimizing Eq. (5). Unlike Weedbrook et al. and Lloyd, we do not assume q is the d-dimensional Bell state. Instead, we derive in Sec. IV that this state gives the greatest advantage for this model. III. HILBERT-SCHMIDT DISTINGUISHABILITY MEASURE Between two arbitrary quantum states ρ and σ, the normalized HS inner product is given by ρ,σ ≡ Tr[ρ†σ]  Tr[ρ†ρ] Tr[σ†σ] . (9) It has a lower extreme value of zero if and only if ρ and σ are states with orthogonal support [17]. It has an upper extreme value of unity if and only if ρ and σ are identical, and it is symmetric between them. The normalized HS inner product is invariant under unitary transformations, and it reduces to the ordinary inner product between quantum states when ρ and σ are pure. Moreover, we will show for the states ρ(0) from Eq. (7) and the remaining state ρ(1) from Eq. (8) that it is straightforwardly related to the physical parameters of QI. Now we will write Eq. (9) explicitly in terms of these physical parameters. To simplify Eq. (9) and write it in terms of the physical parameters of QI, we replace ρ and σ with ρ(0) and ρ(1), respectively. This is computed explicitly in the Appendix. Defining H01 as the normalized HS inner product between ρ(0) and ρ(1) to condense notation, our relations (from the Appendix) simplify H01 to H01 ≡ ρ(0), ρ(1) = 1  1 + η2(dSKI − 1) (10) where KI ≡ Tr[2 I ] −1 is the inverse of the purity of the idler state I. Here, the physical parameters that completely characterize QI for a fixed p0 are the relative signal fraction η, the dimension of the signal subsystem dS, and the entanglement between signal and idler which is captured by KI. Next, we want to show that both the minimum error probability p E and H01 are extremized simultaneously with respect to these variables so that we can use the distinguishability measure H01 to determine which states minimize the unavoidable error probability without diagonalization. To show that both H01 and p E are extremized simultaneously, we must show that they are both monotonic with respect to parameters η, dS, and KI. For a multivariate function, we take monotonicity to mean monotonic with respect to changes in each variable when all others are held constant. One can verify that H01 is strictly monotonic by taking the gradient of Eq. (10) and showing that each term maintains the same sign over the intervals η ∈ [0, 1], dS ∈ [2,∞], and KI ∈ [1, dI]. The interval for dS is justified if one assumes a qubit is the smallest signal used and one is allowed to use an arbitrary number of modes. From physical considerations, we can argue that p E monotonically decreases with increasing η, dS, and KI given the possible values of the parameters. Holding dS and KI fixed, it is clear that the error probability strictly decreases with increasing η since it parametrizes the degradation of the signal due to noise. As the signal becomes less noisy, it becomes easier to distinguish it from noise, thus decreasing the chance of error. Given that dS represents the possible modes the signal can be in as well as the number of modes distinguishable by the detector, increasing dS only increases the dimension that is used to distinguish the known signal from surrounding noise. Therefore, when η and KI are held fixed, increasing dS strictly decreases the probability of mistaking signal with noise. Alternatively, lower-dimensional signals form a subset of higher-dimensional signals, and expanding the set of states one is minimizing over cannot produce a worse result. As in [10], KI is the effective accessible dimension of the idler subsystem that expands the space of joint states obtainable through local manipulations of the signal subsystem (e.g., as in dense coding). Let d S ≡ dSKI represent the effective  dimension of the signal subsystem. From here, we see that d S ∈ [dS, d]. When d S = dS, one can only reduce H01 down to an amount limited by the dimension of the signal subsystem; this is equivalent to a CI protocol. When d S = d, one has access to the entire dimension of the idler subsystem to minimize H01. As KI increases, the accessible dimension of the signal increases, thus decreasing the probability of mistaking signal from noise. Where both H01 and p E decrease monotonically with respect to η, dS, and KI, we can reach the minimum H01 and p E along parametric curves of increasing dS, KI, and η. Along these trajectories, H01 is monotonic with respect to p E . Because of this, the set of values of η, dS, and KI that minimizes H01 also minimizes p E . Therefore, one only needs to consider H01 when seeking to minimize Eq. (5). Looking at Eq. (10), for a fixed η and composite dimension d, it is clear that the minimum possible value of H01 is taken when KI = dI. Therefore, the states that minimize Eq. (5) are those the idler subsystems of which have minimum purity (and therefore maximum entanglement with the signal). This is equivalent to illumination protocols the remaining states of which, ρ(1), are maximally mixed. Although all protocols for which KI = dI minimize the error probability for a fixed dimension d, one must maximize dI to maximize the advantage of QI. In the next section, we use the Schmidt decomposition to show that the d-dimensional Bell state is the only state that both has a remaining state that is maximally mixed and maximizes the idler dimension dI. IV. PROOF THE BELL STATE GIVES THE MAXIMUM ADVANTAGE In the previous section, we showed that the advantage of QI is quantified by KI, and when KI = dI one has gained the maximum advantage to distinguish ρ(0) from ρ(1) for fixed values of η and d. Therefore, if two states of equal dimension both have remaining states that are maximally mixed, they will have the same value of H01, but their advantages may be different. Under this circumstance, the QI protocol with the greater value of dI will have a greater advantage. Given an arbitrary entangled pure state q = |φφ|, its Schmidt decomposition is |φ =         rmin m=1  λm|smS|imI,          m λm = 1 (11) where rmin is the minimum rank between S and I, |smS and |imI are orthonormal eigenbasis vectors for the signal and idler subspaces, respectively, and √λm are the real nonnegative Schmidt coefficients. From here, we see that one must have dS > dI or dS = dI to get KI = dI. Otherwise, its greatest value is restricted by the rank of the signal subsystem. Assuming maximum idler rank KI = dI, and the circumstances dS > dI or dS = dI, the latter case achieves the largest possible effective signal dimension of d S = d2 S for a fixed signal, dS. Because the d-dimensional Bell state by definition is the only state with dS = dI and KI = dI, it gives the greatest advantage of QI over CI for any other choice of q. Thus, we have found that the d-dimensional Bell state minimizes the error probability in the case of postselection on the biphoton section of the composite Hilbert space. V. DISCUSSION In this paper, we treated QI as a QCD protocol to determine which states minimize the error probability and give the greatest advantage of QI. Most approaches that address this problem require some diagonalization process such as when computing the trace norm or relative entropy. To avoid this problem we used the normalized HS inner product as a measure of distinguishability, which only requires the trace of the matrix product between density operators. Using this HS distinguishability measure, we identified three parameters (η, dS,KI ) in the postselected model of QI that completely determine the distinguishability between ρ(0) and ρ(1). The most important of these parameters is KI = Tr[2 I ] −1 since it quantifies the advantage of QI over CI. When KI = dI, one gains the maximum advantage afforded by QI, and when KI = 1, S and I share zero entanglement, which is equivalent to using a CI protocol. Although our analysis was on QI, we believe that the HS inner product may have applications to other quantum information protocols. Similar analysis using the HS inner product may be possible for other protocols that use distributed entanglement among ancilla states to gain an advantage when sending or receiving information. It is our intention to extend this research by considering such applications.	Discriminating between quantum states is a fundamental problem in quantum information protocols. The optimum approach saturates the Helstrom bound, which quantifies the unavoidable error probability of mistaking one state for another. Computing the error probability directly requires complete knowledge and diagonalization of the density matrices describing these states. Both of these fundamental requirements become impractically difficult to obtain as the dimensions of the states grow large. In this paper, we analyze quantum illumination as a quantum channel discrimination protocol and circumvent these issues by using the normalized Hilbert-Schmidt inner product as a measure of distinguishability. Using this measure, we show that the greatest advantage gained by quantum illumination over conventional illumination occurs when one uses a Bell state.
20	2017	Regular patterns in the information flow of local dephasing channels	Filippo Giraldi	PHYSICAL REVIEW A 95, 022109 (2017) Regular patterns in the information flow of local dephasing channels  (Received 15 October 2016; revised manuscript received 12 January 2017; published 9 February 2017) Consider local dephasing processes of a qubit that interacts with a structured reservoir of frequency modes or a thermal bath, with Ohmic-like spectral density (SD). It is known that non-Markovian evolution appears uniquely above a temperature-dependent critical value of the Ohmicity parameter and non-Markovianity can be induced by properly engineering the external environment. In the same scenario, we find regular patterns in the flow of quantum information: Alternate directions appear in correspondence with periodic intervals of the Ohmicity parameter α0. The information flows back into the system over long times at zero temperature for 2 + 4n<α0 < 4 + 4n, where n = 0,1,2,..., and at nonvanishing temperatures for 3 + 4n<α0 < 5 + 4n. Under special conditions, backflow of information appears also for nonvanishing, even natural values of the Ohmicity parameter, at zero temperature, and for odd natural values at nonvanishing temperatures. Otherwise, the long-time information flows into the environment. In the transition from vanishing to arbitrary nonvanishing temperature, the long-time backflow of information is stable for 3 + 4n<α0 < 4 + 4n, while it is reversed for 2 + 4n<α0 < 3 + 4n and 4 + 4n<α0 < 5 + 4n. The patterns in the information flow are not altered if the low-frequency Ohmic-like profiles of the SDs are perturbed with additional factors that consist in arbitrary powers of logarithmic forms. Consequently, the flow of information can be controlled, directed, and reversed over long times by engineering a wide variety of reservoirs that includes and continuously departs from the Ohmic-like structure at low frequencies. Non-Markovianity and recoherence appear according to the same rules along with the backflow of information. DOI: 10.1103/PhysRevA.95.022109 I. INTRODUCTION In open quantum systems the loss, revival, or maintenance of quantum correlations is deeply related to the structure of the external environment [1,2]. Several studies have been performed on the connection among non-Markovian dynamics, flow of quantum information, and the nature of the coupling between the system and the environment [3–7]. Non-Markovianity was usually interpreted via memory effects and persistent interactions between system and environment. In recent years new definitions and measures of non-Markovianity have been proposed; see Refs. [8,9] for a review. Non-Markovianity can be explained in terms of the flow of quantum information, which is defined in various ways: via Fisher information [10], fidelity [11] or mutual information [12], to name a few. The trace-distance measure introduced in Ref. [13] estimates the relative distinguishability of two arbitrary quantum states. In Markovian processes this measure diminishes monotonically in time. This behavior can be seen as a loss of quantum information by the open system, while in non-Markovian dynamics the memory effects can be interpreted as a flow of quantum information from the external environment back into the open system. The dephasing process of a qubit (two-level system) that interacts with a structured reservoir of frequency modes *giraldi.filippo@gmail.com; giraldi@ukzn.ac.za is a referential scenario for the study of phenomena such as dissipation, decoherence, recoherence, non-Markovianity, and information flow [1,2,14–17]. The open dynamics can be described in terms of the spectral density (SD) of the system. This fundamental function depends on the couplings between the open system and the frequency modes of the external environment [14,18–21]. This formalism is adopted also for the description of the open dynamics in fermionic environments [22–25]. For the system of a qubit that interacts with a bosonic reservoir the measures of non-Markovianity mentioned above suggest the same conditions for the appearance of non-Markovian dynamics [26] and are easily evaluated from the dephasing rate and the dephasing factor of the system [3–6,27]. Persistent negative values of the dephasing rate or, equivalently, a decreasing dephasing factor indicate backflow of information into the open system and witness non-Markovianity. A remarkable analysis of the dependence of nonMarkovianity on the low-frequency part of the environmental spectrum was performed in Ref. [3]. Nonconvexity properties that involve the SD provide conditions for the appearance of non-Markovian dynamics. For Ohmic-like SDs with exponential cutoff, the transition from Markovian to non-Markovian dynamics is found in correspondence with a critical value of the Ohmicity parameter. The temporary backflow of information and recoherence manifest uniquely for values of the Ohmicity parameters that are larger than such a critical value. This value is equal to 2 at zero temperature, grows monotonically by increasing the temperature, and becomes 3 at infinite 2469-9926/2017/95(2)/022109(13) 022109-1 ©2017 American Physical Society FILIPPO GIRALDI PHYSICAL REVIEW A 95, 022109 (2017) temperature. See Ref. [3] for details. Great efforts have been made for the experimental observation of these phenomena: Simulations of open system dynamics have been performed with trapped ions [28] and transitions from Markovian to non-Markovian dynamics have been obtained in an all-optical experiments [29], to name a few. As a continuation of the scenario described above, here we consider the local dephasing process of a qubit that interacts either with a structured reservoir of frequency modes or with a thermal bath. In addition to the Ohmic-like condition, the SDs under study include removable logarithmic singularities at low frequencies [30,31] and are arbitrarily shaped at higher frequencies. We study the decoherence and recoherence processes by evaluating the dephasing factor and we investigate the flow of quantum information by analyzing the dephasing rate. We also search for regular patterns in the direction of the flow of information that allow a full manipulation of the flow itself by engineering the low-frequency structure of the external environment. The paper is organized as follows. Section II is devoted to the description of the model. In Sec. III, asymptotic coherence is related to integral properties of the SD. In Sec. IV, the SDs under study are defined in terms of removable logarithmic singularities. The decoherence and recoherence processes are studied in Sec. V, by analyzing the dephasing factor at both zero and nonvanishing temperature. Patterns in the flow of quantum information are shown in Sec. VI. A summary is given in Sec. VII. Details on the calculations are provided in the Appendix. II. MODEL The system of a qubit that interacts locally with a reservoir of frequency modes is described by the microscopic Hamiltonian [1–4] in units where ¯h = 1. The transition frequency of the qubit is ω0, while σz represents the z-component Pauli spin operator. The index k runs over the frequency modes. The parameter ωk represents the frequency of the kth mode, while b† k and bk are the raising and lowering operator, respectively, of the same mode. The coefficient gk represents the coupling strength between the qubit and the kth frequency mode. The reduced density matrix ρ(t) represents the mixed state of the qubit at the time t and is obtained by tracing the density matrix of the whole system at the time t over the Hilbert space of the external environment [1]. The model is exactly solvable [15–17]. Let the qubit be initially decoupled from the external environment that is represented by a structured reservoir of field modes or by a thermal bath. The reduced time evolution is described in the interaction picture by the master equation The function γ (t) represents the dephasing rate and is related to the temperature of the thermal bath. At zero temperature, The function J (ω) represents the SD of the system and is defined in terms of the coupling constants gk via the form J (ω) =  k |gk| 2δ(ω − ωk). If the external environment is initially in a thermal state, T > 0, the dephasing rate is represented here as γT (t) and reads where the effective SD JT (ω) is defined for every nonvanishing temperature as and kB is the Boltzmann constant. The quantum coherence between the states |0 and |1 of the qubit is described by the off-diagonal element ρ0,1(t) of the density matrix that undergoes the evolution [15–17] The function (t) represents the dephasing factor and depends on the temperature T of the thermal bath and on the coupling between the system and the environment. At zero temperature, T = 0, the dephasing factor is indicated here as 0(t) and results in the form If the external environment is initially in a thermal state, T > 0, the dephasing factor is represented here as T (t) and reads Both for vanishing and nonvanishing temperature, the dephasing factor is related to the dephasing rate via the time derivative γ0(t) = ˙ 0(t) and γT (t) = ˙ T (t). According to Eq. (6), recoherence corresponds to negative values of the dephasing rate.III. COHERENCE The loss or persistence of coherence between the two energy eigenstates of the qubit depends on integral properties of the SDs. At zero temperature, T = 0, coherence is not entirely lost over long times if the second negative moment of the SD is finite,  This quantity is also referred to as the total Huang-Rys factor in the framework of optical spectroscopy [19–21] and is relevant for the appearance of coherence. In fact, under the condition (9) the coherence term shows long-time persistence of residual coherence If the external environment consists in a thermal bath, T > 0, and the conditionä holds, the coherence term tends over long times to the nonvanishing asymptotic valueThe maximum modulus of the ratio between asymptotic and initial coherence is obtained at zero temperature, T = 0, from Eq. (10). Residual coherence persists over long times if the dephasing factor does not diverge asymptotically, while coherence is fully lost if the dephasing factor diverges [see Eq. (6)]. Consequently, the dependence of coherence on the structure of the SD can be analyzed via the dephasing factor itself. We intend to study the short- and long-time behavior of the dephasing factor for a large variety of SDs in conditions where the second negative moment is either finite, Eq. (9), or infinite, Similarly, at nonvanishing temperatures, T > 0, we consider both the condition (11) and the following one: IV. SPECTRAL DENSITIES WITH REMOVABLE LOGARITHMIC SINGULARITIES The fast development of quantum technologies allows the engineering of the most various environments. According to the remarkable analysis performed in Refs. [32,33], an impurity that is trapped in a double-well potential and is surrounded by a cold gas reproduces, under suitable conditions, a qubit that interacts with an Ohmic-like environment. The Ohmicity parameter increases by enhancing the scattering length that is related to the boson-boson coupling [33]. In the case where the gas is free and one dimensional, the SD changes from sub-Ohmic to Ohmic and to super-Ohmic by increasing the scattering length. In the two-dimensional noninteracting condition the spectrum is Ohmic and the super-Ohmic regime is obtained if the magnitude of the interaction decreases. The SD is super-Ohmic in the noninteracting condition if the gas is three dimensional. We refer to [33] for details. In light of the above observation we focus on SDs that include the Ohmic-like condition at low frequencies and are arbitrarily shaped at higher frequency. We intend to analyze the feature of the open dynamics, the flow of quantum information, non-Markovianity, and recoherence of the qubit. We evaluate the accuracy of the results obtained for the experimentally feasible Ohmic-like SDs by perturbing the power laws of the Ohmic-like profiles with additional factors that are represented by arbitrary powers of logarithmic forms. Positive (negative) values of the first logarithmic power enhance (reduce) the power-law profiles. In this way, we consider a wide variety of SDs that cover and continuously depart from the Ohmic-like condition [30]. For the sake of convenience, the SDs J (ω) are described via the dimensionless auxiliary function         (ν). This function is defined for every ν  0 by the scaling property J (ν)/ =         (ν) in terms of a general scale frequency  of the system. At nonvanishing temperatures the auxiliary function         T (ν) of the effective SD JT (ω) is         T (ν) =         (ν) coth(¯hν/2kBT ). In this way, the action of the thermal bath is represented by a transformed SD. We consider two general classes of SDs, which are defined below. A. Spectral densities with natural powers of logarithmic forms The first class of SDs under study is defined by auxiliary functions         (ν) that are continuous for every ν > 0 and exhibit the following asymptotic behavior [34] as ν → 0+:         (ν) ∼ ∞ j=0 nj k=0 cj,kναj (− ln ν) k , (15) where 0  nj < ∞, αj < αj+1 for every j  0, and αj ↑ +∞ as j → +∞. Furthermore, we consider α0  0, and n0 = 0 if α0 = 0. The power α0 is referred to as the Ohmicity parameter [2,14]. In fact, if n0 = 0, the corresponding SDs are super-Ohmic for α0 > 1, Ohmic for α0 = 1, and subOhmic for 1 > α0 > 0, as ω → 0+. The singularity in ν = 0 is removable by defining         (0) as the finite limit of         (ν) as ν → 0+. Notice that Eq. (15) describes a large variety of low-frequency asymptotic forms that include exponential and stretched exponential functions, power laws, and natural powers of logarithmic forms. The summability of the SD is guaranteed by the constraint         (ν) = O(ν−1−χ0 ) as ν → +∞, where χ0 > 0. Additionally, the Mellin transforms         ˆ (s) and         ˆ T (s) of the auxiliary functions         (ν) and         T (ν), and the meromorphic continuations [34,35] are required to decay sufficiently fast as | Im s| → +∞. See the Appendix for details. B. Spectral densities with arbitrary powers of logarithmic forms In light of the asymptotic analysis performed in Refs. [35,36], the second class of SDs under study is described by auxiliary functions with the following asymptotic expansion as ν → 0+:         (ν) ∼ ∞ j=0 wj ναj (− ln ν) βj . (16) The powers βj are real valued, arbitrarily positive or negative, or vanishing, while α0 > 0. The logarithmic singularity in ν = 0 is removed by setting         (0) = 0. Let the parameter n¯ be the least natural number such that αk−1 + 1  n<α ¯ k + 1, where the index k is a nonvanishing natural number. The function         (n¯) (ν) is required to be continuous on the interval (0,∞). The integral  ∞ 0         (ν) exp[−ıξν]dν must converge uniformly for all sufficiently large values of the variable ξ and the integral          (n¯) (ν) exp[−ıξν]dν has to converge at ν = +∞ uniformly for all sufficiently large values of the variable ξ . The auxiliary function is required to be differentiable k times and the asymptotic expansion at ν → 0+, is required to hold for every k = 0,1,...,n¯. Furthermore, for every k = 0,...,n¯ − 1, the function         (k) (ν) has to vanish as ν → +∞. We refer to [35,36] for details. If compared to the first class of SDs, introduced in Sec.IV A, the second class is characterized by more constraints but includes arbitrary positive or negative, or vanishing, real-valued powers of logarithmic forms. In both the classes under study the auxiliary functions         (ν) are non-negative and summable, due to physical grounds, and, apart from the constraints reported above, arbitrarily shaped at high frequencies [30]. V. DEPHASING FACTOR We start the analysis of the dephasing factor by considering a structured reservoir of frequency modes as the external environment. The SDs under study belong to the first class (Sec. IV A). Over short times, t  1/, the dephasing factor increases quadratically in time 0(t) ∼ l0 2 t 2 , (17) where l0 =  ∞ 0 J (ω)dω. This behavior is independent of the low- or high-frequency structure of the SD. Instead, the evolution of the dephasing factor over long times, t          1/, is various and is determined by the low-frequency structure of the SD, given by Eq. (15). If α0 = n0 = 0 the dephasing factor grows linearly in time for t          1/, If α0 is an even natural number and n0 vanishes, consider the least nonvanishing index k0 such that either αk0 does not take even natural values or αk0 = 2mk0 , where the natural numbers mk0 and nk0 do not vanish. The function 0(t) is obtained in the former case from Eqs. (23) and (24) by replacing the power α0 with αk0 and n0 with nk0 , or in the latter case from Eqs. (25) and (26) by replacing the power m0 with mk0 and n0 with nk0 . We consider SDs such that the index k0 exists with the required properties. A. The dephasing factor at zero temperature for the second class of spectral densities At this stage we focus on SDs that belong to the second class (Sec. IV B) and that are characterized by a finite negative second moment [Eq. (9)]. This condition requires that the Ohmicity parameter α0 is larger than unity, α0 > 1. Over short times, t  1/, the dephasing factor grows quadratically in time according to Eq. (17), independently of the low- or high-frequency structure of the SD. Over long times, t          1/, the dephasing factor relaxes to the asymptotic value 0(∞) according to arbitrarily positive or negative, or vanishing powers of logarithmic forms C. The dephasing factor at nonvanishing temperatures for the second class of spectral densities We consider SDs such that the auxiliary functions         T (ν) belong to the second class under study (Sec. IV B) and exhibit a finite second negative moment [Eq. (11)]. This constraint requires α0 > 2. Over short times, t  1/, the dephasing factor grows quadratically in time according to Eq. (32), independently of the low- or high-frequency structure of the SD. As far as the long-time evolution is concerned, a variety of logarithmic relaxations of the dephasing factor to the asymptotic value T (∞) are obtained for t          1/, The comparison between the above relaxations and those obtained for the first class of SDs suggests that, at nonvanishing temperatures, the long-time evolution of the dephasing factor exhibits for both classes under study the same dependence on the low-frequency structure of the SD. Numerical computations have been performed by adopting the toy SDs where (ν) represents the Heaviside step function. The first example of SDs [Eq. (47)] is tailored at low frequencies by a quadratic logarithmic term and power laws, exhibits an exponential cutoff at high frequencies, and belongs to the first class under study (Sec. IV A). The second example of SDs [Eq. (48)] is tailored at low frequencies by arbitrary powers of logarithmic forms and power laws, with finite support [0,/2], and belongs to the second class under study (Sec. IV B). Plots of the coherence term are drawn in Fig. 1. The persistence of asymptotic coherence is confirmed by the long-time nonvanishing behavior of the curves. Numerical analysis of the dephasing factor are displayed in Figs. 2–4. The short-time quadratic growth is confirmed by the parallel asymptotic lines, with slope 2, appearing in Fig. 2. The VI. REGULAR PATTERNS IN THE LONG-TIME INFORMATION FLOW For the system under study the trace distance measure of non-Markovianity that is defined in Refs. [4,13] takes a simple expression in terms of the dephasing rate and dephasing factor. In this case the non-Markovianity measure results in the The open dynamics is Markovian if the dephasing rate is non-negative. On the contrary, persistent negative values of the dephasing rate are a source of non-Markovianity and are interpreted as a flow of information from the environment back into the system. Consequently, at zero temperature, T = 0, the open dynamics is Markovian if the function J (ω)/ω is nonincreasing. If the SD is differentiable this condition reads for every ω > 0. Consequently, if the open dynamics is nonMarkovian, the function J (ω)/ω, for T = 0, or the function JT (ω)/ω, for T > 0, is increasing in an interval of frequencies of nonvanishing measure at least. In this way, a nonvanishing contribution is provided to the integral of Eq. (49). Let the SD be differentiable for every ω > 0. If the open dynamics is non-Markovian the constraint (50), for T = 0, or (51), for T > 0, is not fulfilled for one value of the frequency at least. In general, the asymptotic behavior of the dephasing rate depends on integral properties of the SDs. Over long times, t          1/, the dephasing rate vanishes at zero temperature, T = 0, if the first negative moment of the SD is finite, If α0 is an even natural number and n0 vanishes, consider the least nonvanishing index k2 such that either αk2 does not take even natural values or αk2 = 2mk2 , where the natural numbers mk2 and nk2 do not vanish. The function γ0(t) is obtained in the former case from Eqs. (56) and (57) by replacing the power α0 with αk2 and n0 with nk2 , or in the latter case from Eqs. (58) and (59) by replacing the power m2 with mk2 and n0 with nk2 . We consider SDs such that the index k2 exists with the required properties. A. The dephasing rate at zero temperature for the second class of spectral densities At this stage we analyze the time evolution of the dephasing rate at zero temperature by considering the second class of SDs under study (Sec. IV B). Over short times, t  1/, the dephasing rate increases linearly according to Eq. (54). This behavior is independent of the low- or high-frequency structure of the SD. Over long times, t          1/, we find various forms of logarithmic relaxations According to the above analysis, at zero temperature and for the first class of SDs under study, the information flows into the environment over short times, t  1/. Over long times, t          1/, the information flows back into the system for the values of the Ohmicity parameter 2 + 4n<α0 < 4 + 4n, where n = 0,1,2,.... Backflow of information is obtained also for every nonvanishing even natural value of the Ohmicity parameter if n0 = 0 and 2 + 4n<αk2  4 + 4n, where n takes natural values. Additionally, if n0 > 0, long-time backflow appears for every even natural value α0 = 4l1, where l1 is a nonvanishing natural number. Along with the backflow of information, the long-time dynamics is non-Markovian, the modulus of the coherence term increases up to the nonvanishing asymptotic value, and recoherence is observed. If the Ohmicity parameter differs from the values reported above, the long-time information flows into the environment, the long-time dynamics is Markovian, and the modulus of the coherence term decreases down to the asymptotic value, as suggested by the long-time behavior of the dephasing factor. If compared to the initial condition, coherence is partially lost for α0 > 1 and is fully lost if 0  α0  1. For the second class of SDs under study, at zero temperature, the information backflow, non-Markovianity, and recoherence exhibit, over long times, exactly the same dependence on the low-frequency structure of the SD as the one found for the first class. Notice that in the whole paper the analysis concerns uniquely the short- and long-time flow of information. Consequently, the dynamics can still be non-Markovian due to intermediate backflows, even if no information flows from the environment back into the system over long times. B. Thermal bath Let the external environment be a thermal bath, T > 0. For SDs that belong to the first class under study (Sec. IV A) and α0 > 0 the dephasing rate increases linearly over short times, t  1/, γT (t) ∼ lT t. (65) This behavior is independent of the low- or high-frequency structure of the SD. Over long times, the dephasing rate divergences or vanishes dependent on the low-frequency profile of the SD that is given by Eq. (15). If 0 < α0 < 1 the dephasing rate diverges for t          1/ according to γT (t) ∼ c0,n0gT (t) 1−α0 lnn0 (t), (66) which describes power laws for n = 0, γT (t) ∼ c0,n0gT (t) 1−α0 . (67) The coefficient gT reads gT = 2kBT cos(πα0/2)(α0) h¯(1 − α0) . If α0 = 1 the dephasing rate diverges for t          1/ as γT (t) ∼ c0,n0πkBT h¯ lnn0 (t). (68) If α0 = 1 and n0 = 0 the dephasing rate converges for t          1/ to the nonvanishing value γT (t) ∼ c0,n0πkBT h¯ . (69) If α0 > 1 and α0 is not an odd natural number, the dephasing factor vanishes for t          1/ according to Eq. (66). If α0 = 1 + 2m3, where m3 and n0 are nonvanishing natural numbers, the dephasing rate vanishes for t          1/ as γT (t) ∼ c0,n0g  T (t) −2m3 lnn0−1 (t), (70) where g  T = π(−1)1+m3 kBT n0(2m3 − 1)!/h¯. The above relaxation provides inverse power laws for n0 = 1, γ0(t) ∼ c0,n0g  T (t) −2m3 . (71) If α0 is an odd natural number and n0 vanishes, consider the least nonvanishing index k3 such that either αk3 does not take odd natural values or αk3 = 1 + 2mk3 , where the natural numbers mk3 and nk3 do not vanish. The function γT (t) is obtained in the former case from Eqs. (66) and (67) by replacing the power α0 with αk3 and n0 with nk3 , or in the latter case from Eqs. (70) and (71) by substituting the power m3 with mk3 and n0 with nk3 . We consider SDs such that the index k3 exists with the required properties. C. The dephasing rate at nonvanishing temperatures for the second class of spectral densities Let the external environment consist of a thermal bath, T > 0, and the auxiliary functions         T (ν) belong to the second class under study (Sec. IV B). Over short times, t  1/, the dephasing rate increases linearly according to Eq. (65), if α0 > 0. Except for this requirement, the behavior is independent of the low- or high-frequency structure of the SD. Over long times, t          1/, the dephasing rate vanishes according to arbitrary powers of logarithmic forms The long-time logarithmic relaxations result in the asymptotic lines of Figs. 7 and 8. The former refer to the quadratic logarithmic term of the first computed SDs (47). The latter correspond to various noninteger logarithmic powers of the second computed SDs (48). The above results show that at nonvanishing temperatures and for the first class of SDs under study, the information flows into the environment over short times, t  1/. Over long times, t          1/, the information flows back into the system for the values of the Ohmicity parameter 3 + 4n<α0 < 5 + 4n, where n = 0,1,2,.... Backflow of information is obtained also for every odd natural value of the Ohmicity parameter if n0 = 0 and 3 + 4n<αk3  5 + 4n, where n takes natural values. Additionally, if n0 > 0, long-time backflow appears for every odd natural value α0 = 1 + 4l2, where l2 is a nonvanishing natural number. If the Ohmicity parameter differs from the values reported above, the long-time information spreads into the environment under Markovian evolution and the modulus of the coherence term decreases down to the asymptotic value. If compared to the initial condition, coherence is partially lost if α0 > 2. Coherence is fully lost if 0 < α0  2. By considering the second class of SDs, the information backflow, non-Markovianity, decoherence, and recoherence exhibit over long times, at nonvanishing temperatures, exactly the same dependence on the low-frequency structure of the SD and on the temperature as the one found for the first class. Consider the transition from vanishing to an arbitrary nonvanishing temperature. For both classes of the SDs under study, the backflow of information is stable for 3 + 4n<α0 < 4 + 4n. Additionally, the backflow is stable for the values 3 + 4n under the special conditions mentioned above. The backflow is inverted for 2 + 4n<α0 < 3 + 4n and 4 + 4n < α0 < 5 + 4n, where n = 0,1,2,.... In the same transition, the long-time recoherence process is unaffected in the former conditions and is destroyed in the latter ones. VII. CONCLUSION We have considered the local dephasing process of a qubit that interacts with a structured reservoir of frequency modes or a thermal bath. We have studied the coherence between the two energy eigenstates of the qubit and the flow of quantum information by analyzing the dephasing factor and dephasing rate over short and long times. The SDs under study are obtained by introducing in the low-frequency Ohmic-like structure additional factors that are represented by powers of logarithmic forms J (ω)∝ ∼(ω/) α0 [− ln(ω/)]β0 for ω  . For the first class of SDs, the Ohmicity parameter α0 takes arbitrarily non-negative real values, while the logarithmic power β0 is natural valued. For the second class, the Ohmicity parameter takes arbitrarily positive real values, while the logarithmic power is real valued, arbitrarily positive or negative, or vanishing. The logarithmic singularities are removable and enhance, for positive logarithmic powers, or reduce, for negative logarithmic powers, the low-frequency power-law profiles of the physically feasible Ohmic-like SDs. Over higher frequencies the SDs are arbitrarily tailored. The full loss or persistence of coherence, over long times, is determined by integral properties of the SD and corresponds to a divergent or a convergent dephasing factor, respectively. For both classes of SDs under study the dephasing factor increases quadratically and the dephasing rate grows linearly over short times, both at zero and at an arbitrary nonvanishing temperature. Over long times, the dephasing factor and the dephasing rate exhibit various relaxations to the asymptotic values that are described by logarithmic and power laws. The dependence on the low-frequency structure of the SD is the same for both classes of SDs. For the second class, the relaxations are arbitrarily faster or slower, or coincide with inverse power laws, due to the arbitrariness of the logarithmic powers. The information flows into the environment over short times, at both vanishing and nonvanishing temperature. Over long times, we have found that regular patterns appear in the direction of the flow of information, back into the system or forth into the environment, dependent on the Ohmicity parameter α0 of the SD, regardless of the logarithmic form factors. At zero temperature, the long-time information flows from the environment back into the system in correspondence with the periodic intervals 2 + 4n<α0 < 4 + 4n for every n = 0,1,2,.... Under special conditions, backflow of information appears at zero temperature also for nonvanishing even natural values of the Ohmicity parameter. At nonvanishing temperatures, backflow of information is obtained over the periodic intervals 3 + 4n<α0 < 5 + 4n. Under special conditions, backflow of information is found at nonvanishing temperatures also for odd natural values of the Ohmicity parameter. In the transition from vanishing to an arbitrary nonvanishing temperature, the backflow of information stably persists over the intervals 3 + 4n<α0 < 4 + 4n and, under the special conditions mentioned above, for the values α0 = 3 + 4n. Instead, the backflow is inverted over the intervals 2 + 4n < α0 < 3 + 4n and 4 + 4n<α0 < 5 + 4n. Non-Markovianity and recoherence of the qubit appear along with the backflow of information. Consequently, the transition from vanishing to an arbitrary nonvanishing temperature does not destroy the recoherence process for 3 + 4n<α0 < 4 + 4n and, under special conditions, for the values α0 = 3 + 4n. Argumentation on the experimental setting is beyond the purposes of the present paper. Still, it is worth mentioning that the present results apply to the Ohmic-like SDs of trapped impurity atoms that are immersed in a Bose-Einstein condensate environment. Furthermore, if the low-frequency power-law profiles of the Ohmic-like SDs are enhanced or reduced via arbitrary positive or negative powers of logarithmic form factors, the direction of the information flow is not altered by the logarithmic terms and depends uniquely on the Ohmicity parameter of the Ohmic-like term of the SD. Consequently, the patterns in the information flow remain stable with respect to the mentioned logarithmic perturbations of the Ohmic-like SDs. We believe that the present analysis provides further scenarios for the implementation of a stable control of the flow of quantum information and the appearance of nonMarkovian dynamics and recoherence via the engineering reservoir approach.	Consider local dephasing processes of a qubit that interacts with a structured reservoir of frequency modes or a thermal bath, with Ohmic-like spectral density (SD). It is known that non-Markovian evolution appears uniquely above a temperature-dependent critical value of the Ohmicity parameter and non-Markovianity can be induced by properly engineering the external environment. In the same scenario, we find regular patterns in the flow of quantum information: Alternate directions appear in correspondence with periodic intervals of the Ohmicity parameter α0. The information flows back into the system over long times at zero temperature for 2 + 4n<α0 < 4 + 4n, where n = 0,1,2,..., and at nonvanishing temperatures for 3 + 4n<α0 < 5 + 4n. Under special conditions, backflow of information appears also for nonvanishing, even natural values of the Ohmicity parameter, at zero temperature, and for odd natural values at nonvanishing temperatures. Otherwise, the long-time information flows into the environment. In the transition from vanishing to arbitrary nonvanishing temperature, the long-time backflow of information is stable for 3 + 4n<α0 < 4 + 4n, while it is reversed for 2 + 4n<α0 < 3 + 4n and 4 + 4n<α0 < 5 + 4n. The patterns in the information flow are not altered if the low-frequency Ohmic-like profiles of the SDs are perturbed with additional factors that consist in arbitrary powers of logarithmic forms. Consequently, the flow of information can be controlled, directed, and reversed ov
21	2017	Secret information reconciliation based on punctured low-density parity-check codes for continuous-variable quantum key distribution	Xue-Qin Jiang,1 Peng Huang,2, Duan Huang, Dakai Lin,2 and Guihua Zeng	Secret information reconciliation based on punctured low-density parity-check codes for continuous-variable quantum key distribution  (Received 30 August 2016; published 13 February 2017) Achieving information theoretic security with practical complexity is of great interest to continuous-variable quantum key distribution in the postprocessing procedure. In this paper, we propose a reconciliation scheme based on the punctured low-density parity-check (LDPC) codes. Compared to the well-known multidimensional reconciliation scheme, the present scheme has lower time complexity. Especially when the chosen punctured LDPC code achieves the Shannon capacity, the proposed reconciliation scheme can remove the information that has been leaked to an eavesdropper in the quantum transmission phase. Therefore, there is no information leaked to the eavesdropper after the reconciliation stage. This indicates that the privacy amplification algorithm of the postprocessing procedure is no more needed after the reconciliation process. These features lead to a higher secret key rate, optimal performance, and availability for the involved quantum key distribution scheme. DOI: 10.1103/PhysRevA.95.022318 I. INTRODUCTION Quantum key distribution (QKD) enables two legitimate communicators, commonly called Alice and Bob, to exchange a cryptographic key by encoding information on photons and makes it impossible for Eve to attack without being detected [1–4]. Its security is guaranteed by the laws of quantum physics. The well-known continuous-variable QKD (CV-QKD) is an important implementation way [5–10]. In most of the CV-QKD schemes, the secret information is encoded on the quadratures X and P of continuous-variable quantum states, and homodyne or heterodyne detectors are utilized to replace the single-photon detectors which are widely employed in the discrete-variable QKD. So far, many CV-QKD schemes have been proposed [11–14]. The most favorable scheme is the Gaussian-modulated coherent state (GMCS) QKD [11], which has been proven theoretically secure against collective attacks [15,16] and coherent attacks [17,18]. Generally, a CV-QKD scheme involves two essential phases, i.e., the quantum transmission procedure and the postprocessing procedure. In the former, Alice and Bob create advantages over Eve via the quantum transmission and parameter estimation, and then a raw key is generated. The postprocessing procedure is employed to distill a final secret key from the obtained raw key. We note that the postprocessing procedure is associated with the final secret key rate and the secure transmission distance, both of which are important parameters for a practical CV-QKD system. Generally, the postprocessing procedure could be described as two stages, and that includes the reconciliation and the privacy amplification. The reconciliation helps to correct errors between the secret strings held by Alice and Bob; *huang.peng@sjtu.edu.cn † duan.huang@foxmail.com ‡ ghzeng@sjtu.edu.cn then legitimate users would have the same string. To achieve the information theoretic security, a privacy amplification algorithm is employed to eliminate Eve’s information by distilling the string in a secure way. Of course, the involved privacy amplification algorithm may also add time complexity. The reconciliation for the CV-QKD scheme has attracted much attention. Initially, a slice reconciliation scheme with low decoding complexity has been proposed [19], and the decoding complexity is further reduced in Ref. [20]. Then this scheme was improved by using a binary low-density paritycheck (LDPC) and polar codes [21,22]. Another important method is associated with the postselection technique [23,24]. Unfortunately, its security against general collective attacks has not yet been proved, and it breaks the symmetry of phase space [25]. Recently, an available approach called multidimensional reconciliation was proposed for the CV-QKD with an additive white Gaussian noise (AWGN) channel [26], and it has been applied in several works with binary LDPC code [7,27,28]. To pursue a higher reconciliation speed, two improved multidimensional reconciliation schemes were proposed, and the decoding speed reached 10 [22] and 25 Mb/s [29] based on polar codes and LDPC codes, respectively. In this paper, we propose a reconciliation scheme for the CV-QKD system based on the punctured LDPC codes. Theoretically, no information leaks to the eavesdropper after the reconciliation stage. Subsequently, the privacy amplification that is often viewed as an important element in previous schemes is not needed in our scheme, while the unconditional security could be still guaranteed. In addition, time complexity of the postprocessing procedure with our scheme is lower than the previous approaches, and the secret key rate and availability of the CV-QKD scheme could also be improved. This paper is arranged as follows. In Sec. II, an unconditionally secure reconciliation scheme based on the punctured LDPC codes is proposed. Then the security of the proposed scheme is investigated in Sec. III. In Sec. IV, we study the II. RECONCILIATION SCHEME BASED ON PUNCTURED LDPC CODES The involved quantum channel for the CV-QKD is a normal linear model with the following relation between Alice and Bob, i.e., y = tx + z, where t = √ηlossT , where ηloss is the detection efficiency and T is the transmission efficiency, and z is the noise term following a centered normal distribution with a unknown variance, i.e., σ2 = N0 + ηlossT ε + Vel. After the quantum transmission through the quantum channel, Alice and Bob share two correlated vectors x = {x1,x2, . . . ,xn} and y = {y1,y2, . . . ,yn}, where n is the total number of received pulses. After obtaining the correlated vectors x and y, legitimate users will enter the reconciliation procedure. Without loss of generality, we consider the reverse reconciliation. The schematic of the proposed scheme is plotted in Fig. 1. In the proposed scheme, a binary string M is chosen randomly as the secret key which will be encoded into a linear error-correcting code, e.g., a LDPC code U in Bob’s side. And then the code U is punctured to be a code U according to the generation way of the punctured LDPC codes. The generated code U and the correlated vector y are combined to generate αk(y,U ) which is sent to Alice by Bob, where αk(y,U ) will be described in detail in the following. With αk(y,U ) and the correlated vector x, Alice obtains a secret key M¯ that satisfies the following constraints, i.e., the reliability constraint Prob{M = M¯ } → 0, (1) and the security constraint I (M : Z)/n → 0, (2) where Z denotes the secret key obtained by Eve, and I (M : Z) denotes the average mutual information between M and Z. Now we describe in detail the proposed reconciliation scheme following the routine presented in the above. Denote the secret key chosen randomly by Bob as M = [m1,m2, . . . ,ms] with length s, and choose independently a random sequence T = [t1,t2, . . . ,tr] with length r, where mi,tj ∈ {0,1}, 1  i  s, 1  j  r. In the proposed scheme, a particular linear error-correcting code C contains subcodes {C1, . . . ,C2s } and its cosets. Each coset consists of 2r codewords. To construct a codeword U uniformly from the coset Cμ, where μ is the decimal presentation of [m1,m2, . . . ,ms], the secret key M and the random sequence T together form the information part of the error-correcting code U. Clearly, the length of the information part of the code U is l = s + r. The LDPC codes are the most promising class of linear codes due to their capacity-approaching performance and low decoding complexity [30,31]. To achieve the reliability constraint presented in Eq. (1), we construct U to be a LDPC code. Generally, a LDPC code of length n is defined by a sparse parity-check matrix H having dimensions of m × n. Assuming that the parity-check matrix H is full rank, we have n − m columns denoted by H1 which corresponds to the information bits, and m columns denoted by H2 which corresponds to the parity bits. Then, the parity-check matrix H may be represented as H = [H1|H2], which is a [n(1 − R) + s] × (n + s) LDPC parity-check matrix for a given code length n and code rate R. It can be partitioned into H = [H1|H2] = [H m|Ht |Hq ], (3) where the systemic part is [H1] = [H m|Ht ] and the nonsystemic part is [H2] = [Hq ]. Furthermore, H m = [hm i,j ] is a [n(1 − R) + s] × s matrix, Ht = [ht i,j ] is a [n(1 − R) + s] × (nR − s) matrix, and Hq = [hq i,j ] is a [n(1 − R) + s] × [n(1 − R) + s] lower triangular matrix with the following the syndrome for the code U. Combining Eqs. (3) and (6), we have [H m,Ht ,Hq ]UT = 0. (7) A coding scheme of the punctured LDPC codes for the Gaussian wiretap channel based on LDPC codes was presented for the communication security [32]. The use of the punctured LDPC codes for discrete-variable QKD was previously considered in [33]. On the other hand, the possibility to decouple an eavesdropper Eve from the data shared by the trusted parties Alice and Bob was previously considered in [34] using physical methods of state engineering, which can be an alternative to efficient postprocessing codes. Following these achievements, we adopt the punctured LDPC codes approaches to decouple Eve from the data shared by Alice and Bob. The aim is to hide the secret key M by means of the puncturing way. In this way, instead of transmitting directly the secret key M, Bob may only need to send U to Alice. According to the punctured LDPC codes, Alice may reconstruct the secret message M by the decoder although it has not been transmitted in the channel. In detail, after Bob having obtained the LDPC code U = [M,T,Q], the secret key M is punctured and then Bob obtains a punctured LDPC code U = [T,Q]. We note here that the secret bits from M are arranged in the front of the code U. This will not influence the generation of the punctured LDPC code U = [T,Q], since the particular form of U = [M,T,Q] can be obtained from an arbitrated code U = [(M,T,Q)] by a suitable permutation operation, where (M,T,Q) denotes that bits of M, T , and Q are randomly arranged. Note that length of the punctured LDPC code U becomes n, and the length of the information is still l. Therefore, we have the code rate R = l/n for the punctured LDPC code U . Given a D-dimension Ak as introduced in Ref. [26], Bob generates αk(y,U ) with y and U , where k = 1,2, . . . ,D. Then, Bob sends αk(y,U ) to Alice on a public classical channel. When Alice and Eve obtain the pairs {x,αk(y,U )} and {E,αk(y,U )}, respectively, Alice calculates V = D k=1 αk(y,U )Ak · x, and Eve may calculate W = D k=1 αk(y,U )Ak · E, where E denotes the vector in possession of the eavesdropper. Denote the secret keys obtained by Alice and Eve as M¯ and Z, respectively. Alice decodes V to extract the secret key M¯ . Similarly, Eve decodes W to extract the secret key Z. In the proposed scheme, we require that Bob must be able to communicate with Alice at rate R such that the secret key can be decoded by Alice following the reliability constraint presented in Eq. (1) and that the leakage of information on the key to the eavesdropper satisfies the security constraint in Eq. (2). To achieve the security constraint, the length s of the secret key rate should be calculated as follows. When U and αk are independent, it has been proven that X (U : E,αk)  X (U ,αk : E), (8) where X (U : E,αk) is the Holevo information among E, αk(y : E), and U . Since αk(y,U ) is a function of y and U , the data-processing inequality gives X (U ,αk : E)  X (y : E), (9) where X (y : E) denotes the Holevo information between y and E. Then we get X (U : E,αk)  X (y : E). (10) The upper bound of X (y : E) has been provided in Refs. [28,35]; then the number of random bits is given by r = X (y : E). Consequently, one has s ≡ I (U : x,αk) − X (U : E,αk)  H(U ) − X (y : E) = l − X (y : E), (11) where the second line follows from X (U : E,αk)  X (y : E) and the assumption of H(U|x,αk) = 0. The last line follows from the assumption that U is chosen uniformly from the linear code C. Equation (11) shows that the length of the final secret key depends on how much information on the raw key has leaked to an eavesdropper. This characteristic is the same as the previously QKD protocols [12,36]. Since l is a constant, the length s is always a decreasing function of the amount of disturbance. We note here that if the information can be encoded coherently on the quantum states and be obtained in a proper way in the quantum transmission procedure, X (y : E) may be reached at saturation as described in Ref. [37]. In this case, the length s may be improved. It has been proved in [38] that given any desired code rate R1 close to capacity, we can first construct a code with lower rate R0, and then puncture it to the desired rate R ∈ [R0,R1]. A necessary condition for these LDPC codes to be capacity achieving is that their parity-check matrix density diverges to infinity as capacity is approached. Specifically, the density grows at least as ln(1/) with respect to the multiplicative gap to capacity  [38]. Therefore, for the desired code rate R1 which achieves a high postprocessing efficiency β, we can construct a code with lower rate R0 and the puncture it to the desired rate R1. However, for long distance CV-QKD, we usually have a low signal-to-noise ratio (SNR), which is associated with a small channel capacity C = (1/2) log2(1 + SNR) and leads to a low code rate for high efficiency β. Furthermore, it is pointed out in [38] (Theorem 3) that given any R1 ∈ (0,1) and row weight κ > 5, let S2 be as implied in [38] (Theorem 1). For any given η > 1 and R0 ∈ (0,S2), if >η(κ/3)R(κ/3−1) 0 / ln 2, then the punctured ensemble with any rate R ∈ [R0,R1] has a vanishing average block error probability under maximumlikelihood decoding on the memoryless binary-input outputsymmetric (MBIOS) channel with capacity C = R/(1 − ). Therefore, we have β = R/C = 1 −  < 1 − η(κ/3)R(κ/3−1) 0 / ln 2. (12) It is easy to see that in case R0 → 0, R(κ/3−1) 0 approaches 0 faster than η(κ/3) approaches ∞, and therefore β → 1. III. SECURITY Now we investigate the security of the proposed reconciliation scheme. To confirm its unconditional security, we prove first that the generated punctured-LDPC code U is uniformly distributed. Theorem III.1. Let the secret key M be uniformly distributed over {0,1}s. Then, both U and U are uniformly distributed over C. And length s of the secret key is determined by Eq. (11). Proof. Note that if the LDPC code is a class of linear block codes, then each information vector [M,T ] corresponds to one LDPC code U = [M,T,Q]. Since both M and T are uniformly distributed and T is statistically independent of M, the LDPC code U is uniformly distributed over C, and it is easy to see that the punctured LDPC code U is also uniformly distributed over the shortened LDPC code C. Hence, the length of the generated punctured LDPC code U may be calculated using Eq. (11).  Next, we investigate the security constraint in the proposed reconciliation scheme based on the punctured LDPC code. We have the results presented in the following theorem. Theorem III.2. If the punctured LDPC code used in the proposed reconciliation scheme achieves the Shannon capacity, one gets 1 n I (M : Z) → 0 as n → ∞. Proof. Assume that the leaked information rate Eve gets about U is Ce, i.e., 1 nX (U : E,αk) = Ce. Making use of Eq. (11), we get 1 n s = l − X (U : E,αk) n = R − Ce. (13) Subsequently, Ce = l − s n = r n . (14) Since there are l − s random bits in the vector T which have been encoded as information by Bob, each secret key M is associated with a coset Cμ of 2r uniformly distributed codewords. It has been proved that capacity-achieving codes of any rate and for any memoryless binary-input outputsymmetric channel can be constructed by puncturing optimal LDPC codes [38]. Therefore, when n → ∞, there exists a coset Ci approaching the eavesdropper’s channel. Then, we have 1 n I (U : Z|M =[m1m2 ··· ms])= log2 |Cμ| n = r n =Ce. (15) We expand the leaked information I (M : Z) as I (M : Z)=I (Z : UM) − I (U : Z|M) =I (U : Z) + I (M : Z|U) − I (U : Z|M). (16) Since M → U → Z is a Markov chain, we have I (M : Z|U) = 0. Furthermore, since αk is a mapping between U and y, αk is also a mapping between Z and E. It is clear that I (U : Z) = X (U : E,αk) = nCe. (17) Substituting Eqs. (15) and (17) into Eq. (16) and multiplying both sides by 1 n yields 1 n I (M : Z) = 1 n X (U : E,αk) − 1 n I (U : Z|M)  Ce − Ce = 0. (18) Consequently, if the punctured LDPC can approach the Shannon capacity when n → ∞, the information about the secret key leaked to Eve is zero.  As stated in Ref. [39], if Alice and Bob know that Eve has no information on the vector M, then the sequence M itself can be used as the final secret key. Obviously, the Theorem III.2 indicates that the privacy amplification is no more needed in the proposed reconciliation scheme. This is very useful since not only the final key rate may be more larger but also the involved CV-QKD system may be more available. Practically, a capacity-achieving punctured LDPC code with rate R can always be constructed. Accordingly, the proposed scheme is available. In addition, Theorem III.2 demonstrates that the security of the proposed reconciliation scheme is independent of the reconciliation directions, i.e., direct or reverse. This means the proposed scheme is also suitable for the direct reconciliation procedure. IV. PERFORMANCES Now we study the performances of the proposed scheme based on the punctured LDPC codes. The computational complexity as well as the reliability will be addressed in this section. A. Complexity and Secret Key Rate Considering the availability of the well-known multidimensional reconciliation in practical application, we mainly investigate the complexity of the proposed scheme by comparing it to the scheme of the postprocessing procedure based on the multidimensional reconciliation scheme proposed in Ref. [26] and the general privacy amplification scheme. As described in Sec. II, the proposed reconciliation scheme does not need to calculate the syndrome but a LDPC encoding procedure is necessary. While in the multidimensional reconciliation scheme, the reconciliation procedure needs a syndrome calculation. Clearly, in the encoding process of the proposed scheme, [n(1 − R) + s] parity bits should be computed serially from q1 to q[n(1−R)+s] by means of Eq. (6). Therefore, the number of XOR operations is η − 2[n(1 − R) + s], where η is the number of 1’s in H. While in the syndrome calculation H · UT of the multidimensional reconciliation scheme, the number of XOR operations is η − [n(1 − R) + s]. In addition, the proposed scheme does not need the privacy amplification algorithm. Denoting the time complexity of the general privacy amplification as Opa, then the complexity difference between the proposed scheme and the postprocessing scheme based on the multidimensional reconciliation scheme and the general privacy amplification scheme is given by  O = n(1 + R) + s + Opa. (19) This means that the complexity of the proposed scheme has been reduced by  O more than the previous postprocessing scheme which is based on the multidimensional reconciliation and privacy amplification. It has been shown that the theoretical secret information obtained using one-way reconciliation is bounded by [26,27] where we let, without loss of generality, the time cost for each operation be unit, and O denotes the time complexity of the involved CV-QKD protocol. Since the complexity of the quantum transmission is very small, O may be regarded mainly as the time complexity of the postprocessing procedure. Accordingly, compared to the conventional postprocessing procedure based on the multidimensional reconciliation scheme and privacy amplification, the final secret key rate with the proposed scheme is given by Kˆ q =  IK O −  O (bps). (22) As an example, we consider the influences of the privacy amplification algorithm based on the Teoplitz matrix on the final secret key rate in the previous postprocessing procedure. The simulation parameters are as follows: the homodyne detection efficiency ηloss = 0.6, the electronic noise Vel = 0.01, the excess noise ε = 0.01, and the attenuation αf = 0.2 dB/km, The VA is optimized and the distance is 25 km. The LDPC decoding algorithm used here is the log-likelihood sum-product algorithm, and the maximum iteration number is set to be 15, 20, 25, and 50, respectively. Simulations of the conventional postprocessing scheme and the proposed reconciliation scheme have been performed on a computer with a dual-core CPU and 4.00 GB of memory. The results are demonstrated in Table I. We could find that the proposed reconciliation scheme leads to a higher secret key rate than that of the the conventional postprocessing scheme. B. Reliability Now, we analyze the reliability constraint of the proposed reconciliation scheme. Let H = [h1,h2, . . . ,hn+s] denote the parity-check matrix of an arbitrated LDPC code U  = [u1,u2, . . . ,un+s], where hi is a column vector of length [n(1 − R) + s], 1  i  (n + s). Let U   = [u1,u2, . . . ,ui1−1,ui1+1, . . . ,ui2−1,ui2+1, . . . ,uis−1,uis+1,..., un+s] denote the capacity-achieving LDPC codes obtained by puncturing s bits of U  with an optimal puncturing scheme. According to punctured codes theory [38,41], we have Prob{U   = U¯ } → 0, (23) where U¯ denotes the code after the decoding process, and the indices of the punctured bits are denoted by i1,i2,...,is. Permute the column of indices i1,i2,...,is of H to the left of H, such as π(H) = [hi1 , . . . ,his ,h1,h2, . . . ,hn+s], where π(·) denotes the corresponding permutation operation. Let H = π(H), U = π(U ), U = π(U  ), and U¯ = π(U¯ ); then one gets Prob{U = U¯ } → 0, (24) where U = [T,Q] and U¯ = [T ,¯ Q¯ ]. It is easy to see that Prob{M = M¯ } → 0, which means the reliability constraint is guaranteed. V. CONCLUSION In conclusion, we present a reconciliation scheme for the CV-QKD based on the punctured LDPC codes. The proposed reconciliation scheme does not need to calculate the syndrome, which would be sent over the public channel in the previous multidimensional reconciliation scheme. By forming a lower triangular matrix in the parity-check matrix of the LDPC codes, the present scheme has lower encoding complexity compared to the multidimensional reconciliation scheme. Furthermore, when the punctured LDPC code achieves the Shannon capacity, there is no information leaked to the eavesdropper in the proposed scheme. Subsequently, the privacy amplification is not needed after the reconciliation process. These features lead to a higher secret key rate, optimal performance, and availability for the CV-QKD scheme. ACKNOWLEDGMENTS	Achieving information theoretic security with practical complexity is of great interest to continuous-variable quantum key distribution in the postprocessing procedure. In this paper, we propose a reconciliation scheme based on the punctured low-density parity-check (LDPC) codes. Compared to the well-known multidimensional reconciliation scheme, the present scheme has lower time complexity. Especially when the chosen punctured LDPC code achieves the Shannon capacity, the proposed reconciliation scheme can remove the information that has been leaked to an eavesdropper in the quantum transmission phase. Therefore, there is no information leaked to the eavesdropper after the reconciliation stage. This indicates that the privacy amplification algorithm of the postprocessing procedure is no more needed after the reconciliation process. These features lead to a higher secret key rate, optimal performance, and availability for the involved quantum key distribution scheme.
22	2015	Explicitly correlated wave function for a boron atom	Mariusz Puchalski,1 Jacek Komasa,1 and Krzysztof Pachucki2	Explicitly correlated wave function for a boron atom  (Received 22 October 2015; published 4 December 2015) We present results of high-precision calculations for a boron atom’s properties using wave functions expanded in the explicitly correlated Gaussian basis. We demonstrate that the well-optimized 8192 basis functions enable a determination of energy levels, ionization potential, and fine and hyperfine splittings in atomic transitions with nearly parts per million precision. The results open a window to a spectroscopic determination of nuclear properties of boron including the charge radius of the proton halo in the 8B nucleus. DOI: 10.1103/PhysRevA.92.062501 PACS number(s): 31.15.ac, 31.30.J− I. INTRODUCTION While for hydrogenic ions the nonrelativistic wave function is known exactly, for all larger atomic systems it has to be obtained numerically, most often with the help of the variational principle. The numerical precision achieved for a few electron systems can, nevertheless, be very high. For example, nonrelativistic energies of the He atom are known with more than 20 digits of accuracy [1–3], of Li with 15 digits [4,5], and very recently the precision achieved for the Be atom reached 11 significant digits [6–8]. The computational approach employed in all those atomic studies is based on explicitly correlated functions of the exponential or Gaussian form, which are the best known representations of the nonrelativistic wave function. For three-electron systems, the most accurate solution of the Schrodinger equation is ¨ obtained with the Hylleraas (exponential times polynomial) basis functions [4,5,9]. In such systems, the accuracy of the theoretical predictions for transition energies and isotope shifts is limited by the approximate treatment of higher-order ∼mα6,7 QED corrections rather than by numerical inaccuracies of the nonrelativistic wave function. Methods with Hylleraas functions have been extended to four-electron atomic systems but only for some restricted selection of basis functions, because of significant difficulties in evaluation of matrix elements [10,11]. Even more difficult integrals appear in the matrix elements of relativistic operators. Unquestionably, significant efforts have to be made to improve the Hylleraas approach, in order for it to be practical for the four and more electron systems. Therefore, at present, the method of choice for such systems is that based on explicitly correlated Gaussian (ECG) functions. The effectiveness of the ECG functions in treating few-electron problems has already been demonstrated by high-precision calculations of the nonrelativistic energies of atomic and molecular systems [12–17]. In particular, for the beryllium atom the highest accuracy has been obtained using the ECG functions [7,18–21]. The main advantage of the ECG method is that the underlying integration is manageable and very fast in numerical evaluation due to the compact formulas involving only elementary functions. On the other hand, the Gaussian functions have the drawback of improper asymptotic behavior since they decay too fast at long interparticle distances. They also have an incorrect short-range form and fail to correctly describe the Kato cusp. However, these two flaws can be overcome if one employs a sufficiently large and well-optimized ECG basis set. The issue is subtler in calculations of relativistic and QED properties, in which the local inaccuracies of the wave functions result in significant numerical loss of mean values. One has to carefully optimize over a large number (oftentimes exceeding 105) of variational parameters matching local behavior of the exact wave function and employ dedicated techniques which accelerate the convergence of nearly singular matrix elements [22]. An additional drawback of the approach based on fully correlated functions is the cost resulting from antisymmetrization of the wave function which grows like N! with the number of electrons N. In this paper we demonstrate that in spite of this high evaluation cost the methods based on ECG functions may give a spectroscopic accuracy for five-electron systems, such as the boron atom, in a realistic computational time. We report on the calculation of nonrelativistic energies of the ground 2 2 P and the excited 3 2 S levels, and the leading relativistic corrections including the fine and hyperfine structure of the ground state. The achieved numerical accuracy is several orders higher than those of any previous calculations and not always in agreement with them. In addition, we provide accurate results for a four-electron B+ ion needed to determine the ionization potential of the boron ground state. II. NONRELATIVISTIC HAMILTONIAN AND CORRECTIONS The determination of accurate wave functions corresponding to the nonrelativistic, clamped nucleus Hamiltonian (in natural units) is determined, all the corrections to the energy E0 in the following perturbative expansion in the fine-structure constant α ∼ 1/137,can be expressed in terms of expectation values | ... | ≡ ... of known operators. Complete nonrelativistic energy E(2) consists of the clamped nucleus energy E0 and the kinetic energy of the nucleus HN, which can be calculated in the  III. REDUCTION OF MATRIX ELEMENTS We represent the wave function i of the five-electron 2 P atomic state in the form The c coefficients are integers and depend on the permutation {a}. Since the number of permutations is 5! = 120, we cannot explicitly write them down here. What is important is that all the matrix elements are either of the standard form ...S with the constant coefficients c or of the Fermi interaction form ...F with the cF coefficients. The 2 P1/2 and 2 P3/2 wave functions are constructed using Clebsch-Gordan coefficients. Expectation values with these wave functions can be reduced to spinless expressions with an algebraic prefactor KJ for J = 1/2 and 3/2. Namely, for an operator Q, the first-order matrix elements with an auxiliary notation {K1/2,K3/2} take the form The above spin reduced matrix elements involve only scalars built of spatial variables ra, and therefore they all can easily be expressed by Gaussian type integrals. IV. NUMERICAL CALCULATIONS AND RESULTS In the numerical calculations we employed the ECG basis functions of progressively doubled size from 1024 to 8192 terms for the B atom, and from 512 to 4096 terms for the B+ ion. The nonlinear parameters were optimized variationally with respect to E0 until the energy reached stability in a desired number of digits. The sequence of energies obtained for consecutive basis sets enables estimation of the basis truncation error. The convergence for the 2 2 P and 3 2 S levels of B and the 2 1 S state of the B+ ion is presented in Table I. The variational energies obtained from the largest expansion are lower than the best results previously reported in Refs. [23,24]. In a similar way, i.e., from the convergence with the growing basis set size, the truncation errors of the expectation values of various operators were estimated. Particular care was taken for the singular or nearly singular operators (p4 a,δ3(ra),δ3(rab),P(r−3 ab )), which exhibit a slow numerical convergence of their mean values. This undesirable effect is particularly pronounced for the ECG functions having improper short-distance behavior. The solution is to employ the regularized matrix elements following Drachman’s recipes [26]. Previously [22], this approach enabled the accuracy of the expectation values to be increased by several orders of magnitude. The expectation values of all the operators involved in the determination of the fine-structure state energy are collected in Table II. All the entries are accompanied by their estimated uncertainty. Table III contains the α-expansion components and the final values of the measurable quantities: the 3 2 S1/2-2 2 P1/2 transition energy, the fine-structure splitting, and the groundstate ionization potential for the most abundant 11B isotope of the boron atom. In the table, the theoretical predictions are compared with recent calculations [27–29] and with the experimental values collected by Kramida and Ryabtsev [30]. The agreement of the new values with the experimental results is apparent and the remaining discrepancies are consistent with estimated uncertainties due to the approximate value of the Bethe logarithm and due to neglected higher-order O(α6) corrections. In contrast, significant differences are observed with all the previous calculations. Although none of the cited theoretical values caries uncertainty, it is clear that the number of digits quoted there is by far too high. One may conclude that the standard configuration interaction (CI), multiconfiguration Dirac-Fock (MCDF), or coupled cluster (CC) methods based on one-electron functions are not capable of supplying results with controlled precision. The numerical results for the hyperfine splitting are presented in Tables IV and V. The former table collects the expectation values of individual operators comprising theHHFS Hamiltonian and include the Fermi contact, orbital term, spindipole term, as well as the term describing the interaction of the nuclear electric quadrupole moment with the electric-field gradient produced by electrons. The head of this table presents a relation of the expectation values to the commonly used hyperfine parameters and the corresponding prefactors used in their evaluation. We observe significant discrepancies for individual contributions in Table IV with the results from the previous calculations by Chen [27]. This is particularly pronounced in the case of the Fermi-contact parameter ac, for which this discrepancy is over 25%. Surprisingly, the differences between Chen’s results and the previously established experimental A-hyperfine constants are much smaller (see Table V). Moreover, the difference between our result and the experimental values is consistent with the O(Z α) 2 unknown relativistic correction, namely, it is about 50% of (Z α) 2 times the corresponding A or B coefficient. The final values for both the 2 2 P levels and both 11B and 10B isotopes are presented in Table V. V. CONCLUSIONS We calculated the energy levels, isotope shifts, and fine and hyperfine structure in the atomic boron with numerical precision of a few parts per million. We demonstrated that the majority of the previous calculations were not as accurate as claimed—instead of five to six digits only the first two were significant. This is particularly apparent for the finestructure splitting and for the Fermi contact interaction, the last one being exceptionally small for the 2P ground state. The smallness of the Fermi contact term makes the hyperfine splitting insensitive to the not-well-known nuclear finite-size effects, thus the comparison with experimental HFS will be a good test of the atomic computational methods. Moreover, the precise value for the mass polarization correction (see Table II) permits the accurate determination of the isotope shift in the 2P-3S transition, which paves the way for determination of the nuclear charge radius of the proton halo in the 8B nucleus, as we have already demonstrated for the beryllium atom [8]. ACK	We present results of high-precision calculations for a boron atom’s properties using wave functions expanded in the explicitly correlated Gaussian basis. We demonstrate that the well-optimized 8192 basis functions enable a determination of energy levels, ionization potential, and fine and hyperfine splittings in atomic transitions with nearly parts per million precision. The results open a window to a spectroscopic determination of nuclear properties of boron including the charge radius of the proton halo in the 8B nucleus. DOI: 10.1103/PhysRevA.92.062501 PACS number(s): 31.15.ac, 31.30.J−
23	2015	Polaronic effects in one- and two-band quantum systems	Tao Yin, Daniel Cocks,2 and Walter Hofstetter	Polaronic effects in one- and two-band quantum systems  (Received 3 October 2015; published 29 December 2015) In this work, we study the formation and dynamics of polarons in a system with a few impurities in a lattice immersed in a Bose-Einstein condensate (BEC). This system has been experimentally realized using ultracold atoms and optical lattices. Here, we consider a two-band model for the impurity atoms, along with a Bogoliubov approximation for the BEC, with phonons coupled to impurities via both intraband and interband transitions. We decouple this Frohlich-type term by an extended two-band Lang-Firsov polaron transformation using a ¨ variational method. The new effective Hamiltonian with two (polaron) bands differs from the original Hamiltonian by modified coherent transport, polaron energy shifts, and induced long-range interaction. A Lindblad masterequation approach is used to take into account residual incoherent coupling between polaron and bath. This polaronic treatment yields a renormalized interband relaxation rate compared to Fermi’s golden rule. For a strongly coupled two-band Frohlich Hamiltonian, the polaron is tightly dressed in each band and can not tunnel ¨ between them, leading to an interband self-trapping effect. DOI: 10.1103/PhysRevA.92.063635 PACS number(s): 67.85.−d, 71.38.−k, 03.65.Yz, 63.20.K− I. INTRODUCTION The field of ultracold atom physics has explored a wide variety of phenomena since its relatively recent accessibility, with a major feature being the tunability of experiments across wide parameter regimes [1–4], to easily access and probe phase transitions [5,6], as well as excitation spectra and dynamics of systems analog to condensed matter [7–10]. Even features such as artificial gauge fields can be implemented for neutral atomic particles, allowing for the investigation of topological phases [11,12]. Within this ultracold toolbox, one ingredient is becoming of increasing interest in recent years which is of vital importance to real solid-state systems: phonons and atom-phonon coupling [13–16]. Such a coupling provides many interesting possibilities [17–19]. For one, it can lead to effective Hamiltonians, such as extended Hubbard models or the Holstein model [20– 24], as well as dissipative two-level system [25–28]. Polaronic effects from electron-phonon interactions have also long been suggested to be the proponent behind high-Tc superconductivity in one- and two-band solid-state systems [29–35]. In ultracold quantum gases, evidence of polarons has been found in systems with trapped ions [36] or systems with a single ion immersed in a degenerate quantum gas [37–43]. On the other hand, the atomic polaron has also been studied both experimentally and theoretically in systems of imbalanced Bose-Fermi mixtures [44–49] and Fermi-Fermi mixtures [50–57]. In the particular case of a system with impurities immersed in a bosonic bath, these impurities couple to bosonic excitations. For suitable parameters, polaronic phenomena arise generically in such systems [58–81]. There are also other proposals for realizing atom-phonon couplings and polaronic effects, including crystals of dipolar molecules [82–84], nanoparticles [85,86], and hybrid atom-ion coupled systems [87]. None of these works consider polaronic phenomena of multiband systems with ultracold quantum gases. In such *taoyin@itp.uni-frankfurt.de systems, the interband dynamics of polarons, as well as intraband dynamics, lead to new effects which have no analog in single-band systems. Motivated by recent experiments [88–91], we consider a system of a few impurities in an optical lattice, populating the lowest two Bloch bands, immersed in a Bose-Einstein condensate (BEC). These impurities are coupled to Bogoliubov phonons (of the BEC) via both intraband and interband transitions. In order to decouple this Frohlich-type term, we derive a generalized two-band Lang- ¨ Firsov polaron transformation. The transformed effective Hamiltonian still contains two bands, where the impurity is now dressed by phonons as a quasiparticle (polaron). We use a variational approach to connect between the weak- and strong-coupling limits and calculate the dressing parameters. Polaronic effects modify both intraband coherent transport and polaron energy shifts, and also induce a long-range interaction between different polarons. We then focus on interband relaxation effects and specify our system as a single impurity trapped in a quasi-onedimensional (quasi-1D) system. We study the residual incoherent coupling between polaron and bath by using a Lindblad master equation. The impurity interband relaxation process under this polaronic treatment is beyond a Fermi’s golden rule description. These polaronic renormalization effects of the interband relaxation rate should be accessible in current experiments. On the other hand, for large impurity-phonon coupling, the polaron is tightly dressed in each band and cannot hop between different bands. In this limit, an interband self-trapping effect is expected. This work is organized as follows: In Sec. II, we introduce the effective two-band Hamiltonian of a realistic experiment setup, with a few impurities in a lattice, immersed in a BoseEinstein condensate of a different atomic species. In Sec. III, we describe in detail the generalized Lang-Firsov polaron transformation for the two-band system. The transformed Hamiltonian with two (polaron) bands can be separated into a coherent part and an incoherent part. The coherent part, arising from the thermal average over the phonon bath, is discussed in Sec.IV. The interband relaxation and decoherence 1050-2947/2015/92(6)/063635(20) 063635-1 ©2015 American Physical Society TAO YIN, DANIEL COCKS, AND WALTER HOFSTETTER PHYSICAL REVIEW A 92, 063635 (2015) effects, which are all included by the incoherent part of this Hamiltonian, are discussed in Sec. V. We derive the Lindblad equation and correlation functions for the residual incoherent impurity-phonon coupling. This polaronic impurity dynamics is closely related to recent experiments. The polaronic interband relaxation rate is compared to Fermi’s golden rule. We give concluding remarks and an outlook in Sec. VI. II. EFFECTIVE HAMILTONIAN Here, we consider a few neutral impurities with mass mI interacting with a Bose-Einstein condensate of another neutral species. The impurities are trapped by a three-dimensional (3D) optical lattice and their Hamiltonian is denoted by HI . The homogeneous BEC system HB is formed by another atomic species with mass mB and a weak repulsive interaction gB. The impurity-BEC interaction Hint is caused by s-wave interactions between the different species, which can be tuned by standard Feshbach resonance techniques. The total Hamiltonian is hence H = HI + HB + Hint. We describe these different terms in detail in the following parts of this section. A. Impurities in optical lattice: HI We consider a quasi-1D system with impurities trapped in an anisotropic 3D optical lattice: with lattice constant d = λ/2 and laser wavelength λ. We use the lattice constant d as the unit of length throughout this paper. The single-impurity recoil energy is ER ≡ π22/(2mI ). The trapping strength in the transverse (y,z) direction is assumed to be much stronger than in longitudinal (x) direction with V x I  V ⊥ I ≡ V y I = V z I . The impurities are therefore tightly trapped in the transverse direction, and remain in the ground state of the associated harmonic oscillator potential:  where J α and εα are the hopping parameters and onsite energy for each band with index α = 0,1. In a deep lattice, the band gap ε  ≡ ε1 − ε0 can be approximated as the longitudinal oscillator frequency ωx . In this work, we only consider interband dynamics between the lowest two Bloch bands and ignore higher-band effects. B. Bosonic bath The impurities are immersed in a homogeneous BEC with weakly repulsive boson-boson interaction gB between the atoms. In a dilute system, this weak interaction can be described by boson-boson scattering length aB as gB = 4π2aB/mB. For vanishing interspecies interaction gIB between impurity and bath, the BEC can be described by standard Bogoliubov theory and treated as a phonon bath (see Fig. 1). Once gIB is introduced, the BEC becomes deformed due to the presence of impurities. This interaction is closely related to the impurity-boson scattering length and other system parameters such as the impurity-boson mass ratio and the impurity confinement strength. The relation can be determined by making use of scattering theory in the low-energy limit, such as the Lippmann-Schwinger equation or effective field theory [92]. For an unconfined impurity, the interspecies interaction gIB can be derived as gIB ≡ 2π2aIB/μ with the reduced mass μ ≡ mImB/(mI + mB) and 3D impurity-boson scattering length aIB. On the other hand, for the confined impurity, gIB needs to be treated carefully due to lattice effects such as confinement-induced resonances [93,94]. In the specific system we considered here, the impurity is confined in one-dimensional tube (quasi-1D) by an anisotropic optical lattice while the bosonic atoms are free in three-dimensional space (3D). When the transverse characteristic length σ⊥ is much smaller than any other length scales, the resulting system is mixed dimensional. At low energies, the interspecies interaction is solely characterized by a single parameter, the effective scattering length aeff IB, whose value can be obtained numerically [95]. This fact also allows us to arbitrarily tune the value of aeff IB by tuning the transverse confinement strength, independently of tuning the Feshbach resonance position. Here, we use the approach in [60,61], where the deformation is treated as a perturbation around the BEC ground state. The bosonic field operator is expanded as ψˆ (r) = ψ0(r) + ϑˆ (r), where ψ0(r) is the order parameter in the absence of interspecies interaction and ϑˆ (r) = ϑ(r) + ζˆ(r) represents the perturbation itself, which consists of a correction to the order parameter ϑ(r) and Bogoliubov excitation operators ζˆ(r). The modified BEC ground state is described as ψ0(r) + ϑ(r), which is the Gross-Pitaevskii solution including the presence of impurities. This modification ϑ(r) shifts the equilibrium positions in order to minimize the total energy of the system. The small excitations ζˆ(r), around the static GP ground state ψ0(r) + ϑ(r) of the condensate, can be described in the terms of Bogoliubov modes with bosonic operators βˆ† q (βˆ q) creating (annihilating) a Bogoliubov quasiparticle with momenta q. The coefficients uq(r) and vq(r) can be determined by Bogoliubov–de Gennes equations C. Atom-phonon coupling The impurity-BEC interaction of the Hamiltonian can be written as with α,β indicating the impurity Bloch bands and mαβ i,j ;q ≡  d3reiq·r Wα∗ i (r)Wβ j (r). This term describes the impurity coupling to the phonon bath by creating or annihilating phonons. The nonlocal coupling terms with i  = j are highly suppressed due to the local form of the interaction which requires overlap between localized Wannier functions. For these reasons, the above integralsmαβ i,j ;q can be well approximated by δijmαβ q eiq·Ri such that the value depends only on α, β, and q. The dimensionless impurity-phonon coupling Mαβ i;q can also be written as Mαβ q eiq·Ri with by using the identity for Hermite polynomial integrals [96]. Here, q⊥ ≡   q2 y + q2 z indicates the transverse phonon momentum. In order to describe polaronic effects resulting from the impurity-BEC coupling, we introduce a dimensionless coupling constant as in [73] and name it κ. This constant depends on the impurity-boson and boson-boson interactions gIB, gBB and the condensate parameters ξ and mB as or equivalently α = 4πn0a2 IBξ . These two coupling constants are related by α = (κμ/mB) 2 /π, where μ is the reduced mass. There are also other coupling constants used [59,74], which are slightly different from α or κ. By tuning the impurity-boson scattering length aeff IB, the coupling constant κ can be tuned continuously. However, the condition in Eq. (10) requires that the coupling constant satisfies the relation III. VARIATIONAL TWO-BAND POLARON TRANSFORMATION A. Transformation with exponential quadratic operators This two-band Hamiltonian in Eq. (19), with Frohlich-type ¨ impurity-phonon coupling, can not be solved analytically even for the case of a single impurity. The goal of this paper is to find a simple but nontrivial variational method which can deal with the two-band system (19) in general. We choose the Lang-Firsov polaron transformation approach and generalize it to two-band system. This canonical transformation is exact and decouples the impurity-BEC interaction term in a new quasiparticle basis. In this basis, the kinetic part in Hamiltonian (19) contains the dynamics of quasiparticle and its interactions between the transformed phonon bath. We solve the coherent part of this transformed Hamiltonian with a variational treatment and take into account the remaining incoherent parts by a master equation. First, we introduce the basic concept of the transformation for a single band before we extend it to the two-band case. When a single impurity moves in a lattice and couples to a phonon bath, there are exact solutions in both the weakand strong-coupling limits [21]. When the impurity-phonon interaction is much weaker than the impurity kinetic energy, the impurity behaves as a free particle in a lattice. On the other hand, when the interaction is much larger than the kinetic part, the impurity will be tightly dressed by a “cloud” of phonons, forming a quasiparticle. The phonons are tied to the impurity such that the impurity cannot move on its own but must drag around a phonon cloud. This increases the effective mass of the quasiparticle. In the intermediatecoupling region, the phonon dressing competes with the impurity dynamics. In order to describe this competition, a variational ground state can be used to connect between the weak- and strong-coupling limits [27,69,97–100]. This variational ansatz is equivalent to a canonical transformation H˜ ≡ eSˆ Heˆ −Sˆ with Sˆ ≡  i,q i,q(bˆ† −q − bˆ q)nˆi where i,q are the variational parameters. The transformed Hamiltonian H˜ still cannot be solved analytically, but can be separated into a coherent part H˜T and an incoherent part H˜inc ≡ H˜ − H˜T where ...T indicates a thermal average over the phonon bath. The coherent part, which is decoupled from the phonon bath, is of the form of an extended (polaronic) Hubbard model. The incoherent part describes the residual coupling between polaron quasiparticle and phonon bath. Compared to the initial “bare” impurity-phonon coupling, this incoherent part is significantly reduced by the polaron transformation. We first focus on the coherent part and neglect the incoherent terms. The variational parameters i,q are determined by minimizing the coherent Hamiltonian energy and approach i,q = Mi,q in the strong-coupling limit. Finally, the residual incoherent part can be included by a perturbative approach such as the Lindblad master equation. In order to find a suitable variational transformation for the two-band system, we modify the Lang-Firsov polaron transformation H˜ = eSˆ He−Sˆ by extending the impurity-phonon interaction to the two-band form. It takes the form with αβ i,q ≡ αβ q eiq·Ri . A similar method was also applied by Sibley and Munn in [101–104] and Stojanovic´ et al. in [99] for a single-band system with nonlocal impurity-phonon coupling. Our initial guess for the variational parameters is the coupling element itself Mαβ q , and we constrain the variational parameters to obey the same symmetry properties as Mαβ q , namely, that of Eq. (14). By using the Baker-Campbell-Hausdorff formula, eSˆ Aeˆ −Sˆ = Aˆ + [S,ˆ Aˆ] + 1 2! [S,ˆ [S,ˆ Aˆ]] + ... , we can derive the transformed Hamiltonian with exponential quadratic operators as outlined in Appendix A. For convenience, these expressions can be written in 2 × 2 matrix form via B. Diagonal transformation matrix In Appendix A, we calculate the coherent part H˜T by averaging over the phonon bath and assuming it is thermal. In contrast to the single-band case, these calculations are demanding when both intraband and interband phonon couplings are included. In order to determine the variational parameters q, we finally need to minimize the free energy of whole coherent Hamiltonian. Until now, we did not make any assumptions for our variational parameters q except for the symmetry relations in Eq. (14). Unfortunately, the general result of the transformed Hamiltonian in Eq. (24) and its corresponding coherent part are still quite complicated. It can be further simplified by making some approximations suitable to our specific system. Due to conservation of energy, the phonon-induced interband dynamics requires the phonon energy to match the impurity band gap, i.e., ωq ≈ ε . This energy scale involves a phonon with particlelike dispersion and momentum |q| ≈          2mBε / significantly far from zero. The interband coupling M01 q for this large phonon momentum is highly reduced due to the Gaussian decay of m01 q in Eq. (15). On the other hand, intraband dynamics requires a phonon energy ωq ≈ J 0; J 1 with phononlike dispersion and small momentum |q| ≈ J α√mB/(gBn0)/ = J α/(c). The intraband coupling Mαα q is not reduced too much at this smaller momentum. In the polaron transformation, the parameters αβ q reflect the dressing of the impurity by phonons and are closely related to Mαβ q . For this reason, we treat the interband coupling as small and approximate the matrices q as diagonal: with variational parameters λ0 q and λ1 q. Since the intraband couplings Mαα q are purely real numbers in our system, we assume λα q are also real numbers. After the transformation with these diagonal matrices q, we have then decoupled the intraband impurity-phonon coupling and leave the (relatively) small interband coupling in the new polaronic two-band Hamiltonian. The coherent part, with phonons eliminated by thermal averaging, is a many-body Hamiltonian In the above formula, we use the fact that operators Xˆ i have only diagonal terms. There are also induced interactions Vˆ P between multiple polarons, which can lead to strong correlations in the system and will be discussed in the next section. In the above calculations, we need to sum over all possible phonon momenta q. In the thermodynamic limit of the phonon bath, we use the relation  q →  (2π) D  dq with quantization volume  for the phonons, and write this explicitly in cylindrical coordinates:  (2π) 3  dq⊥  dqx 2πq⊥. For the polaronic intraband hopping J α P , we only need to calculate nearest-neighbor terms with j = i ± 1 regardless of the specific value of i. By noting that IV. COHERENT POLARON DYNAMICS AND INTERACTIONS A. Single-polaron band structures In the previous section, we derived a general form of the two-band polaron transformation and calculated the resulting coherent part of the Hamiltonian in Eq. (25). For the system with a single polaron, there are only intraband terms in the coherent Hamiltonian (25). The single impurity is dressed by a coherent phonon cloud in each band. The residual interband polaron-bath coupling will appear only in the incoherent Hamiltonian. It is easy to diagonalize the single-polaron coherent part in the momentum representation and minimize the free energy F ≡ −kBT ln α,k exp (−Eα k /kBT ) for this two-band system, with polaron dispersion: Here, k is quasimomentum in longitudinal direction. These variational parameters, which are real numbers, can then be determined by the self-consistent equations Before numerically calculating the variational parameters λα q, we first discuss some properties of this self-consistent equation. Considering a simplified model with momentumindependent variational parameters λ in a single-band system, we choose to minimize only the ground-state energy. The selfconsistent equation (30) will be modified as These two solutions λ−,λ+, corresponding to two local minima of ground-state energy, indicate the impurity is respectively loosely or tightly dressed by phonons. At a critical impurityphonon coupling with q fq|Mq| 2 = 27/8, when the two minima become equal, the lowest-energy state solution abruptly switches from λ− to λ+, indicating a first-order polaronic transition. On the other hand, the solution of λ is a smooth and continuous crossover when the adiabatic condition is broken with small value of |J |. This transition-crossover behavior also appears later when solving the self-consistent equation (30) numerically, although we have considered a much more simplified model here. Strictly speaking, this sharp polaronic transition in the adiabatic regime is due to the mean-field approximation by thermal averaging of the phonon degrees of freedom. This drawback could be improved if we were to treat the incoherent dynamics properly, by taking into account fluctuations or using a master-equation method. We now compare the variational parameters λα q = αα q /Mαα q from Eq. (30) and polaron dressing effects for If the impurity-BEC coupling gIB increases, the polaron effective mass will increase exponentially as mα P = mα 0 exp (Sα T ), with mα 0 indicating the impurity effective mass at κ = 0. In Fig. 3, we compare energy spectrum Eα k , renormalization of intraband hopping J α P /J α, polaron effective mass mα P/mα 0 and renormalization factor Sα T for each band at different temperatures. Due to phonon dressing effects, the polaronic band gap ε  P ≡ ε1 P − ε0 P is also increased. This will affect the interband relaxation dynamics. In Fig. 4, we show the onsite polaron energy and band-gap renormalization versus impurity-BEC coupling constant. In both Figs. 3(a) and 4(a), without loss of generality, we set the initial lower-band onsite energy ε0 to zero. As shown in Fig. 4(b), the band-gap renormalization is almost temperature independent. These quantities are only slightly affected by temperature due to different variational transformation matrices q, as predicted in Eq. (26). Although the band gap is not significantly changed, this renormalization effect is important for interband resonance conditions, which are required for Landau-Zener tunneling to take place in a tilted lattice [105,106]. B. Effective interactions between polarons Here, we briefly discuss interactions between polarons, by considering the additional interactions between impurities beyond the original Hamiltonian (19), which are given by interband transitions, which are due to the interband polaronbath coupling. In Fig. 5(a), we show the induced density-density interactions V αβ i,j versus distance |i − j | between polarons between each band. Since these interactions are induced by the impurity-BEC coupling, the long-range behavior is related to the condensate healing length ξ . Here, we assume we can obtain large values for ξ by tuning the Bose-Bose scattering length aB to a small positive value or keeping the condensate density n0 small. In Fig. 5(b), we also show the ratio of V αβ i,j between onsite and nearest-neighbor terms versus ξ . These ratios increase with the BEC healing length, indicating a longer effective range of the interactions. The effects of these interactions, which are beyond the scope of this paper, might include new ordered polaron phases in our system with Hamiltonian (25). As shown in Fig. 5(c), if we consider that the transverse confinement is due to a very deep optical lattice instead of a single well, different 1D tubes could V. LINDBLAD EQUATION AND INTERBAND DYNAMICS The incoherent part of the Hamiltonian H˜inc = H˜ − H˜T includes the residual coupling between polarons and phonon bath: On the other hand, as the impurity-BEC coupling is increased, the polaron relaxation rate γ P q from Eq. (50) is renormalized by the polaron band gap ε  P and renormalization factor. The increased polaron band gap will involve more phonons by shifting the cutoff momentum (49), while the renormalization factor reduces the whole momentum range. In Fig. 6, we also show the polaron relaxation rate γ P q with blue (red) curve at κ = 1 (2) and total relaxation rate γ P with blue (red) area. As shown in Fig. 6, the renormalization of the relaxation rate is different for various impurity-BEC mass ratios. For larger mass ratio (heavy impurity) such as a system with single 133Cs impurity coupled with 87Rb BEC, the polaron relaxation processes will be enhanced by the shift of the cutoff momentum and increase the total rate. On the other hand, for smaller mass ratio (light impurity) such as a 6 Li impurity coupled with 23Na BEC, the higher-momentum cutoff for|qσx | is not so important due to Gaussian decay of γq . In this case, the renormalization factor will reduce γ P q as well as the total relaxation rate γ P. We can also expect that, for extremely strong impurity-BEC coupling, the total relaxation rate will be reduced due to this renormalization factor with any mass ratio. Finally, we compare the polaronic interband relaxation rate γ P and Fermi golden rule results γ 0 in Fig. 7 as a function of coupling constant and for different impurity-BEC mass ratios. As we have already seen in Fig. 6, the polaron formation will renormalize the interband relaxation rate differently, depending on the mass ratio. We first consider the weak impurity-BEC coupling regime with small κ in Fig. 7(a). For a heavy impurity coupled to a light BEC bath, the interband relaxation process involves more phonon modes and will be enhanced. The difference in the ratio 0 1 1 1.1 0 100 200 0 1 2 (a) (b) κ κ γ P/γ0 2 mI /m B=133/87 mI /m B=40/41 mI /m B=6/23 FIG. 7. (Color online) Ratio between polaron interband relaxation rate γ P and Fermi golden rule results γ 0 at different impurityBEC mass ratio. (a) In the weak-coupling regime, this ratio is increased with the coupling constant κ for a heavy impurity coupled to a light BEC bath, and decreased for a light impurity coupled to a heavy BEC bath. The boson-boson scattering length here is chosen as aB = 100a0 and κc = 2.4. (b) In the strong-coupling regime, the polaron interband relaxation processes for any impurity-BEC mass ratio are suppressed. This leads to an interband self-trapping effect. The boson-boson scattering length here is aB = 0.2a0 and κc = 255. We use mI = 133, mB = 87 for a 133Cs impurity and 87Rb BEC (blue line), mI = 40, mB = 41 for a 40K impurity and 41K BEC (green line), and mI = 6, mB = 23 for a 6 Li impurity and 23Na BEC (red line). Other parameters are V x I = 9ER, V ⊥ I = 25V x I , and n0 = 1 × 1014 cm−3. γ P/γ 0 between polaronic relaxation and the Fermi golden rule result increases with coupling constant κ. On the other hand, for a light impurity coupled to a heavy BEC, the impurity is dressed by heavy phonons in each band and tends to localize in the same band. Although the interband relaxation process does involve more phonon modes than the Fermi golden rule result, this effect is highly suppressed due to Gaussian decay of γq with high momentum. The ratio γ P/γ 0 will be reduced in this system. We also notice that the ratio γ P/γ 0 is not sensitive to the longitudinal trapping potential. On the other hand, we can access the strong-coupling regime by tuning the Bose-Bose scattering aB to a small positive value since the allowed maximum coupling constant in Eq. (18) goes as κc ∝ 1/(n0a3 B) 1/4 . For the strong impurityBEC coupling region with large κ in Fig. 7(b), though the Lindblad master equation might not be accurate enough for such strong interband coupling, we can still obtain some qualitative insight. In each band, the polaron is tightly dressed by phonons with different coupling strength, such that the polaron behaves as a quasiparticle with rather different properties in each band. Also, the band gap ε  P is enhanced in comparison to the bare case, so the polaron cannot hop between bands by creating or annihilating phonons. As shown in Fig. 7(b), we indeed obtain a suppressed interband relaxation process. This interband self-trapping effect is expected in a strongly coupled impurity-BEC system. In a realistic impurity-BEC system, this effect might be also observed together with the well-known self-trapping effect due to deformation of BEC [59,88,114–118]. VI. CONCLUSIONS In conclusion, we have studied a two-band Hamiltonian with Frohlich impurity-phonon coupling with both intraband ¨ and interband terms. Such a Hamiltonian can be realized in experiments where few impurities are immersed in a Bose-Einstein condensate of another species. The impurities are trapped by an anisotropic optical lattice and behave as quasi-1D particles. Based on the Lang-Firsov transformation, we have derived and applied a variational two-band polaron transformation. We have calculated the coherent part of the resulting effective Hamiltonian with two (polaron) bands. In each band the impurity is dressed by phonons as a quasiparticle (polaron) with different properties. The polaronic 	In this work, we study the formation and dynamics of polarons in a system with a few impurities in a lattice immersed in a Bose-Einstein condensate (BEC). This system has been experimentally realized using ultracold atoms and optical lattices. Here, we consider a two-band model for the impurity atoms, along with a Bogoliubov approximation for the BEC, with phonons coupled to impurities via both intraband and interband transitions. We decouple this Frohlich-type term by an extended two-band Lang-Firsov polaron transformation using a ¨ variational method. The new effective Hamiltonian with two (polaron) bands differs from the original Hamiltonian by modified coherent transport, polaron energy shifts, and induced long-range interaction. A Lindblad masterequation approach is used to take into account residual incoherent coupling between polaron and bath. This polaronic treatment yields a renormalized interband relaxation rate compared to Fermi’s golden rule. For a strongly coupled two-band Frohlich Hamiltonian, the 
24	2011	Quantifying, characterizing, and controlling information flow in ultracold atomic gases	P. Haikka,1,S. McEndoo,1,2 G. De Chiara,3,4 G. M. Palma,5 and S. Maniscalco	Quantifying, characterizing, and controlling information flow in ultracold atomic gases We study quantum information flow in a model comprised of a trapped impurity qubit immersed in a BoseEinstein-condensed reservoir. We demonstrate how information flux between the qubit and the condensate can be manipulated by engineering the ultracold reservoir within experimentally realistic limits. We show that this system undergoes a transition from Markovian to non-Markovian dynamics, which can be controlled by changing key parameters such as the condensate scattering length. In this way, one can realize a quantum simulator of both Markovian and non-Markovian open quantum systems, the latter ones being characterized by a reverse flow of information from the background gas (reservoir) to the impurity (system) Introduction. In past decades, high precision control of ultracold atomic gases has allowed the realization of experiments unveiling fundamental phenomena in the physics of many-body quantum systems at low temperatures. Key examples are the observation of Anderson localization [1], the superfluid-Mott insulator transition [2], the creation of Tonks-Girardeau gases [3], and the atom laser [4], just to mention a few. More recently, hybrid systems composed of quantum dots, single trapped ions, and optical lattices coupled to Bose-Einstein condensates (BECs) have been studied both theoretically and experimentally [5]. These systems are studied in the framework of open quantum systems [6], effectively described as one or more two-level systems (qubits) interacting with a reservoir consisting of the ultracold gas. The possibility of manipulating crucial parameters of the reservoir, such as the scattering length [7], combined with the continuous improvements in quantum control of qubits, highlights the enormous potential of hybrid systems as quantum simulators of both condensed-matter models and open quantum systems. In this Rapid Communication, we study a qubit system composed of an impurity atom trapped in a double-well potential, interacting with a BEC environment. This model has been shown to describe an effective pure-dephasing model [8]. Our focus is on the dynamics of quantum information between the qubit system and the ultracold reservoir. We show how information flux can be manipulated by experimentally achievable means, such as changing the scattering length, the effective dimension of the background gas, or the trapping geometry of the qubit. Recently, dynamics of information flow has been an active area of research in the open quantum systems community due to several proposals to link it to the division of quantum processes into Markovian and non-Markovian ones [9–12]. The latter ones have been defined as processes where an *pmehai@utu.fi; www.openq.fi open system recovers some previously lost information and therefore temporarily combats the destructive effect of the environment as a sink for quantum properties. Such effects, limiting the performance of all quantum devices, can indeed be seen as loss of information on the system. Therefore, looking at the dynamics of information flow gives us an indication of the time of usability of a given quantum system for quantum information processing and, more generally, for quantum technologies. Bose-Einstein condensates are often referred to as typical examples of non-Markovian reservoirs; however, a quantitative and qualitative analysis of such a claim does not exist in previous literature. This is partly due to the fact that non-Markovianity measures have been introduced only recently. Here we present the first characterization of nonMarkovian effects in the context of ultracold gases. We derive an analytic expression for the non-Markovianity measure of Ref. [9] valid for general pure-dephasing qubit models. We find a crossover between Markovian and non-Markovian dynamics in an experimentally accessible parameter space of the model, and we uncover the physical mechanisms at the root of non-Markovian phenomena induced by the ultracold background gas. Our findings pave the way to the realization of quantum simulators for non-Markovian open quantum system models with ultracold atomic gases. It is worth mentioning that the first quantum simulator for Markovian open quantum systems has been experimentally realized very recently in the trapped ion context [13,14]. Experiments on quantum simulators of non-Markovian open quantum systems, on the other hand, have not yet been performed and are, in general, more demanding than their Markovian counterpart. Non-Markovian quantum simulators would allow one to tackle crucial fundamental open questions in the theory of non-Markovian open quantum systems, such as the generalization of the Lindblad theorem. The model. The setup we consider (see Fig. 1) consists of an impurity atom trapped in a deep double-well potential  VA(r). The impurity atom forms a qubit system with the two qubit states represented by the occupation of the impurity atom in the left or the right well: |L and |R, respectively. The impurity atom couples to a bosonic background gas B trapped in a shallow potential VB(r), which forms a Bose-Einsteincondensed environment for the qubit system. The Hamiltonian for this system, derived in Ref. [8], iswhere σz = |RR|−|LL| and Ek =  k[k + 2g(D) B nD ] is the energy of kth Bogoliubov mode ck of the condensate with boson-boson coupling frequency g(D) B and condensate density nD. D denotes the effective dimension of the environment. The energy of a free mode is k = h¯ 2k2/(2mB), where k = |k| and mB is the mass of a background gas particle. Furthermore, gk and ξk are coupling constants that depend on the spatial form of the states |L and |R and on the shape of the Bogoliubov modes. Their specific form is elaborated in Ref. [8]. When the background gas is at zero temperature, the reduced dynamics of the impurity atom is captured by the following time-local master equation (ME): where gAB is the impurity-boson coupling frequency, τ is a trap parameter, and L is half the distance between the two wells of the double-well potential. We have derived ME (2) using the time-convolutionless projection operator technique to second order in the coupling constant gAB [6]. Remarkably, in this case the secondorder ME describes the reduced dynamics exactly [15]. Solving the ME reveals that the impurity atom dephases without exchanging energy with the background gas. More precisely, ρii(t) = ρii(0) and ρij (t) = e− (t) ρij (0) when i = j , where ρij = i|ρ|j  and i,j = R,L. The decoherence function  (t) =  t 0 ds γ (s) coincides with that derived in Ref. [8]; however, here we wish to stress the connection between the decay rate and the non-Markovian features. The authors of Ref. [8] discovered situations when the decoherence function  (t) is nonmonotonic and conjectured that this is due to non-Markovian effects in the reduced dynamics. Already the form of the ME (2) supports this intuition; the theory of non-Markovian quantum jumps has shown that there is a profound connection between non-Markovian effects and negative regions of the decay rates of Lindblad-structured MEs as the one of Eq. (2) [16]. The full characterization of non-Markovian systems, however, is usually not an easy task. In the following, we derive an analytic expression for the non-Markovianity measure, discover the existence of a Markovian–non-Markovian crossover, and expose the physical mechanisms at the root of this transition in the system dynamics. Non-Markovianity measure. Breuer, Laine, and Piilo (BLP) have proposed a rigorous definition for non-Markovianity of a quantum channel  based on the dynamics of the so-called information flux σ(t) = dD[ρ1(t),ρ2(t)]/dt [9]. This is the temporal change in the distinguishability D[ρ1(t),ρ2(t)] = 1 2 ||ρ1(t) − ρ2(t)||1 of two evolving quantum states ρ1,2(t) = (t)ρ1,2(0) as measured by the trace distance. Negative information flux describes information leaking from the system to its environment and it is associated to Markovian dynamics. Instead, if it is possible to find a pair of states ρ1,2(0) for which the information flux is positive for some interval of time, that is, the system regains some of the previously lost information, then process  is considered non-Markovian. The amount of non-Markovianity is defined to be the maximal amount of information that the system may recover from its environment, formally NBLP = maxρ1,2  σ >0 ds σ(s). Generally, calculating NBLP is difficult because of the optimization over all pairs of initial states. Indeed, an analytic expression for the non-Markovianity measure has been calculated, until now, only for the Jaynes-Cummings model, the driven qubit model, and the depolarizing channel [9,17,18]. For the model studied in this Rapid Communication, we find that σ(t) > 0 if and only if γ (t) < 0; that is, the process is non-Markovian precisely when the decay rate can take temporarily negative values. Within experimentally relevant values of the physical parameters, we have discovered at most a single time interval t ∈ [a,b], when the decay rate is negative and information flows back to the system after an initial period of information loss. Therefore, instead of using the original measure faithfully and quantifying non-Markovianity as the maximal amount of information that the system may recover, we introduce a normalized quantity that reveals the maximal fraction of the previously lost information that the system can recover: Unlike NBLP, the modified quantifier N is bounded between zero (system only leaks information) and one (system regains all previously lost information) and is therefore more meaningful as a number. We have confirmed numerically that in the relevant case of dephasing noise the above quantity is maximized for the same pair of initial states that maximize NBLP. These are the states whose Bloch vectors lie on the opposite sides of the equator of the Bloch sphere [17]. Using We are now ready to study how changes in the background scattering length and in the dimensionality of the BEC affect the dynamics of information flow. Three-dimensional BEC. As a first step, we consider a threedimensional (3D) background BEC with equal confinement of the background gas in all directions. We consider a 87Rb condensate of density n3 = n0 = 1020 m−3 and 23Na impurity atoms trapped in an optical lattice with lattice wavelength λ = 600 nm and trap parameter τ = 45 nm. The impurity-boson coupling is gAB = 2πh¯ 2 aAB/mAB, where mAB = mAmB/(mA + mB) and mA and mB are the masses of the impurity atoms and the bosons, respectively, and aAB = 55 a0, where a0 is the Bohr radius. Similarly, the boson-boson coupling frequency is g3D B = 4πh¯ 2aB/mB, but now we assume that the s-wave scattering length of the background gas can be tuned from its natural value aB = aRb ≈ 5.3 nm via Feshbach resonances. We explore a range of values of aB consistent with the assumption of dilute gas and with the regime of weakly interacting gases. The latter is a stronger condition, requiring √a3 B n0        1. As a consequence, we can tune the scattering length up to a maximum value given by aB ≈ 3 aRb. Figure 2 shows the non-Markovianity measure Ndeph as a function of aB for three different values of the well separation L. Increasing L magnifies the fraction of recovered information flow due to the increased ability of the condensate to resolve the qubit system. Similarly, non-Markovian effects are amplified for stronger interaction of the condensate. However, we find that the scattering length alone plays a crucial role in the emergence of non-Markovian reservoir memory effects. When the background gas is free or very weakly interacting, 0  aB  acrit B ≈ 0.034 aRb, the qubit only leaks information to the BEC environment. Instead, for a strong enough interaction strength of the background gas, aB > acrit B , the condensate can take on the role of information storage and feed some information back to the qubit. This result holds for any value of L. This finding challenges the conclusion of Ref. [8], where the scattering length dependent Markovian–non-Markovian crossover was only attributed to the 1D case. We have discovered that the situation is indeed more subtle and we will show next that the crossover point exists in all three dimensions. Lower dimensions. By a suitable modification of the potential of the condensate VB(r), we can create a quasi2D background gas where the gas is trapped in a slightly anisotropic, pancake-shaped harmonic trap. Assuming that the scattering length is still much smaller than the axial length of the condensate, aB          az, the coupling term is modified to g2D B = √8πh¯ 2 aB/(mBaz) and the 2D condensate density is n2 = √πn0az [19]. Within the limits of a dilute gas, we can increase the scattering length up to aB ≈ 2 aRb. The potential VB(r) can be also modified to create a cigar-shaped quasi-1D background gas with transversal width a⊥. The consequent coupling is g1D B = 2¯h2aB/(mBa2 ⊥) and the 1D density is n1 = n0π a2 ⊥, again provided that gas is weakly enough confined, aB          a⊥ [20]. In the quasi-1D regime, diluteness of the gas allows at most aB  aRb. In Fig. 3, we plot the non-Markovianity measure Ndeph in the quasi-1D, quasi-2D, and 3D cases. In lower dimensions, we find the critical values acrit,2D B ≈ 0.122aRb and acrit,1D B ≈ 0.183aRb. Clearly, when the dimensionality of the background gas is lowered, the crossover value of the scattering length acrit B increases. Crucially, as we remarked before, acrit,3D B > 0 and therefore it is possible to create both Markovian and nonMarkovian dynamical processes in all three dimensions. We note here that the quantities we have chosen to vary, namely, the scattering length aB and the well separation L, are indeed the most relevant quantities for manipulating the information flowback. The trap parameter τ determines the trapping frequency of the double-well trap VA(r) and acts as a natural cutoff parameter in the decay rate of Eq. (3). As long as the double-well trap is deep enough to prevent hopping between the two sites, the particular value of τ has only a minor effect on information flow. It is also clear from the form of the decay rate that the boson-impurity coupling gAB cannot affect the Markovian– non-Markovian crossover. Moreover, we have found that its value has negligible effect on the non-Markovianity quantifier Ndeph. Intuitively, it affects the amount of outgoing and incoming information almost equally but leaves their ratio unchanged. In order to explain the key non-Markovian features in the dynamics of the qubit system when the dimensionality and the scattering length of the background gas vary, we need to take a closer look at the spectrum of the BEC reservoir. Environmental spectrum. The crossover between Markovian and non-Markovian processes is best understood in terms of the environmental spectrum J (ω). Consider the general dephasing model introduced by Palma, Suominen, and Ekert describing qubit dynamics: ρii(t) = ρii(0) and ρij (t) = e− ˜ (t) ρij (0), where  ˜ (t) ∼  dω J (ω)(1 − cos ωt)/ω [21]. The dynamical process is non-Markovian if and only if γ˜(t) = d ˜ (t)/dt < 0 for some interval of time. We assume an Ohmic-type spectrum J (ω) ∼ ωs and recall the convention that the spectrum is sub-Ohmic when s < 1, Ohmic when s = 1 or super-Ohmic when s > 1. Introducing an ad hoc exponential cutoff so that J (ω) = ωs exp{−ω2/ω2 C}, where ωC is the cutoff frequency, it is straightforward to show that the dynamics is non-Markovian when s>scrit = 2. Therefore, in a general setting, a qubit dephasing under the effect of either a sub-Ohmic or an Ohmic environment can only leak information to its environment. If the environment has a super-Ohmic spectrum, the issue is less straightforward: only if the spectrum is sufficiently super-Ohmic with s>scrit, information can flow back to the system from the environment. The qubit in an ultracold bosonic environment considered in this work is a special case of the model above with J (ω) =  k |gk| 2δ(¯hω − Ek). The reservoir spectrum J (ω) does not have a simple analytical expression due to the complicated form of the coupling constant gk. However, it can be shown that, in the case of a free background gas in one, two, or three dimensions, the spectrum is sub-Ohmic, Ohmic, or super-Ohmic, respectively [8]. The spectrum changes critically when one considers the boson-boson coupling quantified by the scattering length aB. In this case, increasing the scattering length effectively increases the value of s. Hence when we increase aB in the 1D case, the spectrum changes from sub-Ohmic to Ohmic to super-Ohmic and once a critical threshold of super-Ohmicity is reached the environment can feed information back to the system. In the 2D non-interacting case, the spectrum is Ohmic and a weaker interaction is required to reach the crossover point scrit, leading to acrit,2D B < acrit,1D B . Finally, in the 3D case, the spectrum is already superOhmic in the noninteracting case, although not super-Ohmic enough to give rise to non-Markovian dynamics. Already a small increase in the scattering length modifies the spectrum so that the direction of information flow can be temporarily reversed. Conclusion. We have studied quantum information flux in an ultracold hybrid system of an impurity atom immersed in a BEC environment. We have shown explicitly how precise control of the ultracold background gas affects the spectrum felt by the qubit and therefore enables the manipulation of the qubit dynamics and the information flux. An important discovery is the existence of a controllable crossover between Markovian and non-Markovian dynamics. In particular, we have discovered experimentally accessible means of reaching non-Markovian dynamical regimes, where the background gas may feed information back to the qubit instead of acting only as a sink for information. Such quantum reservoir engineering is fundamental for understanding decoherence processes in quantum information processing and, more specifically, for the realization of quantum simulators. The loss of any quantum property—be it quantum superposition or entanglement or quantum discord—can be seen as due to information lost by the system because of its interaction with the environment. In this sense, studying information flux is a convenient way to quantify the tendency of the quantum system to retain those quantum properties necessary for quantum technologies. Non-Markovian systems are able to regain previously lost information and, therefore, compared to Markovian systems, they guarantee a longer operational time for quantum devices. A natural future direction, which will constitute the subject of a followup paper, is the study of the optimal conditions for entanglement-keeping in quantum registers of impurities, as indicated by information flow. Finally, we would like to stress that the ideas presented in this work can be realized in present-day experiments. The measurement of the degree of non-Markovianity can be achieved by measuring the impurity coherence, since Ndeph = [ρLR(b) − ρLR(a)]/[ρLR(0) − ρLR(a)]. This can be done by mapping the states |L and |R to the superpositions |L±|R. This atomic “beam splitter” would allow one to infer, from the measurement of the population imbalance of the two wells, the coherence ρLR [22].	
25	2011	Distance between quantum states in the presence of initial qubit-environment correlations: A comparative study	Jerzy Dajka and Jerzy Łuczka Institute of Physics, University of Silesia, 40-007 Katowice, Poland Peter Hanggi ¨ Institute of Physics, University of Augsburg, D-86135 Augsburg, Germany (Received 8 July 2011; published 27 September 2011)	Distance between quantum states in the presence of initial qubit-environment correlations: A comparative study The time evolution of the trace distance between two states of an open quantum system may increase due to initial system-environment correlations, thus exhibiting a breakdown of distance contractivity of the reduced dynamics. We analyze how the time evolution of the distance depends on the chosen distance measure. Here we elucidate the behavior of the trace distance, the Hilbert-Schmidt distance, the Bures distance, the Hellinger distance, and the quantum Jensen-Shannon divergence for two system-environment setups, namely a qubit bilinearly coupled to an infinite and a finite-size environment with the latter composed of harmonic oscillators. I. INTRODUCTION Quantum states represented by density matrices ρ can be determined by quantum state tomography and compared using various quantifiers. Distances and other similarity measures provide a quantitative method to evaluate how close two states are together or how precisely a quantum channel can transmit information. Unfortunately, there is no single, ideal measure of distinguishability of different states. There are no criteria for the distance measure to be “better than another.” Even the natural requirement that the distance between states should have properties of a metric (i.e., identity of indiscernibles, symmetry, and the triangle inequality) is relaxed in a case of fidelity which is a celebrated statistical similarity measure. Loosely speaking, two states are close to each other if the distance is small. We also expect that two different distances are equivalent if any two states that become closer to one another in the sense of one distance measure also become closer in the sense of the second, and vice versa. There are diverse ways of introducing a notion of distance between two quantum states [1]. Examples of such distance measures comprise the trace distance, Hilbert-Schmidt distance, Bures distance, Hellinger distance, and Jensen-Shannon divergence, to mention a few, see also Refs. [1–5]. These metrics possess distinct properties like being Riemannian, monotone (contractive), with bounds and relations among them [6]. Let us recall that any positive and trace-preserving map E defined on the whole space of operators ρ on the Hilbert space is contractive with respect to a given distance D[ρ1,ρ2] if In particular, when E = Et is a completely positive quantum dynamical semigroup such that ρ(t) = Etρ(0), then contractivity means that As a consequence, the distance cannot increase in time and the distinguishability of any states cannot increase above an initial value. In particular, if a quantum open system and its environment are initially prepared in an uncorrelated state, the reduced dynamics is completely positive and hence contractive with respect to some metrics. In consequence, the distance D[ρ1,ρ2] between two states can tend to zero when the system approaches a unique steady state (i.e., the dynamics is relaxing). We emphasize that contractivity is not a universal feature but depends on the metric: Quantum evolution may be contractive with respect to a given metric and may not be contractive with respect to other metric measures. Moreover, contractivity of quantum evolution can break down provided that the system is initially correlated with its environment. Effects induced by such correlations have been studied in various context [7–10]. First experiments on initial systemenvironment correlations were reported in Ref. [11]. Examples of an exact reduced dynamics which fail contractivity with respect to the trace distance are presented in Refs. [12,13]: The trace distance of different states grows above its initial value and the distinguishability growth occurs not only at the short time scales but is shown to be a feature of the long-time limit as well. The trace metric is likely the most important measure of the size for distance in quantum information processing, and according to Ref. [13], an increase of the distance can be interpreted in terms of the exchange of information between the system and its environment. If the distance increases over its initial value, information which is locally inaccessible at the initial time is transferred to the open system. This transfer of information enlarges the distinguishability of the open-system states which suggests various ways for the experimental detection of initial correlations. With this study we demonstrate that the correlation-induced distinguishability growth is not generic with respect to distance measures, but distinctly depends on the assumed form of the metric measure. The outline of the paper is as follows. In Sec. II we list several forms of the distance measure. In Sec. III, we define a dephasing model of the qubit plus environment [14] and the environment is assumed to be infinite. We also present the reduced dynamics of the qubit for a particular initial qubitenvironment state which is correlated (entangled). Properties of time evolution of the distance between two states of the qubit are demonstrated for selected metrics. In Sec. IV we consider the similar model, but now with a finite-size environment consisting of just one boson. We study distances between two states and analyze its properties. Finally, Sec. V provides our summary and some conclusions. II. A SELECTION OF DIFFERENT DISTANCE MEASURES The question of similarity between quantum states can have very different meanings depending on the context in which the question is posed. One can distinguish at least two main classes of problems. The first is related to the geometric structure of a set of states, and the second is related to the statistical content of quantum states. These two classes are not disjoint due to the richness of links joining different quantifiers [6]. Here we limit our consideration to measures which are, or are expected (as the Jensen-Shannon divergence discussed below) to be a metric. We will consider the following types of the distance between any two states ρ1 and ρ2: (1) The use of the trace distance presents a contraction in the sense discussed in the Introduction and is limited to the unit interval The trace distance, being Euclidean, has apart from its geometric characteristics, also a profound statistical meaning as a quantifier for “statistical distinguishability” of quantum states [5]. Due to its universal character the trace distance has been considered in the context of a contractivity breakdown caused by the system-environment correlations [12,13]. In this paper it will serve as a natural reference for other measures to be compared with. (2) The space of density matrices describing states of a quantum system can be equipped with a very natural scalar product [5] leading to the Hilbert-Schmidt distance This distance is restricted by the inequality relation 0  DHS[ρ1,ρ2]  2DT [ρ1,ρ2]. The Hilbert-Schmidt distance is of Riemann type. Unfortunately, it generally does not possess the “contractivity property” discussed in the Introduction. Fortunately enough, however, archetype quantum systems such as qubits constitute useful exceptions, as will be discussed in further detail below. (3) There is a very elegant and deep geometric structure useful for studying general quantum systems, namely the Hilbert-Schmidt fiber bundle [6]. Its base manifold is equipped with a natural metric [6] (i.e., the Bures distance) The Bures distance is contractive and can be expressed by the fidelity and hence, additionally to its geometric character, the Bures distance inherits a clear statistical interpretation. In this case  Among the variety of distances between states there are measures whose definition originate from the statistical interpretation of quantum states [6]. One of them is the socalled Hellinger distance where the quantum affinity reads The Hellinger distance assumes values from the interval  The notion of (information) entropy occurs in almost all branches of physics as a tool of quantifying information or relative information contained in states, either classical or quantum. There are certain technical difficulties in using certain types of information entropies [15]. These measures are, in general, not metrics. The Jensen-Shannon divergence is a tool which allows one to overcome this sort of problem. It is defined in terms of a symmetrized relative entropy between states; here, however, we use instead the following expression [3]: is the von Neumann entropy. This quantity takes values from the unit interval Whether the Jensen-Shannon divergence is a metric for all mixed states remains an unsolved problem [15,16]. Below, we will consider one-qubit system (with an N = 2-dimensional Hilbert space) for which one can represent the density matrices in the form  where ri = [xi,yi,zi] is the Bloch vector and σ = [σx ,σy ,σz] are the Pauli matrices. In this case, the trace and HilbertSchmidt distances are equivalent, namely [17] This distance is equal to the ordinary Euclidean distance between the two states on the Bloch sphere (i.e., DHS(ρ1,ρ2) = |r1 − r2|). Moreover, the expression for the Bures distance simplifies because the fidelity assumes the form [6] The Helinger distance can explicitly be calculated using the relation for the affinity (8). Then the affinity is expressed by the relation [2] where r2 i = x2 i + y2 i + z2 i . The Jensen-Shannon divergence (9) is expressed by the von Neumann entropy which is given by It has been proved that for qubits the Jensen-Shannon divergence is a metric [16]. In prior works [12,13], examples showing that the trace distance of different states can grow above its initial value have been presented. Our objective here is to investigate whether the growth of distance measure is preserved as well for the other metric measures introduced above. III. MODEL A: QUBIT COUPLED TO INFINITE ENVIRONMENT OF OSCILLATORS In this section, we consider the same model as in Ref. [12]. For the readers convenience and to keep the paper selfcontained, we provide all necessary definitions and notation. The model consists of a qubit Q (two-level system) coupled to its environment B and we limit our considerations to the case when the process of energy dissipation is negligible and only pure dephasing is acting as the mechanism responsible for decoherence of the qubit dynamics [14]. Such a system can be described by the Hamiltonian (with ¯h = 1) where Sz is the z component of the spin operator and is represented by the diagonal matrix Sz = diag[1, − 1] of elements 1 and −1. The parameter ε is the qubit energy splitting, IQ and IB are identity operators (matrices) in corresponding Hilbert spaces of the qubit Q and the environment B, respectively. The operators a† (ω) and a(ω) are the bosonic creation and annihilation operators, respectively. The real-valued spectrum function h(ω) characterizes the environment. The coupling is described by the function g(ω) and the function g∗(ω) is the complex conjugate to g(ω). The Hamiltonian (16) can be rewritten in the block-diagonal structure [18] where Re is a real part of the scalar product 0|f  of two states in the environment Hilbert space. The parameter λ ∈ [0,1] controls the initial entanglement of the qubit with the environment. For λ = 0 the qubit and the environment are initially uncorrelated while for λ = 1 the entanglement is most prominent for a given class of initial states. The initial state (20) of the total system evolves according to the formula The density matrix of the total (isolated) system is  (t) = |(t)(t)|. In turn, the partial trace TrB over the environment B yields the density matrix ρλ(t) = TrB (t) of the qubit. It can be expressed in the matrix form as Without loss of generality we have assumed here that the functions g(ω) and f (ω) are real valued. A. Analysis of different distance measures For the analysis of distance properties of the model considered, we still have to specify two quantities: the spectral density gh(ω) = g(ω)/h(ω) and the coherent state determined by the function f (ω). The spectral density function gh(ω) completely defines the coupling and modes of the environment. Typically the spectral function is taken as some continuous function of frequency to indicate that the environment can be treated as infinite compared to the system. With this study we restrict ourselves to the case in which this function assumes the explicit form where α > 0 is the qubit-environment coupling constant, ωc is a cutoff frequency, and μ > −1 is the “Ohmicity” parameter: the case −1 <μ< 0 corresponds to the sub-Ohmic, μ = 0 to the Ohmic, and μ > 0 to the super-Ohmic environments, respectively. Comparing this equation with the expression for the standard spectral function J (ω) (see, e.g., Refs. [20,21]), one can find the relation [18] As follows from our previous study, only in the case of superOhmic environment, the trace distance can increase. Therefore below we analyze only this regime. To determine the coherent state |f , we can propose any integrable function f (ω) but for convenience let  The only reason for such a choice is the possibility to calculate explicit formulas for the functions in Eqs. (28) and (29). As a result one gets and (z) is the Euler gamma function. We next examine the time evolution of the distance for all four distance measures: namely the trace distance DT , the Bures distance DB, the Hellinger distance DH , and the quantum Jensen-Shannon measure DJS. We recall that the trace and Hilbert-Schmidt distances are equivalent. As shown in Ref. [12], the only chance to observe an increase of the distance between two states is to vary the parameters of the environment encoded in |λ in Eq. (21). The simplest theoretical possibility is to manipulate the correlation parameter λ. When two different states are determined by two different sets of numbers b(k) ± (k = 1,2) in Eq. (20) for the same state |λ then Aλ1 (t) = Aλ2 (t) and an increasing growth of the distance becomes not possible. In Fig. 1, we depict the time evolution of the distances D(t) = D[ρ0(t),ρλ(t)] between the initially noncorrelated and correlated states for four metrics. We observe that only for the trace metric, the distance D[ρ0(t),ρλ(t)] can increase above itsinitial value and there is some optimal value of the correlation parameter 0 <λ< 1 for which the distinguishability of final states is the best. Because this case was studied in Ref. [12], we do not present the details here for the trace distance properties. In the remaining three cases, the distance between states at arbitrary time t > 0 is always smaller than the distance at time t = 0 and the distinguishability of final states is weaker than for the initial states. An interesting feature is the appearance of the absolute minimal distance at some time tm > 0 during the time evolution of the qubit. At an early stage of time evolution, the distance decreases, reaching a minimum before it increases again and eventually saturates at asymptotic long times. The conclusion from our analysis depicted in Fig. 1 hence is as follows: An increase of the distance above its initial value between two qubit states presents not a universal property of the correlated initial state, but instead is rather sensitive to the chosen metric measure. Among our chosen five different metric measures, only the trace and the HilbertSchmidt metrics exhibit this typical property for the considered decoherence model. IV. MODEL B: QUBIT COUPLED TO A FINITE ENVIRONMENT The preparation of an initial state as determined by Eq. (20) requires highly sophisticated quantum engineering tools which presently seem not feasible or at best difficult to realize. Fortunately, interesting features of distances between states resulting from initial system-environment correlations can be studied with a simplified setup. Following such reasoning we next study a qubit that is coupled to a finite-size environment. In this case the notion of decoherence is absent in a strict sense of the term. Nevertheless, the considered qubit constitutes an open system. Our choice of a finite bosonic environment is motivated by recent progress in quantum engineering of nonclassical electromagnetic fields which can be prepared in various states, both in the optical [22] and in the microwave [23] energetic regimes. As an example, we consider a single boson mode. The total Hamiltonian (19) then reduces to the form where g0 is a coupling constant. The initial state of the total system is, in general, correlated, namely The state |0is a vacuum state (a ground state) of the boson and the choice for the state |F is limited to two classes studied in quantum optics, being known to be distinct with respect to their nonclassical character. First we use |F=|z to be a coherent state. Next we analyze the case when |F=|N is a number eigenstate. The density matrix of the qubit assumes the same structure as in Eq. (26), but now with the modified function Aλ(t). A. Case of initial coherent states Let for any complex number z = |z|eiφ, the state |F=|z be a coherent state of the boson. Then the function Aλ(t) is given by and g = g0/ω is the rescaled coupling. Because the total system is finite, time evolution of the qubit states is periodic. However, it is not unitary evolution. The distance between two states of the qubit is also a periodic function of time. Let us now inspect the time dependence of all four distances: trace DT = DHS/ √2, Bures DB, Hellinger DH , and Jensen-Shannon DJS distances. In Fig. 2 we illustrate the role of the initial qubit-environment correlations in the case when two different initial states are determined by two different states|λ with different λ1 and λ2. The most peculiar feature is that for all measures, the distance at any time t > 0 is not smaller than at initial time. It is in clear contrast to the case of the infinite environment case when only the trace distance can increase about its initial value. Now, at the beginning, for t > 0, all distances increase above their initial value reaching the maximal value, which in turn grows when the correlation parameter λ → 1. The maximal amplitude of distance oscillations is shown up for maximally entangled states (i.e., for λ = 1). It also depends on other system parameters, in particular on the state of environment which is determined by two quantities: the amplitude |z| and phase φ of the coherent state |z. The inspection of the results revealed that there are regimes of optimal values of |z| for which the distinguishability of two qubit states is most prominent. We present it in Fig. 3 for the trace and Jensen-Hellinger distances. The remaining two (Bures and Hellinger) distances exhibit similar behavior as the Jensen-Shannon distance. In the two bottom panels of Fig. 3 we demonstrate how the phase of the coherent state changes the distance. Again, as previously, we present only two cases. Two other cases are similar to the Jensen-Shannon one. Let us observe that in some regimes the trace distance possesses distinctive features which are different from other distance measures. Next, let us consider the case when two different states are determined by two different sets of numbers b(k) ± (k = 1,2) in Eq. (20) but with the same state |λ. One state is fixed by b+ = b− = 1/ √2. The second state is conveniently parameterized by two angles θ and ζ on the Bloch sphere and is determined by the relations b+ = cos(θ/2) and b− = exp(iζ ) sin(θ/2). The result is depicted in Fig. 4 which shows that the amplitude of time-periodic oscillations of the distance can typically be increased by increasing the geometrical distance of initial states on the Bloch sphere. However, there are some exceptions such as, for example, for the case ζ = π/4 in the case of the trace distance.B. Case of initial number states Let |F=|N be a number eigenstate of the boson. Contrary to coherent states, such eigenstates are orthogonal and the state (35) becomes maximally entangled, that is, its partial trace, taken with respect to the bosonic degree of freedom, is an identity and it corresponds to the maximally mixed state of the qubit. In this case, the function Aλ(t) assumes the form As in the first case, time evolution of the qubit states is time periodic and in consequence distance is also a periodic function of time. In Fig. 5, we present two forms of the distance, namely the trace and the Jensen-Shannon ones. Only these two distance measures can exhibit the increased distance over their initial value. The “optimal” environment state is the first excited one (i.e., when N = 1). This state is highly nonclassical. The question of whether there is any relation between the nonclassical character of the environment and the distance between reduced qubit states remains open and will be postponed for further considerations. Further excited states diminish the positive value of difference D(t) − D(0) or invert it into a negative value. Two remaining (Bures and Hellinger) distances behave in a similar way as the Jensen-Shannon one but they are removed down and never exceed their initial values. V. SUMMARY The objective to distinguish two quantum channels presents a most important challenge for quantum information processing tasks. The difficulty of the distinguishability issue leads naturally to a study of the problem on restricted classes of channels. With this work we presented two models and we have elucidated the properties of four distance measures for quantum states for the situation of a qubit which is coupled to an environment. At initial times, the system is in a correlated (entangled) state. Our chosen measures include the trace (and equivalent Hilbert-Schmidt), Bures, Hellinger, and Jensen-Shannon distances. We have considered two examples of the environment: namely an infinite one consisting of bosons and a finite one consisting of a single boson. We have demonstrated that in the case of the infinite environment, only the trace distance exhibits an increase above its initial value. All other remaining distances studied do not exhibit this property. In the case of a finite environment, however, some kind of universality is observed for the case when the boson consists in a mixture of ground and coherent states. In this second case, all distances behave more or less similarly: the distance measures oscillate with a common frequency between an initial value and some maximal positive value, which is different for differently chosen metrics. Nevertheless, their time dependence behaves qualitatively the same. This is not the case when the boson is in a mixture of the ground and excited states; only the trace and Jensen-Shannon distances are allowed to grow above the initial value. Our main conclusion is as follows: The result of an increase of the distance measure above its initial value constitutes no universal property; its behavior upon evolving time strongly depends on the employed distance measure. in this respect, the trace distance receives a special status. The authors are confident that this work may stimulate yet additional studies. Particularly, it would be interesting to investigate in some detail the objective of universally valid, initial-state-dependent and/or system-dependent properties of the various distance measures in use. The generalization of our results to (i) other classes of initial correlations between the system and environments of a different nature and (ii) for nonzero temperatures provide yet other appealing routes for future research.	A comparative study The time evolution of the trace distance between two states of an open quantum system may increase due to initial system-environment correlations, thus exhibiting a breakdown of distance contractivity of the reduced dynamics. We analyze how the time evolution of the distance depends on the chosen distance measure. Here we elucidate the behavior of the trace distance, the Hilbert-Schmidt distance, the Bures distance, the Hellinger distance, and the quantum Jensen-Shannon divergence for two system-environment setups, namely a qubit bilinearly coupled to an infinite and a finite-size environment with the latter composed of harmonic oscillators. I. INTRODUCTION